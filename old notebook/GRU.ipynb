{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class TemporalEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self, input_size: Tuple[int, int], num_images: int, device: str\n",
    "        ) -> None:\n",
    "        super().__init__()\n",
    "        # Set the input size of the image.\n",
    "        self.input_size = input_size\n",
    "        # Set the size of the flattened image.\n",
    "        self.flatten_size = input_size[0] * input_size[1]\n",
    "        # Set a list of GRUs, one for each image.\n",
    "        self.gru = nn.GRU(\n",
    "            self.flatten_size, self.flatten_size, num_layers=num_images,\n",
    "            batch_first=True)\n",
    "        #self.grus = nn.ModuleList(\n",
    "        #    [nn.GRU(self.flatten_size, self.flatten_size)\n",
    "        #     for _ in range(num_images)])\n",
    "        # Set the device used for the computations.\n",
    "        self.to(device)\n",
    "        self.device = device\n",
    "\n",
    "    def to(self, device: str) -> None:\n",
    "        super().to(device)\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, x: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        batch_size = x.shape[0]\n",
    "        n_channels = x.shape[1]\n",
    "        # Set the initial hidden states \n",
    "        initial_hidden_state = torch.zeros(\n",
    "            batch_size, n_channels, self.flatten_size, dtype=torch.float32,\n",
    "            device=self.device)\n",
    "\n",
    "        _, out = self.gru(x.flatten(start_dim=2), initial_hidden_state)\n",
    "        # Iterate over the images and pass them through the GRUs.\n",
    "        '''for i, gru in enumerate(self.grus):\n",
    "            # Flatten the image.\n",
    "            img = x[:, i].flatten(start_dim=1)\n",
    "            # If it is the first image, use the initial hidden state.\n",
    "            if i == 0:\n",
    "                h = initial_hidden_state\n",
    "            # Get the forward pass of the GRU.\n",
    "            h, _ = gru(img, h)''';\n",
    "        \n",
    "        # Turn the hidden state to the original shape.\n",
    "        out = out.view(batch_size, n_channels, self.input_size[0],\n",
    "                       self.input_size[1])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "img_ex = np.load('./artifacts/np_dataset-v0/B07_2022_06_02.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(96, 1, 446, 780)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_ex.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = TemporalEncoder(input_size=(64, 64), num_images=3, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def ls(path: Path): \n",
    "    \"Return files on Path, sorted\"\n",
    "    return sorted(list(path.iterdir()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "import wandb\n",
    "from tqdm import tqdm as progress_bar\n",
    "\n",
    "# from cloud_diffusion.utils import ls\n",
    "\n",
    "PROJECT_NAME = \"ddpm_clouds\"\n",
    "DATASET_ARTIFACT = 'capecape/gtc/np_dataset:v0'\n",
    "\n",
    "class DummyNextFrameDataset:\n",
    "    \"Dataset that returns random images\"\n",
    "    def __init__(self, num_frames=4, img_size=64, N=1000):\n",
    "        self.img_size = img_size\n",
    "        self.num_frames = num_frames\n",
    "        self.N = N\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.randn(self.num_frames, self.img_size, self.img_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.N\n",
    "\n",
    "\n",
    "class CloudDataset:\n",
    "    \"\"\"Dataset for cloud images\n",
    "    It loads numpy files from wandb artifact and stacks them into a single array\n",
    "    It also applies some transformations to the images\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 files, # list of numpy files to load (they come from the artifact)\n",
    "                 num_frames=4, # how many consecutive frames to stack\n",
    "                 scale=True, # if we images to interval [-0.5, 0.5]\n",
    "                 img_size=64, # resize dim, original images are big (446, 780)\n",
    "                 valid=False, # if True, transforms are deterministic\n",
    "                ):\n",
    "        \n",
    "        tfms = [T.Resize((img_size, int(img_size*1.7)))] if img_size is not None else []\n",
    "        tfms += [T.RandomCrop(img_size)] if not valid else [T.CenterCrop(img_size)]\n",
    "        self.tfms = T.Compose(tfms)\n",
    "        self.load_data(files, num_frames, scale)\n",
    "        \n",
    "    def load_day(self, file, scale=True):\n",
    "        one_day = np.load(file)\n",
    "        if scale:\n",
    "            one_day = 0.5 - self._scale(one_day)\n",
    "        return one_day\n",
    "\n",
    "    def load_data(self, files, num_frames, scale):\n",
    "        \"Loads all data into a single array self.data\"\n",
    "        data = []\n",
    "        # TODO: download all files\n",
    "        for file in progress_bar(files[0:2], leave=False):\n",
    "            one_day = self.load_day(file, scale)\n",
    "            wds = np.lib.stride_tricks.sliding_window_view(\n",
    "                one_day.squeeze(),\n",
    "                num_frames,\n",
    "                axis=0).transpose((0,3,1,2))\n",
    "            data.append(wds)\n",
    "            # pbar.comment = f\"Creating CloudDataset from {file}\"\n",
    "        self.data = np.concatenate(data, axis=0)\n",
    "\n",
    "    def shuffle(self):\n",
    "        \"\"\"Shuffles the dataset, useful for getting \n",
    "        interesting samples on the validation dataset\"\"\"\n",
    "        idxs = torch.randperm(len(self.data))\n",
    "        self.data = self.data[idxs]\n",
    "        return self\n",
    "\n",
    "    @staticmethod\n",
    "    def _scale(arr):\n",
    "        \"Scales values of array in [0,1]\"\n",
    "        m, M = arr.min(), arr.max()\n",
    "        return (arr - m) / (M - m)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.tfms(torch.from_numpy(self.data[idx]))\n",
    "    \n",
    "    def __len__(self): return len(self.data)\n",
    "\n",
    "    def save(self, fname=\"cloud_frames.npy\"):\n",
    "        np.save(fname, self.data)\n",
    "\n",
    "\n",
    "class CloudDatasetInference(CloudDataset):\n",
    "     def load_data(self, files, num_frames=None, scale=None):\n",
    "        \"Loads all data into a single array self.data\"\n",
    "        data = []\n",
    "        max_length = 100\n",
    "        # TODO: download everything\n",
    "        for file in files[0:2]:\n",
    "            one_day = self.load_day(file, scale)\n",
    "            data.append(one_day)\n",
    "            max_length = min(max_length, len(one_day))\n",
    "        self.data = np.stack([d[:max_length] for d in data], axis=0).squeeze()\n",
    "\n",
    "\n",
    "def download_dataset(at_name, project_name):\n",
    "    \"Downloads dataset from wandb artifact\"\n",
    "    def _get_dataset(run):\n",
    "        artifact = run.use_artifact(at_name, type='dataset')\n",
    "        return artifact.download()\n",
    "\n",
    "    if wandb.run is not None:\n",
    "        run = wandb.run\n",
    "        artifact_dir = _get_dataset(run)\n",
    "    else:\n",
    "        run = wandb.init(project=project_name, job_type=\"download_dataset\")\n",
    "        artifact_dir = _get_dataset(run)\n",
    "        run.finish()\n",
    "\n",
    "    files = ls(Path(artifact_dir))\n",
    "    return files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mriccardo-spolaor94\u001b[0m (\u001b[33mai-industry\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.21"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\riccardo\\Desktop\\cloud-movement-diffusion\\wandb\\run-20230525_101152-2ei726lv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/ai-industry/ddpm_clouds/runs/2ei726lv\" target=\"_blank\">snowy-flower-17</a></strong> to <a href=\"https://wandb.ai/ai-industry/ddpm_clouds\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact np_dataset:v0, 3816.62MB. 30 files... Done. 0:0:0.1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b85a310a04d4f7298167edd9e7c9eec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">snowy-flower-17</strong>: <a href=\"https://wandb.ai/ai-industry/ddpm_clouds/runs/2ei726lv\" target=\"_blank\">https://wandb.ai/ai-industry/ddpm_clouds/runs/2ei726lv</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230525_101152-2ei726lv\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's grab 5 samples: torch.Size([5, 4, 64, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\riccardo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "files = download_dataset(DATASET_ARTIFACT, project_name=PROJECT_NAME)\n",
    "train_ds = CloudDataset(files)\n",
    "print(f\"Let's grab 5 samples: {train_ds[0:5].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3, 64, 64])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[0:3][:, :3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\riccardo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "out = model(train_ds[0:3][:,:3].to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3, 64, 64])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
