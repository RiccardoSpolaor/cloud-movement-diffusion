{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "592KgIBCtbxt"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "FsmkDobltqUd"
      },
      "outputs": [],
      "source": [
        "!pip install -q wandb tqdm matplotlib diffusers fastcore fastprogress torchmetrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "xn_pBvNhtbxv"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "def ls(path: Path):\n",
        "    \"Return files on Path, sorted\"\n",
        "    return sorted(list(path.iterdir()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DmDy1gou52FF",
        "outputId": "c0516871-3d83-420a-9023-08d4e4a0ecc1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import wandb\n",
        "wandb.login(key='6b22cbf359c5924f4500afc1ae572d6827998186', relogin=True, force=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Qzo4ppMBtbxx"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision.transforms as T\n",
        "import wandb\n",
        "from tqdm import tqdm as progress_bar\n",
        "import cv2\n",
        "\n",
        "# from cloud_diffusion.utils import ls\n",
        "\n",
        "PROJECT_NAME = \"cloud_diffusion\"\n",
        "DATASET_ARTIFACT = 'ai-industry/cloud_diffusion/SEVIR:v0'\n",
        "\n",
        "class DummyNextFrameDataset:\n",
        "    \"Dataset that returns random images\"\n",
        "    def __init__(self, num_frames=4, img_size=64, N=1000):\n",
        "        self.img_size = img_size\n",
        "        self.num_frames = num_frames\n",
        "        self.N = N\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.randn(self.num_frames, self.img_size, self.img_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.N\n",
        "\n",
        "\n",
        "class CloudDataset:\n",
        "    \"\"\"Dataset for cloud images\n",
        "    It loads numpy files from wandb artifact and stacks them into a single array\n",
        "    It also applies some transformations to the images\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 files, # list of numpy files to load (they come from the artifact)\n",
        "                 num_frames=4, # how many consecutive frames to stack\n",
        "                 scale=True, # if we images to interval [-0.5, 0.5]\n",
        "                 img_size=64, # resize dim, original images are big (446, 780)\n",
        "                 valid=False, # if True, transforms are deterministic\n",
        "                ):\n",
        "        #self.means=[]\n",
        "        #self.stds=[]\n",
        "        #tfms = [T.Normalize(self.means, self.stds)]\n",
        "        tfms = [T.RandomCrop(img_size)] if not valid else [T.CenterCrop(img_size)]\n",
        "        self.tfms = T.Compose(tfms)\n",
        "        self.load_data(files, num_frames, scale)\n",
        "\n",
        "\n",
        "    def _scale(self, arr):\n",
        "        \"Scales values of array in [0,1]\"\n",
        "        m, M = arr.min(), arr.max()\n",
        "        return (arr - m) / (M - m)\n",
        "\n",
        "    def _calculate_mean_std(self, arr):\n",
        "        \"Calculate mean and std for normalization\"\n",
        "        mean, std = arr.mean(), arr.std()\n",
        "        self.means.append(mean)\n",
        "        self.stds.append(std)\n",
        "\n",
        "    def _resize(self, arr, img_size):\n",
        "        num_events, num_frames = arr.shape[0], arr.shape[3]\n",
        "        resized_array = np.empty((num_events, img_size, img_size, num_frames))\n",
        "        for event in range(num_events):\n",
        "            resized_array[event] = cv2.resize(arr[event], (img_size, img_size))\n",
        "        return resized_array.transpose((0, 3, 1, 2))\n",
        "\n",
        "    def load_channel(self, file, scale=True):\n",
        "        one_channel = np.load(file)\n",
        "        one_channel = one_channel.astype('float32')\n",
        "        if scale:\n",
        "            one_channel = 0.5 - self._scale(one_channel)\n",
        "            # self._calculate_mean_std(one_channel)\n",
        "        return one_channel\n",
        "\n",
        "    def create_windows(self, data, num_frames):\n",
        "        windows = []\n",
        "        for event in data:\n",
        "            wds = np.lib.stride_tricks.sliding_window_view(\n",
        "                        event,\n",
        "                        num_frames,\n",
        "                        axis=0)[::num_frames].transpose(0,4,1,2,3) # (windows, frames, channels, height, width)\n",
        "            windows.append(wds)\n",
        "        windows = np.array(windows)\n",
        "        shape = windows.shape\n",
        "        windows = windows.reshape(shape[0] * shape[1], shape[2], shape[3], shape[4], shape[5])\n",
        "        windows = windows.astype('float32')\n",
        "        return windows # (batch, channels, frames, height, width)\n",
        "\n",
        "    def load_data(self, files, num_frames, scale, img_size=64):\n",
        "        \"Loads all data into a single array self.data\"\n",
        "        channels = []\n",
        "        # Load all channels\n",
        "        for file in progress_bar(files, leave=False):\n",
        "            one_channel = self.load_channel(file, scale)\n",
        "            resized_array = self._resize(one_channel, img_size)\n",
        "            channels.append(resized_array)\n",
        "            del one_channel\n",
        "        all_channels = np.stack(channels, axis=2)\n",
        "        self.data = self.create_windows(all_channels, num_frames)\n",
        "\n",
        "    def shuffle(self):\n",
        "        \"\"\"Shuffles the dataset, useful for getting\n",
        "        interesting samples on the validation dataset\"\"\"\n",
        "        idxs = torch.randperm(len(self.data))\n",
        "        self.data = self.data[idxs]\n",
        "        return self\n",
        "\n",
        "    @staticmethod\n",
        "    def _scale(arr):\n",
        "        \"Scales values of array in [0,1]\"\n",
        "        m, M = arr.min(), arr.max()\n",
        "        return (arr - m) / (M - m)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.tfms(torch.from_numpy(self.data[idx]))\n",
        "\n",
        "    def __len__(self): return len(self.data)\n",
        "\n",
        "    def save(self, fname=\"cloud_frames.npy\"):\n",
        "        np.save(fname, self.data)\n",
        "\n",
        "def download_dataset(at_name, project_name):\n",
        "    \"Downloads dataset from wandb artifact\"\n",
        "    def _get_dataset(run):\n",
        "        artifact = run.use_artifact(at_name, type='dataset')\n",
        "        return artifact.download()\n",
        "\n",
        "    if wandb.run is not None:\n",
        "        run = wandb.run\n",
        "        artifact_dir = _get_dataset(run)\n",
        "    else:\n",
        "        run = wandb.init(project=project_name, job_type=\"download_dataset\")\n",
        "        artifact_dir = _get_dataset(run)\n",
        "        run.finish()\n",
        "\n",
        "    files = ls(Path(artifact_dir))\n",
        "    return files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 324,
          "referenced_widgets": [
            "0aa8e243b7b14d23bdf8c7b39d5da28c",
            "61e80d1afedb4954818647b4c8cbcb58",
            "c8886ca6803742e086d2deb55ba177ad",
            "1b8b64a34b604a578a8ef98d2781ad49",
            "9efd1f635a91459186f79173adddc157",
            "217788d83e954f1bafa2b26d743b3823",
            "b8d6932945f24208850725a6d2269b69",
            "f393910e8a1443309d4105f4feae30dc"
          ]
        },
        "id": "IT1ZhhDFtbx0",
        "outputId": "af261c60-f13d-40a4-de37-79af00223229"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmaidacundo\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.15.4"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230702_104932-41mc34kx</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/maidacundo/cloud_diffusion/runs/41mc34kx' target=\"_blank\">clear-cherry-66</a></strong> to <a href='https://wandb.ai/maidacundo/cloud_diffusion' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/maidacundo/cloud_diffusion' target=\"_blank\">https://wandb.ai/maidacundo/cloud_diffusion</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/maidacundo/cloud_diffusion/runs/41mc34kx' target=\"_blank\">https://wandb.ai/maidacundo/cloud_diffusion/runs/41mc34kx</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact SEVIR:v0, 2756.25MB. 3 files... \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   3 of 3 files downloaded.  \n",
            "Done. 0:0:3.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0aa8e243b7b14d23bdf8c7b39d5da28c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">clear-cherry-66</strong> at: <a href='https://wandb.ai/maidacundo/cloud_diffusion/runs/41mc34kx' target=\"_blank\">https://wandb.ai/maidacundo/cloud_diffusion/runs/41mc34kx</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20230702_104932-41mc34kx/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Let's grab 5 samples: torch.Size([5, 4, 3, 64, 64])\n"
          ]
        }
      ],
      "source": [
        "files = download_dataset(DATASET_ARTIFACT, project_name=PROJECT_NAME)\n",
        "train_ds = CloudDataset(files)\n",
        "print(f\"Let's grab 5 samples: {train_ds[0:5].shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "id": "wwL8jtPAtbx1",
        "outputId": "fcfc29b1-55d8-4d81-9856-6fb1a17ed97a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f51b416fcd0>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Show the first sample image of the training dataset\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.imshow(train_ds[0][0].permute(1,2,0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oS2y3URjtbx1"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "yQfuFLG5tbx2"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "import wandb\n",
        "import fastcore.all as fc\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from diffusers import UNet2DModel\n",
        "\n",
        "\n",
        "\n",
        "def init_unet(model):\n",
        "    \"From Jeremy's bag of tricks on fastai V2 2023\"\n",
        "    for o in model.down_blocks:\n",
        "        for p in o.resnets:\n",
        "            p.conv2.weight.data.zero_()\n",
        "            for p in fc.L(o.downsamplers): nn.init.orthogonal_(p.conv.weight)\n",
        "\n",
        "    for o in model.up_blocks:\n",
        "        for p in o.resnets: p.conv2.weight.data.zero_()\n",
        "\n",
        "    model.conv_out.weight.data.zero_()\n",
        "\n",
        "class WandbModel:\n",
        "    \"A model that can be saved to wandb\"\n",
        "    @classmethod\n",
        "    def from_checkpoint(cls, model_params, checkpoint_file):\n",
        "        \"Load a UNet2D model from a checkpoint file\"\n",
        "        model = cls(**model_params)\n",
        "        print(f\"Loading model from: {checkpoint_file}\")\n",
        "        model.load_state_dict(torch.load(checkpoint_file))\n",
        "        return model\n",
        "\n",
        "    @classmethod\n",
        "    def from_artifact(cls, model_params, artifact_name):\n",
        "        \"Load a UNet2D model from a wandb.Artifact, need to be run in a wandb run\"\n",
        "        artifact = wandb.use_artifact(artifact_name, type='model')\n",
        "        artifact_dir = Path(artifact.download())\n",
        "        chpt_file = list(artifact_dir.glob(\"*.pth\"))[0]\n",
        "        return cls.from_checkpoint(model_params, chpt_file)\n",
        "\n",
        "def get_unet_params(model_name=\"unet_small\", num_frames=4*3): # TODO parametrize the num frames and num channels\n",
        "    \"Return the parameters for the diffusers UNet2d model\"\n",
        "    if model_name == \"unet_small\":\n",
        "        return dict(\n",
        "            block_out_channels=(16, 32, 64, 128), # number of channels for each block\n",
        "            norm_num_groups=8, # number of groups for the normalization layer\n",
        "            in_channels=num_frames, # number of input channels\n",
        "            out_channels=1, # number of output channels\n",
        "            )\n",
        "    elif model_name == \"unet_big\":\n",
        "        return dict(\n",
        "            block_out_channels=(32, 64, 128, 256), # number of channels for each block\n",
        "            norm_num_groups=8, # number of groups for the normalization layer\n",
        "            in_channels=num_frames, # number of input channels\n",
        "            out_channels=1, # number of output channels\n",
        "            )\n",
        "    else:\n",
        "        raise(f\"Model name not found: {model_name}, choose between 'unet_small' or 'unet_big'\")\n",
        "\n",
        "class UNet2D(UNet2DModel, WandbModel):\n",
        "    def __init__(self, *x, **kwargs):\n",
        "        super().__init__(*x, **kwargs)\n",
        "        init_unet(self)\n",
        "\n",
        "    def forward(self, *x, **kwargs):\n",
        "        return super().forward(*x, **kwargs).sample ## Diffusers's UNet2DOutput class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "rJzdvyzMx-MI"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "import wandb\n",
        "import numpy as np\n",
        "\n",
        "## For Training\n",
        "\n",
        "def to_wandb_image(img):\n",
        "    \"Convert a tensor to a wandb.Image\"\n",
        "    return wandb.Image(torch.cat(img.split(1), dim=-1).cpu().numpy())\n",
        "\n",
        "def log_images(xt, samples):\n",
        "    \"Log sampled images to wandb\"\n",
        "    device = samples.device\n",
        "    frames = torch.cat([xt[:, :-1,...].to(device), samples], dim=1)\n",
        "    wandb.log({\"sampled_images\": [to_wandb_image(img) for img in frames]})\n",
        "\n",
        "def save_model(model, model_name):\n",
        "    \"Save the model to wandb\"\n",
        "    model_name = f\"{wandb.run.id}_{model_name}\"\n",
        "    models_folder = Path(\"models\")\n",
        "    if not models_folder.exists():\n",
        "        models_folder.mkdir()\n",
        "    torch.save(model.state_dict(), models_folder/f\"{model_name}.pth\")\n",
        "    at = wandb.Artifact(model_name, type=\"model\")\n",
        "    at.add_file(f\"models/{model_name}.pth\")\n",
        "    wandb.log_artifact(at)\n",
        "\n",
        "\n",
        "## For Inference\n",
        "def htile(img):\n",
        "    \"Horizontally tile a batch of images.\"\n",
        "    return torch.cat(img.split(1), dim=-1)\n",
        "\n",
        "def vtile(img):\n",
        "    \"Vertically tile a batch of images.\"\n",
        "    return torch.cat(img.split(1), dim=-2)\n",
        "\n",
        "def vhtile(*imgs):\n",
        "    \"Vertically and horizontally tile a batch of images.\"\n",
        "    return vtile(torch.cat([htile(img) for img in imgs], dim=0))\n",
        "\n",
        "def scale(arr):\n",
        "    \"Scales values of array in [0,1]\"\n",
        "    m, M = arr.min(), arr.max()\n",
        "    return (arr - m) / (M - m)\n",
        "\n",
        "def preprocess_frames(data):\n",
        "    \"Preprocess frames for wandb.Video\"\n",
        "    sdata = scale(data.squeeze())\n",
        "    # print(sdata.shape)\n",
        "    def tfm(frame):\n",
        "        rframe = 255 * frame\n",
        "        return rframe.cpu().numpy().astype(np.uint8)\n",
        "    return [tfm(frame) for frame in sdata]\n",
        "\n",
        "def to_video(data):\n",
        "    \"create wandb.Video container\"\n",
        "    frames = preprocess_frames(data)\n",
        "    vid = np.stack(frames)[:, None, ...]\n",
        "    return wandb.Video(vid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "rl-EEZoD52FN"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "class mCSI(nn.Module):\n",
        "    \"\"\"Compute the critical success index (CSI) score.\"\"\"\n",
        "    def __init__(self, thresholds: List[float] = [ 16., 74., 133. ], eps: float = 1e-4) -> None:\n",
        "        super().__init__()\n",
        "        self.thresholds = thresholds\n",
        "        self.eps = eps\n",
        "\n",
        "    @staticmethod\n",
        "    def _threshold(y_true: torch.FloatTensor, y_pred: torch.FloatTensor, threshold: float) -> torch.FloatTensor:\n",
        "        \"\"\"Apply a threshold to both the target and the prediction tensors.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        y_true : FloatTensor\n",
        "            The target tensor.\n",
        "        y_pred : FloatTensor\n",
        "            The prediction tensor.\n",
        "        threshold : float\n",
        "            The threshold to apply.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        FloatTensor\n",
        "            The thresholded target tensor.\n",
        "        FloatTensor\n",
        "            The thresholded prediction tensor.\n",
        "        \"\"\"\n",
        "\n",
        "        y_true_res = (y_true >= threshold).float()\n",
        "        y_pred_res = (y_pred >= threshold).float()\n",
        "\n",
        "        is_nan = torch.isnan(y_true) | torch.isnan(y_pred)\n",
        "\n",
        "        y_true_res[is_nan] = 0\n",
        "        y_pred_res[is_nan] = 0\n",
        "\n",
        "        return y_true_res, y_pred_res\n",
        "\n",
        "    def forward(self, pred, target):\n",
        "        \"\"\"\n",
        "        Compute the critical success index (CSI) score.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        pred, target:   torch.Tensor\n",
        "            shape = (batch_size, seq_len, height, width)\n",
        "        \"\"\"\n",
        "        results = 0.\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for thresh in self.thresholds:\n",
        "                target, pred = self._threshold(target, pred, thresh)\n",
        "                hits = torch.sum(target * pred, dim=(-2, -1)).int()\n",
        "                misses = torch.sum(target * (1 - pred), dim=(-2, -1)).int()\n",
        "                fas = torch.sum((1 - target) * pred, dim=(-2, -1)).int()\n",
        "                csi = hits / (hits + misses + fas + self.eps)\n",
        "                results += csi.mean()\n",
        "\n",
        "        return results / len(self.thresholds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "z1l_534VxxaQ",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import random, argparse\n",
        "from pathlib import Path\n",
        "\n",
        "import wandb\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.optim import AdamW\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim.lr_scheduler import OneCycleLR\n",
        "from torch.utils.data.dataloader import default_collate\n",
        "from torchmetrics import PeakSignalNoiseRatio, StructuralSimilarityIndexMeasure\n",
        "\n",
        "from fastprogress import progress_bar\n",
        "\n",
        "\n",
        "def noisify_last_frame(frames, noise_func):\n",
        "    \"Noisify the last frame of a sequence\"\n",
        "    past_frames = frames[:,:-1]\n",
        "    last_frame  = frames[:,-1:]\n",
        "    past_frames = past_frames.reshape(128, -1, 64, 64)\n",
        "    noise, t, e = noise_func(last_frame)\n",
        "    noise = noise[:,:,0,:,:]\n",
        "    return torch.cat([past_frames, noise], dim=1), t, e\n",
        "\n",
        "def noisify_collate(noise_func):\n",
        "    def _inner(b):\n",
        "        \"Collate function that noisifies the last frame\"\n",
        "        return noisify_last_frame(default_collate(b), noise_func)\n",
        "    return _inner\n",
        "\n",
        "class NoisifyDataloader(DataLoader):\n",
        "    \"\"\"Noisify the last frame of a dataloader by applying\n",
        "    a noise function, after collating the batch\"\"\"\n",
        "    def __init__(self, dataset, *args, noise_func=None, **kwargs):\n",
        "        super().__init__(dataset, *args, collate_fn=noisify_collate(noise_func), **kwargs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "XfTZnkBpyM-O"
      },
      "outputs": [],
      "source": [
        "from functools import partial\n",
        "\n",
        "import torch\n",
        "from fastprogress import progress_bar\n",
        "\n",
        "from diffusers.schedulers import DDIMScheduler\n",
        "\n",
        "\n",
        "## DDPM params\n",
        "## From fastai V2 Course DDPM notebooks\n",
        "betamin,betamax,n_steps = 0.0001,0.02, 1000\n",
        "beta = torch.linspace(betamin, betamax, n_steps)\n",
        "alpha = 1.-beta\n",
        "alphabar = alpha.cumprod(dim=0)\n",
        "sigma = beta.sqrt()\n",
        "\n",
        "def noisify_ddpm(x0):\n",
        "    \"Noise by ddpm\"\n",
        "    device = x0.device\n",
        "    n = len(x0)\n",
        "    t = torch.randint(0, n_steps, (n,), dtype=torch.long)\n",
        "    ε = torch.randn(x0.shape, device=device)\n",
        "    ᾱ_t = alphabar[t].reshape(-1, 1, 1, 1, 1).to(device)\n",
        "    xt = ᾱ_t.sqrt()*x0 + (1-ᾱ_t).sqrt()*ε\n",
        "    return xt, t.to(device), ε\n",
        "\n",
        "@torch.no_grad()\n",
        "def diffusers_sampler(model, past_frames, sched, **kwargs):\n",
        "    \"Using Diffusers built-in samplers\"\n",
        "    model.eval()\n",
        "    device = next(model.parameters()).device\n",
        "    new_frame = torch.randn_like(past_frames[:,-1:], dtype=past_frames.dtype, device=device)\n",
        "    preds = []\n",
        "    pbar = progress_bar(sched.timesteps, leave=False)\n",
        "    for t in pbar:\n",
        "        pbar.comment = f\"DDIM Sampler: frame {t}\"\n",
        "        noise = model(torch.cat([past_frames, new_frame], dim=1), t)\n",
        "        new_frame = sched.step(noise, t, new_frame, **kwargs).prev_sample\n",
        "        preds.append(new_frame.float().cpu())\n",
        "    return preds[-1]\n",
        "\n",
        "def ddim_sampler(steps=350, eta=1.):\n",
        "    \"DDIM sampler, faster and a bit better than the built-in sampler\"\n",
        "    ddim_sched = DDIMScheduler()\n",
        "    ddim_sched.set_timesteps(steps)\n",
        "    return partial(diffusers_sampler, sched=ddim_sched, eta=eta)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "psosQcqNxxMY"
      },
      "outputs": [],
      "source": [
        "class MiniTrainer:\n",
        "    \"A mini trainer for the diffusion process\"\n",
        "    def __init__(self,\n",
        "                 train_dataloader,\n",
        "                 valid_dataloader,\n",
        "                 model,\n",
        "                 sampler,\n",
        "                 device=\"cuda\",\n",
        "                 loss_func=nn.MSELoss(),\n",
        "                 n_predicted=4,\n",
        "                 ):\n",
        "        self.train_dataloader = train_dataloader\n",
        "        self.valid_dataloader = valid_dataloader\n",
        "        self.loss_func = loss_func\n",
        "        self.psnr = PeakSignalNoiseRatio().to(device)\n",
        "        self.ssim = StructuralSimilarityIndexMeasure().to(device)\n",
        "        self.m_csi = mCSI().to(device)\n",
        "        self.model = model.to(device)\n",
        "        self.scaler = torch.cuda.amp.GradScaler()\n",
        "        self.device = device\n",
        "        self.sampler = sampler\n",
        "        self.val_batch = next(iter(valid_dataloader))\n",
        "        self.n_predicted = n_predicted\n",
        "\n",
        "    def train_step(self, loss):\n",
        "        \"Train for one step\"\n",
        "        self.optimizer.zero_grad()\n",
        "        self.scaler.scale(loss).backward()\n",
        "        self.scaler.step(self.optimizer)\n",
        "        self.scaler.update()\n",
        "        self.scheduler.step()\n",
        "\n",
        "    def one_epoch(self, epoch=None):\n",
        "        \"Train for one epoch, log metrics and save model\"\n",
        "        self.model.train()\n",
        "        pbar = progress_bar(self.train_dataloader, leave=False)\n",
        "        for batch in pbar:\n",
        "            frames, t, noise = to_device(batch, device=self.device)\n",
        "            with torch.autocast(\"cuda\"):\n",
        "                print(frames.shape)\n",
        "                print(noise.shape)\n",
        "                predicted_noise = self.model(frames, t)\n",
        "                loss = self.loss_func(noise, predicted_noise)\n",
        "            self.train_step(loss)\n",
        "            wandb.log({\"train_mse\": loss.item(),\n",
        "                       \"learning_rate\": self.scheduler.get_last_lr()[0]})\n",
        "            pbar.comment = f\"epoch={epoch}, MSE={loss.item():2.3f}\"\n",
        "\n",
        "    def one_epoch_validation(self, epoch=None):\n",
        "        \"Validates on val set\"\n",
        "        pbar = progress_bar(self.valid_dataloader, leave=False)\n",
        "        psnr_metric = 0\n",
        "        mse_metric = 0\n",
        "        ssmi_metric = 0\n",
        "        m_csi_metric = 0\n",
        "        for val_batch in pbar:\n",
        "            frames = val_batch[0].to(self.device)\n",
        "            target = frames[:,-self.n_predicted].unsqueeze(1)\n",
        "            past_frames=frames[:,:-self.n_predicted]\n",
        "            samples = []\n",
        "            for _ in range(self.n_predicted):\n",
        "              print('prima',past_frames.shape)\n",
        "              prediction_frames = self.sampler(self.model, past_frames=past_frames).to(self.device)\n",
        "              past_frames = torch.cat([past_frames, prediction_frames], dim=1)\n",
        "              past_frames = past_frames[:,1:]\n",
        "              print('dopo',past_frames.shape)\n",
        "\n",
        "            psnr_metric += self.psnr(past_frames, target).float().cpu()\n",
        "            ssmi_metric += self.ssim(past_frames, target).float().cpu()\n",
        "            mse_metric += self.loss_func(past_frames, target).float().cpu()\n",
        "            m_csi_metric += self.m_csi(past_frames, target).float().cpu()\n",
        "        psnr_metric = psnr_metric / len(self.valid_dataloader)\n",
        "        ssmi_metric = ssmi_metric / len(self.valid_dataloader)\n",
        "        mse_metric = mse_metric / len(self.valid_dataloader)\n",
        "        m_csi_metric = m_csi_metric / len(self.valid_dataloader)\n",
        "        wandb.log({\"val_psnr\": psnr_metric,\n",
        "                   \"val_ssmi\": ssmi_metric,\n",
        "                   \"val_mse\": mse_metric,\n",
        "                   \"val_m_csi\": m_csi_metric})\n",
        "\n",
        "    def prepare(self, config):\n",
        "        wandb.config.update(config)\n",
        "        config.total_train_steps = config.epochs * len(self.train_dataloader)\n",
        "        self.optimizer = AdamW(self.model.parameters(), lr=config.lr, eps=1e-5)\n",
        "        self.scheduler = OneCycleLR(self.optimizer, max_lr=config.lr, total_steps=config.total_train_steps)\n",
        "\n",
        "    def fit(self, config):\n",
        "        self.prepare(config)\n",
        "        val_past_frames, _, _ = to_device(self.val_batch, device=self.device)\n",
        "        val_past_frames = val_past_frames[:min(config.n_preds, 1)]  # log first prediction\n",
        "        for epoch in progress_bar(range(config.epochs), total=config.epochs, leave=True):\n",
        "            self.one_epoch(epoch)\n",
        "            if config.validate_epochs:\n",
        "                self.one_epoch_validation(epoch)\n",
        "\n",
        "            # log predictions:\n",
        "            if epoch % config.log_every_epoch == 0:\n",
        "                samples = self.sampler(self.model, past_frames=val_past_frames)\n",
        "                print('samples:', samples.shape)\n",
        "                import matplotlib.pyplot as plt\n",
        "                axs = plt.subplots(2, 4, figsize=(10, 10))[1].ravel()\n",
        "                i=0\n",
        "                for ax in axs:\n",
        "                    if i < 4:\n",
        "                        ax.imshow(val_past_frames[0,i,0].cpu())\n",
        "                    else:\n",
        "                        ax.imshow(samples[0,i-4].detach().cpu())\n",
        "                    ax.axis('off')\n",
        "                    i+=1\n",
        "                plt.tight_layout()\n",
        "                plt.show()\n",
        "                # self.one_epoch_validation(epoch)\n",
        "                log_images(self.val_batch, samples)\n",
        "\n",
        "        save_model(self.model, config.model_name)\n",
        "\n",
        "\n",
        "def set_seed(s, reproducible=False):\n",
        "    \"Set random seed for `random`, `torch`, and `numpy` (where available)\"\n",
        "    try: torch.manual_seed(s)\n",
        "    except NameError: pass\n",
        "    try: torch.cuda.manual_seed_all(s)\n",
        "    except NameError: pass\n",
        "    try: np.random.seed(s%(2**32-1))\n",
        "    except NameError: pass\n",
        "    random.seed(s)\n",
        "    if reproducible:\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "\n",
        "def to_device(t, device=\"cpu\"):\n",
        "    if isinstance(t, (tuple, list)):\n",
        "        return [_t.to(device) for _t in t]\n",
        "    elif isinstance(t, torch.Tensor):\n",
        "        return t.to(device)\n",
        "    else:\n",
        "        raise(\"Not a Tensor or list of Tensors\")\n",
        "\n",
        "\n",
        "def ls(path: Path):\n",
        "    \"Return files on Path, sorted\"\n",
        "    return sorted(list(path.iterdir()))\n",
        "\n",
        "\n",
        "def parse_args(config):\n",
        "    \"A brute force way to parse arguments, it is probably not a good idea to use it\"\n",
        "    parser = argparse.ArgumentParser(description='Run training baseline')\n",
        "    for k,v in config.__dict__.items():\n",
        "        parser.add_argument('--'+k, type=type(v), default=v)\n",
        "    args = vars(parser.parse_args())\n",
        "\n",
        "    # update config with parsed args\n",
        "    for k, v in args.items():\n",
        "        setattr(config, k, v)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "-uNhPHz3wpqD"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "from types import SimpleNamespace\n",
        "\n",
        "import wandb\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# from cloud_diffusion.dataset import download_dataset, CloudDataset\n",
        "#from cloud_diffusion.utils import NoisifyDataloader, MiniTrainer, set_seed, parse_args\n",
        "#from cloud_diffusion.ddpm import noisify_ddpm, ddim_sampler\n",
        "# from cloud_diffusion.models import UNet2D, get_unet_params\n",
        "\n",
        "\n",
        "PROJECT_NAME = \"cloud_diffusion\"\n",
        "DATASET_ARTIFACT = 'ai-industry/cloud_diffusion/SEVIR:v0'\n",
        "\n",
        "config = SimpleNamespace(\n",
        "    epochs=50, # number of epochs\n",
        "    model_name=\"unet_small\", # model name to save [unet_small, unet_big]\n",
        "    strategy=\"ddpm\", # strategy to use ddpm\n",
        "    noise_steps=1000, # number of noise steps on the diffusion process\n",
        "    sampler_steps=333, # number of sampler steps on the diffusion process\n",
        "    seed=42, # random seed\n",
        "    batch_size=128, # batch size\n",
        "    img_size=64, # image size\n",
        "    device=\"cuda\", # device\n",
        "    num_workers=0, # number of workers for dataloader\n",
        "    num_frames_train=4, # number of frames to use as input\n",
        "    num_frames_val=6,\n",
        "    lr=5e-4, # learning rate\n",
        "    validation_days=3, # number of days to use for validation\n",
        "    log_every_epoch=5, # log every n epochs to wandb\n",
        "    n_preds=8, # number of predictions to make\n",
        "    )\n",
        "\n",
        "def train_func(config):\n",
        "    config.model_params = get_unet_params(config.model_name, 10)\n",
        "\n",
        "    set_seed(config.seed)\n",
        "    device = torch.device(config.device)\n",
        "\n",
        "    # downlaod the dataset from the wandb.Artifact\n",
        "    files = download_dataset(DATASET_ARTIFACT, PROJECT_NAME)\n",
        "    # TODO divide train and validation files in SEVIR dataset\n",
        "    # train_days, valid_days = files[:-config.validation_days], files[-config.validation_days:]\n",
        "    train_ds = CloudDataset(files=files, num_frames=config.num_frames_train, img_size=config.img_size).shuffle()\n",
        "    valid_ds = CloudDataset(files=files, num_frames=config.num_frames_val, img_size=config.img_size)\n",
        "\n",
        "    # DDPM dataloaders\n",
        "    train_dataloader = NoisifyDataloader(train_ds, config.batch_size, shuffle=True,\n",
        "                                         noise_func=noisify_ddpm,  num_workers=config.num_workers)\n",
        "    valid_dataloader = NoisifyDataloader(valid_ds, config.batch_size, shuffle=False,\n",
        "                                          noise_func=noisify_ddpm,  num_workers=config.num_workers)\n",
        "\n",
        "    # model setup\n",
        "    model = UNet2D(**config.model_params)\n",
        "\n",
        "    # sampler\n",
        "    sampler = ddim_sampler(steps=config.sampler_steps)\n",
        "\n",
        "    # A simple training loop\n",
        "    trainer = MiniTrainer(train_dataloader, valid_dataloader, model, sampler, device)\n",
        "    trainer.fit(config)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config.model_params = get_unet_params(config.model_name, 10)\n",
        "\n",
        "set_seed(config.seed)\n",
        "device = torch.device(config.device)\n",
        "\n",
        "# downlaod the dataset from the wandb.Artifact\n",
        "files = download_dataset(DATASET_ARTIFACT, PROJECT_NAME)\n",
        "# TODO divide train and validation files in SEVIR dataset\n",
        "# train_days, valid_days = files[:-config.validation_days], files[-config.validation_days:]\n",
        "train_ds = CloudDataset(files=files, num_frames=config.num_frames_train, img_size=config.img_size).shuffle()\n",
        "valid_ds = CloudDataset(files=files, num_frames=config.num_frames_val, img_size=config.img_size)"
      ],
      "metadata": {
        "id": "qHVis0UQHLk2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config.model_params = get_unet_params(config.model_name, 10)"
      ],
      "metadata": {
        "id": "QBDNyt1_RYS1"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DDPM dataloaders\n",
        "train_dataloader = NoisifyDataloader(train_ds, config.batch_size, shuffle=True,\n",
        "                                      noise_func=noisify_ddpm,  num_workers=config.num_workers)\n",
        "valid_dataloader = NoisifyDataloader(valid_ds, config.batch_size, shuffle=False,\n",
        "                                      noise_func=noisify_ddpm,  num_workers=config.num_workers)\n",
        "\n",
        "# model setup\n",
        "model = UNet2D(**config.model_params)\n",
        "\n",
        "# sampler\n",
        "sampler = ddim_sampler(steps=config.sampler_steps)\n",
        "\n",
        "# A simple training loop\n",
        "trainer = MiniTrainer(train_dataloader, valid_dataloader, model, sampler, device)"
      ],
      "metadata": {
        "id": "c8j7giH3QzcB"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = MiniTrainer(train_dataloader, valid_dataloader, model, sampler, device)\n",
        "with wandb.init(project=PROJECT_NAME, entity='ai-industry', config=config, tags=[\"ddpm\", config.model_name]):\n",
        "  trainer.fit(config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "29a67b76bd144a98b58acc2958dc66d6",
            "be91eff8a6f84f0590e7cbb76be958eb",
            "4428a9c3e3de4ab5ac231ea46bb6cd97",
            "bd055822f21e4cb2b81de8e0e0cf5002",
            "6e7e824133344f4189f66b87bb21fc27",
            "fc58932d2b024ab7aa923aaaed5938c9",
            "a97240f88cc14e328417f94802442ae2",
            "bfa6f9a2c6914d7e984f3651fd26171a"
          ]
        },
        "id": "RGFoNcYpHNVq",
        "outputId": "94105446-9c4d-4d2f-a405-4e9ae6f1b1c3"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.15.4"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230702_110331-fq4pydy7</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/ai-industry/cloud_diffusion/runs/fq4pydy7' target=\"_blank\">bright-spaceship-146</a></strong> to <a href='https://wandb.ai/ai-industry/cloud_diffusion' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/ai-industry/cloud_diffusion' target=\"_blank\">https://wandb.ai/ai-industry/cloud_diffusion</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/ai-industry/cloud_diffusion/runs/fq4pydy7' target=\"_blank\">https://wandb.ai/ai-industry/cloud_diffusion/runs/fq4pydy7</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      <progress value='0' class='' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      0.00% [0/50 00:00&lt;?]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([128, 10, 64, 64])\n",
            "torch.Size([128, 1, 3, 64, 64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([128, 1, 64, 64])) that is different to the input size (torch.Size([128, 1, 3, 64, 64])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([128, 10, 64, 64])\n",
            "torch.Size([128, 1, 3, 64, 64])\n",
            "torch.Size([128, 10, 64, 64])\n",
            "torch.Size([128, 1, 3, 64, 64])\n",
            "torch.Size([128, 10, 64, 64])\n",
            "torch.Size([128, 1, 3, 64, 64])\n",
            "torch.Size([128, 10, 64, 64])\n",
            "torch.Size([128, 1, 3, 64, 64])\n",
            "torch.Size([128, 10, 64, 64])\n",
            "torch.Size([128, 1, 3, 64, 64])\n",
            "torch.Size([128, 10, 64, 64])\n",
            "torch.Size([128, 1, 3, 64, 64])\n",
            "torch.Size([128, 10, 64, 64])\n",
            "torch.Size([128, 1, 3, 64, 64])\n",
            "torch.Size([128, 10, 64, 64])\n",
            "torch.Size([128, 1, 3, 64, 64])\n",
            "torch.Size([128, 10, 64, 64])\n",
            "torch.Size([128, 1, 3, 64, 64])\n",
            "torch.Size([128, 10, 64, 64])\n",
            "torch.Size([128, 1, 3, 64, 64])\n",
            "torch.Size([128, 10, 64, 64])\n",
            "torch.Size([128, 1, 3, 64, 64])\n",
            "torch.Size([128, 10, 64, 64])\n",
            "torch.Size([128, 1, 3, 64, 64])\n",
            "torch.Size([128, 10, 64, 64])\n",
            "torch.Size([128, 1, 3, 64, 64])\n",
            "torch.Size([128, 10, 64, 64])\n",
            "torch.Size([128, 1, 3, 64, 64])\n",
            "torch.Size([128, 10, 64, 64])\n",
            "torch.Size([128, 1, 3, 64, 64])\n",
            "torch.Size([128, 10, 64, 64])\n",
            "torch.Size([128, 1, 3, 64, 64])\n",
            "torch.Size([128, 10, 64, 64])\n",
            "torch.Size([128, 1, 3, 64, 64])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:red\">(failed 1).</strong> Press Control-C to abort syncing."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "29a67b76bd144a98b58acc2958dc66d6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>learning_rate</td><td>▁▁▁▁▂▂▂▂▃▃▄▄▅▅▆▇▇█</td></tr><tr><td>train_mse</td><td>█▆▆▆▃▅▄▁▆▅▅▁▂▁▆▅▆▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>learning_rate</td><td>2e-05</td></tr><tr><td>train_mse</td><td>1.00304</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">bright-spaceship-146</strong> at: <a href='https://wandb.ai/ai-industry/cloud_diffusion/runs/fq4pydy7' target=\"_blank\">https://wandb.ai/ai-industry/cloud_diffusion/runs/fq4pydy7</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20230702_110331-fq4pydy7/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-98a9b70a5ceb>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMiniTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproject\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPROJECT_NAME\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ai-industry'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ddpm\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-12-6f334b4d4df5>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mval_past_frames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_past_frames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_preds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# log first prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprogress_bar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate_epochs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_epoch_validation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-6f334b4d4df5>\u001b[0m in \u001b[0;36mone_epoch\u001b[0;34m(self, epoch)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mpbar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprogress_bar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpbar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m             \u001b[0mframes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/fastprogress/fastprogress.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_interrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/fastprogress/fastprogress.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 633\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    634\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 677\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    678\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-21-342b65566e0e>\u001b[0m in \u001b[0;36m_inner\u001b[0;34m(b)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_inner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;34m\"Collate function that noisifies the last frame\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnoisify_last_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoise_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_inner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-342b65566e0e>\u001b[0m in \u001b[0;36mnoisify_last_frame\u001b[0;34m(frames, noise_func)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mpast_frames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mframes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mlast_frame\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mframes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mpast_frames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpast_frames\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mnoise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnoise_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_frame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mnoise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnoise\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: shape '[128, -1, 64, 64]' is invalid for input of size 3538944"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0aa8e243b7b14d23bdf8c7b39d5da28c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_61e80d1afedb4954818647b4c8cbcb58",
              "IPY_MODEL_c8886ca6803742e086d2deb55ba177ad"
            ],
            "layout": "IPY_MODEL_1b8b64a34b604a578a8ef98d2781ad49"
          }
        },
        "61e80d1afedb4954818647b4c8cbcb58": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9efd1f635a91459186f79173adddc157",
            "placeholder": "​",
            "style": "IPY_MODEL_217788d83e954f1bafa2b26d743b3823",
            "value": "0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\r"
          }
        },
        "c8886ca6803742e086d2deb55ba177ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b8d6932945f24208850725a6d2269b69",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f393910e8a1443309d4105f4feae30dc",
            "value": 1
          }
        },
        "1b8b64a34b604a578a8ef98d2781ad49": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9efd1f635a91459186f79173adddc157": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "217788d83e954f1bafa2b26d743b3823": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b8d6932945f24208850725a6d2269b69": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f393910e8a1443309d4105f4feae30dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "29a67b76bd144a98b58acc2958dc66d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_be91eff8a6f84f0590e7cbb76be958eb",
              "IPY_MODEL_4428a9c3e3de4ab5ac231ea46bb6cd97"
            ],
            "layout": "IPY_MODEL_bd055822f21e4cb2b81de8e0e0cf5002"
          }
        },
        "be91eff8a6f84f0590e7cbb76be958eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6e7e824133344f4189f66b87bb21fc27",
            "placeholder": "​",
            "style": "IPY_MODEL_fc58932d2b024ab7aa923aaaed5938c9",
            "value": "0.001 MB of 0.012 MB uploaded (0.000 MB deduped)\r"
          }
        },
        "4428a9c3e3de4ab5ac231ea46bb6cd97": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a97240f88cc14e328417f94802442ae2",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bfa6f9a2c6914d7e984f3651fd26171a",
            "value": 0.11905726087308657
          }
        },
        "bd055822f21e4cb2b81de8e0e0cf5002": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6e7e824133344f4189f66b87bb21fc27": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fc58932d2b024ab7aa923aaaed5938c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a97240f88cc14e328417f94802442ae2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bfa6f9a2c6914d7e984f3651fd26171a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}