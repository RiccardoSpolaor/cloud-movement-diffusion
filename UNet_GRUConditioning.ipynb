{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "592KgIBCtbxt"
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "xn_pBvNhtbxv"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def ls(path: Path): \n",
    "    \"Return files on Path, sorted\"\n",
    "    return sorted(list(path.iterdir()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FsmkDobltqUd",
    "outputId": "4143b299-8d85-4344-e790-b7f154cb5eda"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q wandb tqdm matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login(key='6b22cbf359c5924f4500afc1ae572d6827998186', relogin=True, force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Qzo4ppMBtbxx"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "import wandb\n",
    "from tqdm import tqdm as progress_bar\n",
    "\n",
    "# from cloud_diffusion.utils import ls\n",
    "\n",
    "PROJECT_NAME = \"cloud_diffusion\"\n",
    "DATASET_ARTIFACT = 'maidacundo/cloud_diffusion/sample_dataset:v0'\n",
    "\n",
    "class DummyNextFrameDataset:\n",
    "    \"Dataset that returns random images\"\n",
    "    def __init__(self, num_frames=4, img_size=64, N=1000):\n",
    "        self.img_size = img_size\n",
    "        self.num_frames = num_frames\n",
    "        self.N = N\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.randn(self.num_frames, self.img_size, self.img_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.N\n",
    "\n",
    "\n",
    "class CloudDataset:\n",
    "    \"\"\"Dataset for cloud images\n",
    "    It loads numpy files from wandb artifact and stacks them into a single array\n",
    "    It also applies some transformations to the images\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 files, # list of numpy files to load (they come from the artifact)\n",
    "                 num_frames=4, # how many consecutive frames to stack\n",
    "                 scale=True, # if we images to interval [-0.5, 0.5]\n",
    "                 img_size=64, # resize dim, original images are big (446, 780)\n",
    "                 valid=False, # if True, transforms are deterministic\n",
    "                ):\n",
    "        \n",
    "        tfms = [T.Resize((img_size, int(img_size*1.7)), antialias=True)] if img_size is not None else []\n",
    "        tfms += [T.RandomCrop(img_size)] if not valid else [T.CenterCrop(img_size)]\n",
    "        self.tfms = T.Compose(tfms)\n",
    "        self.load_data(files, num_frames, scale)\n",
    "        \n",
    "    def load_day(self, file, scale=True):\n",
    "        one_day = np.load(file)\n",
    "        one_day = one_day.astype('float32')\n",
    "        if scale:\n",
    "            one_day = 0.5 - self._scale(one_day)\n",
    "        return one_day\n",
    "\n",
    "    def load_data(self, files, num_frames, scale):\n",
    "        \"Loads all data into a single array self.data\"\n",
    "        data = []\n",
    "        # TODO: download all files\n",
    "        for file in progress_bar(files, leave=False):\n",
    "            one_day = self.load_day(file, scale)\n",
    "            wds = np.lib.stride_tricks.sliding_window_view(\n",
    "                one_day.squeeze(),\n",
    "                num_frames,\n",
    "                axis=0).transpose((0,3,1,2))\n",
    "            data.append(wds)\n",
    "            # pbar.comment = f\"Creating CloudDataset from {file}\"\n",
    "        self.data = np.concatenate(data, axis=0)\n",
    "\n",
    "    def shuffle(self):\n",
    "        \"\"\"Shuffles the dataset, useful for getting \n",
    "        interesting samples on the validation dataset\"\"\"\n",
    "        idxs = torch.randperm(len(self.data))\n",
    "        self.data = self.data[idxs]\n",
    "        return self\n",
    "\n",
    "    @staticmethod\n",
    "    def _scale(arr):\n",
    "        \"Scales values of array in [0,1]\"\n",
    "        m, M = arr.min(), arr.max()\n",
    "        return (arr - m) / (M - m)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.tfms(torch.from_numpy(self.data[idx]))\n",
    "    \n",
    "    def __len__(self): return len(self.data)\n",
    "\n",
    "    def save(self, fname=\"cloud_frames.npy\"):\n",
    "        np.save(fname, self.data)\n",
    "\n",
    "\n",
    "class CloudDatasetInference(CloudDataset):\n",
    "     def load_data(self, files, num_frames=None, scale=None):\n",
    "        \"Loads all data into a single array self.data\"\n",
    "        data = []\n",
    "        max_length = 100\n",
    "        # TODO: download everything\n",
    "        for file in files:\n",
    "            one_day = self.load_day(file, scale)\n",
    "            data.append(one_day)\n",
    "            max_length = min(max_length, len(one_day))\n",
    "        self.data = np.stack([d[:max_length] for d in data], axis=0).squeeze()\n",
    "\n",
    "\n",
    "def download_dataset(at_name, project_name):\n",
    "    \"Downloads dataset from wandb artifact\"\n",
    "    def _get_dataset(run):\n",
    "        artifact = run.use_artifact(at_name, type='dataset')\n",
    "        return artifact.download()\n",
    "\n",
    "    if wandb.run is not None:\n",
    "        run = wandb.run\n",
    "        artifact_dir = _get_dataset(run)\n",
    "    else:\n",
    "        run = wandb.init(project=project_name, job_type=\"download_dataset\")\n",
    "        artifact_dir = _get_dataset(run)\n",
    "        run.finish()\n",
    "\n",
    "    files = ls(Path(artifact_dir))\n",
    "    return files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 315
    },
    "id": "IT1ZhhDFtbx0",
    "outputId": "3ed5b231-6c5e-42e9-c056-e7f13ce4c7c1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmaidacundo\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/wandb/run-20230525_203259-j5quy62o</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/maidacundo/cloud_diffusion/runs/j5quy62o' target=\"_blank\">glorious-aardvark-11</a></strong> to <a href='https://wandb.ai/maidacundo/cloud_diffusion' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/maidacundo/cloud_diffusion' target=\"_blank\">https://wandb.ai/maidacundo/cloud_diffusion</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/maidacundo/cloud_diffusion/runs/j5quy62o' target=\"_blank\">https://wandb.ai/maidacundo/cloud_diffusion/runs/j5quy62o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact sample_dataset:v0, 360.00MB. 30 files... \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   30 of 30 files downloaded.  \n",
      "Done. 0:0:0.7\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d92a8fa8b24143f8a771fdfe5519bf03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">glorious-aardvark-11</strong> at: <a href='https://wandb.ai/maidacundo/cloud_diffusion/runs/j5quy62o' target=\"_blank\">https://wandb.ai/maidacundo/cloud_diffusion/runs/j5quy62o</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230525_203259-j5quy62o/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's grab 5 samples: torch.Size([5, 4, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "files = download_dataset(DATASET_ARTIFACT, project_name=PROJECT_NAME)\n",
    "train_ds = CloudDataset(files)\n",
    "print(f\"Let's grab 5 samples: {train_ds[0:5].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H7dLIET0tbx0",
    "outputId": "714928ed-7896-4c67-c1f8-5fede68a945c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 64, 64])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "id": "wwL8jtPAtbx1",
    "outputId": "98eb9f0e-f40c-4d70-974d-3c545cdd3901"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fb893cb3010>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show the first sample image of the training dataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(train_ds[0][0], cmap='gray')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "oS2y3URjtbx1"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9LM4PVWQtbx1",
    "outputId": "d82c6b42-792a-45f5-fa38-9cc87b7aab70"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: diffusers in /usr/local/lib/python3.10/dist-packages (0.16.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from diffusers) (2.28.2)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.10/dist-packages (from diffusers) (6.6.0)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from diffusers) (9.4.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from diffusers) (2023.5.5)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from diffusers) (1.24.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from diffusers) (3.10.7)\n",
      "Requirement already satisfied: huggingface-hub>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from diffusers) (0.14.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.13.2->diffusers) (23.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.13.2->diffusers) (4.5.0)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.13.2->diffusers) (2023.5.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.13.2->diffusers) (6.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.13.2->diffusers) (4.65.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata->diffusers) (3.15.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->diffusers) (2.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers) (1.26.16)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers) (3.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->diffusers) (2019.11.28)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install diffusers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qf1vzcXYtbx2",
    "outputId": "4f61a703-c48a-44dd-ba35-9428d68f46e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: denoising_diffusion_pytorch in /usr/local/lib/python3.10/dist-packages (1.6.4)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from denoising_diffusion_pytorch) (2.0.0)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from denoising_diffusion_pytorch) (9.4.0)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (from denoising_diffusion_pytorch) (0.19.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from denoising_diffusion_pytorch) (4.65.0)\n",
      "Requirement already satisfied: pytorch-fid in /usr/local/lib/python3.10/dist-packages (from denoising_diffusion_pytorch) (0.3.0)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from denoising_diffusion_pytorch) (0.15.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from denoising_diffusion_pytorch) (1.24.2)\n",
      "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from denoising_diffusion_pytorch) (0.6.1)\n",
      "Requirement already satisfied: ema-pytorch in /usr/local/lib/python3.10/dist-packages (from denoising_diffusion_pytorch) (0.2.3)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate->denoising_diffusion_pytorch) (6.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate->denoising_diffusion_pytorch) (5.9.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate->denoising_diffusion_pytorch) (23.0)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch->denoising_diffusion_pytorch) (11.7.99)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /usr/local/lib/python3.10/dist-packages (from torch->denoising_diffusion_pytorch) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /usr/local/lib/python3.10/dist-packages (from torch->denoising_diffusion_pytorch) (11.7.101)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->denoising_diffusion_pytorch) (4.5.0)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.10/dist-packages (from torch->denoising_diffusion_pytorch) (11.10.3.66)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->denoising_diffusion_pytorch) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->denoising_diffusion_pytorch) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /usr/local/lib/python3.10/dist-packages (from torch->denoising_diffusion_pytorch) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.10/dist-packages (from torch->denoising_diffusion_pytorch) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch->denoising_diffusion_pytorch) (11.7.99)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /usr/local/lib/python3.10/dist-packages (from torch->denoising_diffusion_pytorch) (2.14.3)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /usr/local/lib/python3.10/dist-packages (from torch->denoising_diffusion_pytorch) (11.7.4.91)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->denoising_diffusion_pytorch) (1.11.1)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.10/dist-packages (from torch->denoising_diffusion_pytorch) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /usr/local/lib/python3.10/dist-packages (from torch->denoising_diffusion_pytorch) (11.7.91)\n",
      "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->denoising_diffusion_pytorch) (2.0.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->denoising_diffusion_pytorch) (3.10.7)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch->denoising_diffusion_pytorch) (67.6.1)\n",
      "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch->denoising_diffusion_pytorch) (0.40.0)\n",
      "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->denoising_diffusion_pytorch) (3.26.1)\n",
      "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->denoising_diffusion_pytorch) (16.0.0)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from pytorch-fid->denoising_diffusion_pytorch) (1.10.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision->denoising_diffusion_pytorch) (2.28.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->denoising_diffusion_pytorch) (2.1.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->torchvision->denoising_diffusion_pytorch) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->torchvision->denoising_diffusion_pytorch) (2019.11.28)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->denoising_diffusion_pytorch) (3.1.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->denoising_diffusion_pytorch) (1.26.16)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->denoising_diffusion_pytorch) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install denoising_diffusion_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fastcore in /usr/local/lib/python3.10/dist-packages (1.5.29)\n",
      "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (from fastcore) (23.0.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from fastcore) (23.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install fastcore "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class TemporalEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        input_size: Tuple[int, int], \n",
    "        hidden_size: int,\n",
    "        num_images: int, \n",
    "        device: str\n",
    "        ) -> None:\n",
    "        super().__init__()\n",
    "        # Set the input size of the image.\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        # Set the size of the flattened image.\n",
    "        self.flatten_size = input_size[0] * input_size[1]\n",
    "        # Set a list of GRUs, one for each image.\n",
    "        self.gru = nn.GRU(\n",
    "            self.flatten_size, hidden_size, num_layers=num_images,\n",
    "            batch_first=True)\n",
    "        #self.grus = nn.ModuleList(\n",
    "        #    [nn.GRU(self.flatten_size, self.flatten_size)\n",
    "        #     for _ in range(num_images)])\n",
    "        # Set the device used for the computations.\n",
    "        self.to(device)\n",
    "        self.device = device\n",
    "\n",
    "    def to(self, device: str) -> None:\n",
    "        super().to(device)\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, x: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        batch_size = x.shape[0]\n",
    "        n_channels = x.shape[1]\n",
    "        # Set the initial hidden states \n",
    "        initial_hidden_state = torch.zeros(\n",
    "            batch_size, n_channels, self.flatten_size, dtype=torch.float32,\n",
    "            device=self.device)\n",
    "\n",
    "        _, out = self.gru(x.flatten(start_dim=2), initial_hidden_state)\n",
    "        # Iterate over the images and pass them through the GRUs.\n",
    "        '''for i, gru in enumerate(self.grus):\n",
    "            # Flatten the image.\n",
    "            img = x[:, i].flatten(start_dim=1)\n",
    "            # If it is the first image, use the initial hidden state.\n",
    "            if i == 0:\n",
    "                h = initial_hidden_state\n",
    "            # Get the forward pass of the GRU.\n",
    "            h, _ = gru(img, h)''';\n",
    "        \n",
    "        \"\"\"# Turn the hidden state to the original shape.\n",
    "        out = out.view(batch_size, n_channels, self.input_size[0],\n",
    "                       self.input_size[1])\"\"\"\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "yQfuFLG5tbx2"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import wandb\n",
    "import fastcore.all as fc\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from diffusers import UNet2DConditionModel\n",
    "\n",
    "try:\n",
    "    from denoising_diffusion_pytorch.simple_diffusion import UViT\n",
    "except:\n",
    "    raise ImportError(\"Please install denoising_diffusion_pytorch with `pip install denoising_diffusion_pytorch`\")\n",
    "\n",
    "\n",
    "def init_unet(model):\n",
    "    \"From Jeremy's bag of tricks on fastai V2 2023\"\n",
    "    for o in model.down_blocks:\n",
    "        for p in o.resnets:\n",
    "            p.conv2.weight.data.zero_()\n",
    "            for p in fc.L(o.downsamplers): nn.init.orthogonal_(p.conv.weight)\n",
    "\n",
    "    for o in model.up_blocks:\n",
    "        for p in o.resnets: p.conv2.weight.data.zero_()\n",
    "\n",
    "    model.conv_out.weight.data.zero_()\n",
    "\n",
    "class WandbModel:\n",
    "    \"A model that can be saved to wandb\"\n",
    "    @classmethod\n",
    "    def from_checkpoint(cls, model_params, checkpoint_file):\n",
    "        \"Load a UNet2D model from a checkpoint file\"\n",
    "        model = cls(**model_params)\n",
    "        print(f\"Loading model from: {checkpoint_file}\")\n",
    "        model.load_state_dict(torch.load(checkpoint_file))\n",
    "        return model\n",
    "\n",
    "    @classmethod\n",
    "    def from_artifact(cls, model_params, artifact_name):\n",
    "        \"Load a UNet2D model from a wandb.Artifact, need to be run in a wandb run\"\n",
    "        artifact = wandb.use_artifact(artifact_name, type='model')\n",
    "        artifact_dir = Path(artifact.download())\n",
    "        chpt_file = list(artifact_dir.glob(\"*.pth\"))[0]\n",
    "        return cls.from_checkpoint(model_params, chpt_file)\n",
    "\n",
    "def get_unet_params(model_name=\"unet_small\", num_frames=4):\n",
    "    \"Return the parameters for the diffusers UNet2d model\"\n",
    "    if model_name == \"unet_small\":\n",
    "        return dict(\n",
    "            block_out_channels=(16, 32, 64, 128), # number of channels for each block\n",
    "            norm_num_groups=8, # number of groups for the normalization layer\n",
    "            in_channels=num_frames, # number of input channels\n",
    "            out_channels=1, # number of output channels\n",
    "            )\n",
    "    elif model_name == \"unet_big\":\n",
    "        return dict(\n",
    "            block_out_channels=(32, 64, 128, 256), # number of channels for each block\n",
    "            norm_num_groups=8, # number of groups for the normalization layer\n",
    "            in_channels=num_frames, # number of input channels\n",
    "            out_channels=1, # number of output channels\n",
    "            )\n",
    "    else:\n",
    "        raise(f\"Model name not found: {model_name}, choose between 'unet_small' or 'unet_big'\")\n",
    "\n",
    "class UNet2DTemporalCondition(UNet2DConditionModel, WandbModel):\n",
    "    def __init__(self, \n",
    "                 *x, \n",
    "                 input_size: Tuple[int, int], \n",
    "                 hidden_size: int, \n",
    "                 num_images: int, \n",
    "                 device: str = \"cuda\",\n",
    "                 **kwargs):\n",
    "        super().__init__(*x, **kwargs)\n",
    "        init_unet(self)\n",
    "        self.temporal_encoder = TemporalEncoder(input_size=input_size, hidden_size=hidden_size, num_images=num_images, device=device)\n",
    "\n",
    "    def forward(self, *x, **kwargs):\n",
    "        print(type(x))\n",
    "        encoder_hidden_states= self.temporal_encoder(*x)\n",
    "        return super().forward(*x, encoder_hidden_states=encoder_hidden_states, **kwargs).sample ## Diffusers's UNet2DConditionModel class\n",
    "\n",
    "## Simple Diffusion paper\n",
    "\n",
    "def get_uvit_params(model_name=\"uvit_small\", num_frames=4):\n",
    "    \"Return the parameters for the diffusers UViT model\"\n",
    "    if model_name == \"uvit_small\":\n",
    "        return dict(\n",
    "            dim=512,\n",
    "            ff_mult=2,\n",
    "            vit_depth=4,\n",
    "            channels=4, \n",
    "            patch_size=4,\n",
    "            final_img_itransform=nn.Conv2d(num_frames,1,1)\n",
    "            )\n",
    "    elif model_name == \"uvit_big\":\n",
    "        return dict(\n",
    "            dim=1024,\n",
    "            ff_mult=4,\n",
    "            vit_depth=8,\n",
    "            channels=4, \n",
    "            patch_size=4,\n",
    "            final_img_itransform=nn.Conv2d(num_frames,1,1)\n",
    "            )\n",
    "    else:\n",
    "        raise(f\"Model name not found: {model_name}, choose between 'uvit_small' or 'uvit_big'\")\n",
    "\n",
    "class UViTModel(UViT, WandbModel): pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "rJzdvyzMx-MI"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import wandb\n",
    "import numpy as np\n",
    "\n",
    "## For Training\n",
    "\n",
    "def to_wandb_image(img):\n",
    "    \"Convert a tensor to a wandb.Image\"\n",
    "    return wandb.Image(torch.cat(img.split(1), dim=-1).cpu().numpy())\n",
    "\n",
    "def log_images(xt, samples):\n",
    "    \"Log sampled images to wandb\"\n",
    "    device = samples.device\n",
    "    frames = torch.cat([xt[:, :-1,...].to(device), samples], dim=1)\n",
    "    wandb.log({\"sampled_images\": [to_wandb_image(img) for img in frames]})\n",
    "\n",
    "def save_model(model, model_name):\n",
    "    \"Save the model to wandb\"\n",
    "    model_name = f\"{wandb.run.id}_{model_name}\"\n",
    "    models_folder = Path(\"models\")\n",
    "    if not models_folder.exists():\n",
    "        models_folder.mkdir()\n",
    "    torch.save(model.state_dict(), models_folder/f\"{model_name}.pth\")\n",
    "    at = wandb.Artifact(model_name, type=\"model\")\n",
    "    at.add_file(f\"models/{model_name}.pth\")\n",
    "    wandb.log_artifact(at)\n",
    "\n",
    "\n",
    "## For Inference\n",
    "def htile(img):\n",
    "    \"Horizontally tile a batch of images.\"\n",
    "    return torch.cat(img.split(1), dim=-1)\n",
    "\n",
    "def vtile(img):\n",
    "    \"Vertically tile a batch of images.\"\n",
    "    return torch.cat(img.split(1), dim=-2)\n",
    "\n",
    "def vhtile(*imgs):\n",
    "    \"Vertically and horizontally tile a batch of images.\"\n",
    "    return vtile(torch.cat([htile(img) for img in imgs], dim=0))\n",
    "\n",
    "def scale(arr):\n",
    "    \"Scales values of array in [0,1]\"\n",
    "    m, M = arr.min(), arr.max()\n",
    "    return (arr - m) / (M - m)\n",
    "\n",
    "def preprocess_frames(data):\n",
    "    \"Preprocess frames for wandb.Video\"\n",
    "    sdata = scale(data.squeeze())\n",
    "    # print(sdata.shape)\n",
    "    def tfm(frame):\n",
    "        rframe = 255 * frame\n",
    "        return rframe.cpu().numpy().astype(np.uint8)\n",
    "    return [tfm(frame) for frame in sdata]\n",
    "\n",
    "def to_video(data):\n",
    "    \"create wandb.Video container\"\n",
    "    frames = preprocess_frames(data)\n",
    "    vid = np.stack(frames)[:, None, ...]\n",
    "    return wandb.Video(vid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fastprogress in /usr/local/lib/python3.10/dist-packages (1.0.3)\n",
      "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.10/dist-packages (0.11.4)\n",
      "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (1.24.2)\n",
      "Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.0.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (23.0)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (10.2.10.91)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (4.5.0)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (11.7.99)\n",
      "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (2.0.0)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (11.7.101)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (11.7.91)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (2.14.3)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (11.4.0.1)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (1.11.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.1.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.10.7)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.8.1->torchmetrics) (67.6.1)\n",
      "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.8.1->torchmetrics) (0.40.0)\n",
      "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8.1->torchmetrics) (16.0.0)\n",
      "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8.1->torchmetrics) (3.26.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.1->torchmetrics) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.1->torchmetrics) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install fastprogress torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "z1l_534VxxaQ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random, argparse\n",
    "from pathlib import Path\n",
    "\n",
    "import wandb\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "from torchmetrics import PeakSignalNoiseRatio, StructuralSimilarityIndexMeasure\n",
    "\n",
    "from fastprogress import progress_bar\n",
    "\n",
    "#from cloud_diffusion.wandb import log_images, save_model\n",
    "\n",
    "def noisify_last_frame(frames, noise_func):\n",
    "    \"Noisify the last frame of a sequence\"\n",
    "    past_frames = frames[:,:-1]\n",
    "    last_frame  = frames[:,-1:]\n",
    "    noise, t, e = noise_func(last_frame)\n",
    "    return torch.cat([past_frames, noise], dim=1), t, e\n",
    "\n",
    "def noisify_collate(noise_func): \n",
    "    def _inner(b): \n",
    "        \"Collate function that noisifies the last frame\"\n",
    "        return noisify_last_frame(default_collate(b), noise_func)\n",
    "    return _inner\n",
    "\n",
    "class NoisifyDataloader(DataLoader):\n",
    "    \"\"\"Noisify the last frame of a dataloader by applying \n",
    "    a noise function, after collating the batch\"\"\"\n",
    "    def __init__(self, dataset, *args, noise_func=None, **kwargs):\n",
    "        super().__init__(dataset, *args, collate_fn=noisify_collate(noise_func), **kwargs)\n",
    "\n",
    "class MiniTrainer:\n",
    "    \"A mini trainer for the diffusion process\"\n",
    "    def __init__(self, \n",
    "                 train_dataloader, \n",
    "                 valid_dataloader, \n",
    "                 model, \n",
    "                 sampler, \n",
    "                 device=\"cuda\", \n",
    "                 loss_func=nn.MSELoss(), \n",
    "                 ):\n",
    "        self.train_dataloader = train_dataloader\n",
    "        self.valid_dataloader = valid_dataloader\n",
    "        self.loss_func = loss_func\n",
    "        self.psnr = PeakSignalNoiseRatio().to(device)\n",
    "        self.ssim = StructuralSimilarityIndexMeasure().to(device)\n",
    "        self.model = model.to(device)\n",
    "        self.scaler = torch.cuda.amp.GradScaler()\n",
    "        self.device = device\n",
    "        self.sampler = sampler\n",
    "        self.val_batch = next(iter(valid_dataloader))[0].to(device)  # grab a fixed batch to log predictions\n",
    "    \n",
    "    def train_step(self, loss):\n",
    "        \"Train for one step\"\n",
    "        self.optimizer.zero_grad()\n",
    "        self.scaler.scale(loss).backward()\n",
    "        self.scaler.step(self.optimizer)\n",
    "        self.scaler.update()\n",
    "        self.scheduler.step()\n",
    "\n",
    "    def one_epoch(self, epoch=None):\n",
    "        \"Train for one epoch, log metrics and save model\"\n",
    "        self.model.train()\n",
    "        pbar = progress_bar(self.train_dataloader, leave=False)\n",
    "        for batch in pbar:\n",
    "            frames, t, noise = to_device(batch, device=self.device)\n",
    "            with torch.autocast(\"cuda\"):\n",
    "                predicted_noise = self.model(frames, t)\n",
    "                loss = self.loss_func(noise, predicted_noise)\n",
    "            self.train_step(loss)\n",
    "            wandb.log({\"train_mse\": loss.item(),\n",
    "                       \"learning_rate\": self.scheduler.get_last_lr()[0]})\n",
    "            pbar.comment = f\"epoch={epoch}, MSE={loss.item():2.3f}\"\n",
    "\n",
    "    def one_epoch_validation(self, epoch=None):\n",
    "        \"Validates on val set\"\n",
    "        pbar = progress_bar(self.valid_dataloader, leave=False)\n",
    "        psnr_metric = 0\n",
    "        mse_metric = 0\n",
    "        ssmi_metric = 0\n",
    "        for val_batch in pbar:\n",
    "            frames = val_batch[0].to(self.device)\n",
    "            samples = self.sampler(self.model, past_frames=frames[:,:-1]).to(self.device)\n",
    "            psnr_metric += self.psnr(samples, frames[:,-1]).float().cpu()\n",
    "            ssmi_metric += self.ssim(samples, frames[:,-1]).float().cpu()\n",
    "            mse_metric += self.loss_func(samples, frames[:,-1]).float().cpu()\n",
    "        psnr_metric = psnr_metric / len(self.valid_dataloader)\n",
    "        ssmi_metric = ssmi_metric / len(self.valid_dataloader)\n",
    "        mse_metric = mse_metric / len(self.valid_dataloader)\n",
    "        wandb.log({\"val_psnr\": psnr_metric,\n",
    "                   \"val_ssmi\": ssmi_metric,\n",
    "                   \"val_mse\": mse_metric})\n",
    "\n",
    "    def prepare(self, config):\n",
    "        wandb.config.update(config)\n",
    "        config.total_train_steps = config.epochs * len(self.train_dataloader)\n",
    "        self.optimizer = AdamW(self.model.parameters(), lr=config.lr, eps=1e-5)\n",
    "        self.scheduler = OneCycleLR(self.optimizer, max_lr=config.lr, total_steps=config.total_train_steps)\n",
    "\n",
    "    def fit(self, config):\n",
    "        self.prepare(config)\n",
    "        self.val_batch = self.val_batch[:min(config.n_preds, 8)]  # log first 8 predictions\n",
    "        for epoch in progress_bar(range(config.epochs), total=config.epochs, leave=True):\n",
    "            self.one_epoch(epoch)\n",
    "            self.one_epoch_validation(epoch)\n",
    "            \n",
    "            # log predictions\n",
    "            if epoch % config.log_every_epoch == 0:\n",
    "                samples = self.sampler(self.model, past_frames=self.val_batch[:,:-1])\n",
    "                log_images(self.val_batch, samples)\n",
    "\n",
    "        save_model(self.model, config.model_name)\n",
    "\n",
    "\n",
    "def set_seed(s, reproducible=False):\n",
    "    \"Set random seed for `random`, `torch`, and `numpy` (where available)\"\n",
    "    try: torch.manual_seed(s)\n",
    "    except NameError: pass\n",
    "    try: torch.cuda.manual_seed_all(s)\n",
    "    except NameError: pass\n",
    "    try: np.random.seed(s%(2**32-1))\n",
    "    except NameError: pass\n",
    "    random.seed(s)\n",
    "    if reproducible:\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def to_device(t, device=\"cpu\"):\n",
    "    if isinstance(t, (tuple, list)):\n",
    "        return [_t.to(device) for _t in t]\n",
    "    elif isinstance(t, torch.Tensor):\n",
    "        return t.to(device)\n",
    "    else:\n",
    "        raise(\"Not a Tensor or list of Tensors\")\n",
    "\n",
    "\n",
    "def ls(path: Path): \n",
    "    \"Return files on Path, sorted\"\n",
    "    return sorted(list(path.iterdir()))\n",
    "\n",
    "\n",
    "def parse_args(config):\n",
    "    \"A brute force way to parse arguments, it is probably not a good idea to use it\"\n",
    "    parser = argparse.ArgumentParser(description='Run training baseline')\n",
    "    for k,v in config.__dict__.items():\n",
    "        parser.add_argument('--'+k, type=type(v), default=v)\n",
    "    args = vars(parser.parse_args())\n",
    "    \n",
    "    # update config with parsed args\n",
    "    for k, v in args.items():\n",
    "        setattr(config, k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "XfTZnkBpyM-O"
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "from fastprogress import progress_bar\n",
    "\n",
    "from diffusers.schedulers import DDIMScheduler\n",
    "\n",
    "\n",
    "## DDPM params\n",
    "## From fastai V2 Course DDPM notebooks\n",
    "betamin,betamax,n_steps = 0.0001,0.02, 1000\n",
    "beta = torch.linspace(betamin, betamax, n_steps)\n",
    "alpha = 1.-beta\n",
    "alphabar = alpha.cumprod(dim=0)\n",
    "sigma = beta.sqrt()\n",
    "\n",
    "def noisify_ddpm(x0):\n",
    "    \"Noise by ddpm\"\n",
    "    device = x0.device\n",
    "    n = len(x0)\n",
    "    t = torch.randint(0, n_steps, (n,), dtype=torch.long)\n",
    "    ε = torch.randn(x0.shape, device=device)\n",
    "    ᾱ_t = alphabar[t].reshape(-1, 1, 1, 1).to(device)\n",
    "    xt = ᾱ_t.sqrt()*x0 + (1-ᾱ_t).sqrt()*ε\n",
    "    return xt, t.to(device), ε\n",
    "\n",
    "@torch.no_grad()\n",
    "def diffusers_sampler(model, past_frames, sched, **kwargs):\n",
    "    \"Using Diffusers built-in samplers\"\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    new_frame = torch.randn_like(past_frames[:,-1:], dtype=past_frames.dtype, device=device)\n",
    "    preds = []\n",
    "    pbar = progress_bar(sched.timesteps, leave=False)\n",
    "    for t in pbar:\n",
    "        pbar.comment = f\"DDIM Sampler: frame {t}\"\n",
    "        noise = model(torch.cat([past_frames, new_frame], dim=1), t)\n",
    "        new_frame = sched.step(noise, t, new_frame, **kwargs).prev_sample\n",
    "        preds.append(new_frame.float().cpu())\n",
    "    return preds[-1]\n",
    "\n",
    "def ddim_sampler(steps=350, eta=1.):\n",
    "    \"DDIM sampler, faster and a bit better than the built-in sampler\"\n",
    "    ddim_sched = DDIMScheduler()\n",
    "    ddim_sched.set_timesteps(steps)\n",
    "    return partial(diffusers_sampler, sched=ddim_sched, eta=eta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "psosQcqNxxMY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "-uNhPHz3wpqD"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from types import SimpleNamespace\n",
    "\n",
    "import wandb\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# from cloud_diffusion.dataset import download_dataset, CloudDataset\n",
    "#from cloud_diffusion.utils import NoisifyDataloader, MiniTrainer, set_seed, parse_args\n",
    "#from cloud_diffusion.ddpm import noisify_ddpm, ddim_sampler\n",
    "# from cloud_diffusion.models import UNet2D, get_unet_params\n",
    "\n",
    "\n",
    "PROJECT_NAME = \"cloud_diffusion\"\n",
    "DATASET_ARTIFACT = 'maidacundo/cloud_diffusion/sample_dataset:v0'\n",
    "\n",
    "config = SimpleNamespace(    \n",
    "    epochs=50, # number of epochs\n",
    "    model_name=\"unet_small\", # model name to save [unet_small, unet_big]\n",
    "    strategy=\"ddpm\", # strategy to use ddpm\n",
    "    noise_steps=1000, # number of noise steps on the diffusion process\n",
    "    sampler_steps=333, # number of sampler steps on the diffusion process\n",
    "    seed=42, # random seed\n",
    "    batch_size=128, # batch size\n",
    "    img_size=64, # image size\n",
    "    device=\"cuda\", # device\n",
    "    num_workers=8, # number of workers for dataloader\n",
    "    num_frames=4, # number of frames to use as input\n",
    "    lr=5e-4, # learning rate\n",
    "    validation_days=3, # number of days to use for validation\n",
    "    log_every_epoch=5, # log every n epochs to wandb\n",
    "    n_preds=8, # number of predictions to make\n",
    "    )\n",
    "\n",
    "def train_func(config):\n",
    "    config.model_params = get_unet_params(config.model_name, config.num_frames)\n",
    "\n",
    "    set_seed(config.seed)\n",
    "    device = torch.device(config.device)\n",
    "\n",
    "    # downlaod the dataset from the wandb.Artifact\n",
    "    files = download_dataset(DATASET_ARTIFACT, PROJECT_NAME)\n",
    "    train_days, valid_days = files[:-config.validation_days], files[-config.validation_days:]\n",
    "    train_ds = CloudDataset(files=train_days, num_frames=config.num_frames, img_size=config.img_size)\n",
    "    valid_ds = CloudDataset(files=valid_days, num_frames=config.num_frames, img_size=config.img_size).shuffle()\n",
    "\n",
    "    # DDPM dataloaders\n",
    "    train_dataloader = NoisifyDataloader(train_ds, config.batch_size, shuffle=True, \n",
    "                                         noise_func=noisify_ddpm,  num_workers=config.num_workers)\n",
    "    valid_dataloader = NoisifyDataloader(valid_ds, config.batch_size, shuffle=False, \n",
    "                                          noise_func=noisify_ddpm,  num_workers=config.num_workers)\n",
    "\n",
    "    # model setup\n",
    "    model = UNet2D(**config.model_params)\n",
    "\n",
    "    # sampler\n",
    "    sampler = ddim_sampler(steps=config.sampler_steps)\n",
    "\n",
    "    # A simple training loop\n",
    "    trainer = MiniTrainer(train_dataloader, valid_dataloader, model, sampler, device)\n",
    "    trainer.fit(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "492bad3c69b142b9914023a64bd79485",
      "a7f28847a7524f23af49828acac0e947",
      "5b5160878fe44aad980e22af51cd4cac",
      "0020ea9994f54760bdd72e899e534934",
      "7d5f32c5fa3e4fffa72114dd99e03b9e",
      "b59ff36ada034f9a882fdb0f8f96db55",
      "be4cab7316c9458eb5d1456c54f267f5",
      "4c0c8c27087f4f0a9b26b2a2bb555891",
      "817d98319cb447609a24a7bd9da9ecc7",
      "b19a3ef2baa0401490e99b4d8d2e9581",
      "14d3e9f84a0641dbb882cebed4dfbe03",
      "e965a7678a7e4985b05fcfb83edc1d59",
      "0ec48bf6fb02486c8f4816a77d134142",
      "76427cc7c1dc4797abb6e635265d448b",
      "b7578f6b38ec411092f57586f7c0563f",
      "87533253720a4674a22b2302990259cb"
     ]
    },
    "id": "5XPIJlC0xO0T",
    "outputId": "b99c16bf-fb2e-4272-f24a-f69ff9284f2a",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/wandb/run-20230525_204417-tu4o0co7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ai-industry/cloud_diffusion/runs/tu4o0co7' target=\"_blank\">avid-thunder-12</a></strong> to <a href='https://wandb.ai/ai-industry/cloud_diffusion' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ai-industry/cloud_diffusion' target=\"_blank\">https://wandb.ai/ai-industry/cloud_diffusion</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ai-industry/cloud_diffusion/runs/tu4o0co7' target=\"_blank\">https://wandb.ai/ai-industry/cloud_diffusion/runs/tu4o0co7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact sample_dataset:v0, 360.00MB. 30 files... \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   30 of 30 files downloaded.  \n",
      "Done. 0:0:0.2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with wandb.init(project=PROJECT_NAME, entity='ai-industry', config=config, tags=[\"ddpm\", config.model_name]):\n",
    "    train_func(config)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0020ea9994f54760bdd72e899e534934": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0ec48bf6fb02486c8f4816a77d134142": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "14d3e9f84a0641dbb882cebed4dfbe03": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b7578f6b38ec411092f57586f7c0563f",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_87533253720a4674a22b2302990259cb",
      "value": 0.9993770962032323
     }
    },
    "492bad3c69b142b9914023a64bd79485": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a7f28847a7524f23af49828acac0e947",
       "IPY_MODEL_5b5160878fe44aad980e22af51cd4cac"
      ],
      "layout": "IPY_MODEL_0020ea9994f54760bdd72e899e534934"
     }
    },
    "4c0c8c27087f4f0a9b26b2a2bb555891": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "5b5160878fe44aad980e22af51cd4cac": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_be4cab7316c9458eb5d1456c54f267f5",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_4c0c8c27087f4f0a9b26b2a2bb555891",
      "value": 1
     }
    },
    "76427cc7c1dc4797abb6e635265d448b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7d5f32c5fa3e4fffa72114dd99e03b9e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "817d98319cb447609a24a7bd9da9ecc7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b19a3ef2baa0401490e99b4d8d2e9581",
       "IPY_MODEL_14d3e9f84a0641dbb882cebed4dfbe03"
      ],
      "layout": "IPY_MODEL_e965a7678a7e4985b05fcfb83edc1d59"
     }
    },
    "87533253720a4674a22b2302990259cb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a7f28847a7524f23af49828acac0e947": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7d5f32c5fa3e4fffa72114dd99e03b9e",
      "placeholder": "​",
      "style": "IPY_MODEL_b59ff36ada034f9a882fdb0f8f96db55",
      "value": "Waiting for wandb.init()...\r"
     }
    },
    "b19a3ef2baa0401490e99b4d8d2e9581": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0ec48bf6fb02486c8f4816a77d134142",
      "placeholder": "​",
      "style": "IPY_MODEL_76427cc7c1dc4797abb6e635265d448b",
      "value": "16.179 MB of 16.189 MB uploaded (0.000 MB deduped)\r"
     }
    },
    "b59ff36ada034f9a882fdb0f8f96db55": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b7578f6b38ec411092f57586f7c0563f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "be4cab7316c9458eb5d1456c54f267f5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e965a7678a7e4985b05fcfb83edc1d59": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
