{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "592KgIBCtbxt"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "xn_pBvNhtbxv"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "def ls(path: Path): \n",
        "    \"Return files on Path, sorted\"\n",
        "    return sorted(list(path.iterdir()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "FsmkDobltqUd"
      },
      "outputs": [],
      "source": [
        "!pip install -q wandb tqdm matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ZRHQ37tJ05B",
        "outputId": "fe66acc3-7933-404b-e766-5f20bd7f1964"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "import wandb\n",
        "wandb.login(key='6b22cbf359c5924f4500afc1ae572d6827998186', relogin=True, force=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Qzo4ppMBtbxx"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision.transforms as T\n",
        "import wandb\n",
        "from tqdm import tqdm as progress_bar\n",
        "\n",
        "# from cloud_diffusion.utils import ls\n",
        "\n",
        "PROJECT_NAME = \"cloud_diffusion\"\n",
        "DATASET_ARTIFACT = 'maidacundo/cloud_diffusion/sample_dataset:v0'\n",
        "\n",
        "class DummyNextFrameDataset:\n",
        "    \"Dataset that returns random images\"\n",
        "    def __init__(self, num_frames=4, img_size=64, N=1000):\n",
        "        self.img_size = img_size\n",
        "        self.num_frames = num_frames\n",
        "        self.N = N\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.randn(self.num_frames, self.img_size, self.img_size)\n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.N\n",
        "\n",
        "\n",
        "class CloudDataset:\n",
        "    \"\"\"Dataset for cloud images\n",
        "    It loads numpy files from wandb artifact and stacks them into a single array\n",
        "    It also applies some transformations to the images\n",
        "    \"\"\"\n",
        "    def __init__(self, \n",
        "                 files, # list of numpy files to load (they come from the artifact)\n",
        "                 num_frames=4, # how many consecutive frames to stack\n",
        "                 scale=True, # if we images to interval [-0.5, 0.5]\n",
        "                 img_size=64, # resize dim, original images are big (446, 780)\n",
        "                 valid=False, # if True, transforms are deterministic\n",
        "                ):\n",
        "        \n",
        "        tfms = [T.Resize((img_size, int(img_size*1.7)), antialias=True)] if img_size is not None else []\n",
        "        tfms += [T.RandomCrop(img_size)] if not valid else [T.CenterCrop(img_size)]\n",
        "        self.tfms = T.Compose(tfms)\n",
        "        self.load_data(files, num_frames, scale)\n",
        "        \n",
        "    def load_day(self, file, scale=True):\n",
        "        one_day = np.load(file)\n",
        "        one_day = one_day.astype('float32')\n",
        "        if scale:\n",
        "            one_day = 0.5 - self._scale(one_day)\n",
        "        return one_day\n",
        "\n",
        "    def load_data(self, files, num_frames, scale):\n",
        "        \"Loads all data into a single array self.data\"\n",
        "        data = []\n",
        "        # TODO: download all files\n",
        "        for file in progress_bar(files, leave=False):\n",
        "            one_day = self.load_day(file, scale)\n",
        "            wds = np.lib.stride_tricks.sliding_window_view(\n",
        "                one_day.squeeze(),\n",
        "                num_frames,\n",
        "                axis=0).transpose((0,3,1,2))\n",
        "            data.append(wds)\n",
        "            # pbar.comment = f\"Creating CloudDataset from {file}\"\n",
        "        self.data = np.concatenate(data, axis=0)\n",
        "\n",
        "    def shuffle(self):\n",
        "        \"\"\"Shuffles the dataset, useful for getting \n",
        "        interesting samples on the validation dataset\"\"\"\n",
        "        idxs = torch.randperm(len(self.data))\n",
        "        self.data = self.data[idxs]\n",
        "        return self\n",
        "\n",
        "    @staticmethod\n",
        "    def _scale(arr):\n",
        "        \"Scales values of array in [0,1]\"\n",
        "        m, M = arr.min(), arr.max()\n",
        "        return (arr - m) / (M - m)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return self.tfms(torch.from_numpy(self.data[idx]))\n",
        "    \n",
        "    def __len__(self): return len(self.data)\n",
        "\n",
        "    def save(self, fname=\"cloud_frames.npy\"):\n",
        "        np.save(fname, self.data)\n",
        "\n",
        "\n",
        "class CloudDatasetInference(CloudDataset):\n",
        "     def load_data(self, files, num_frames=None, scale=None):\n",
        "        \"Loads all data into a single array self.data\"\n",
        "        data = []\n",
        "        max_length = 100\n",
        "        # TODO: download everything\n",
        "        for file in files:\n",
        "            one_day = self.load_day(file, scale)\n",
        "            data.append(one_day)\n",
        "            max_length = min(max_length, len(one_day))\n",
        "        self.data = np.stack([d[:max_length] for d in data], axis=0).squeeze()\n",
        "\n",
        "\n",
        "def download_dataset(at_name, project_name):\n",
        "    \"Downloads dataset from wandb artifact\"\n",
        "    def _get_dataset(run):\n",
        "        artifact = run.use_artifact(at_name, type='dataset')\n",
        "        return artifact.download()\n",
        "\n",
        "    if wandb.run is not None:\n",
        "        run = wandb.run\n",
        "        artifact_dir = _get_dataset(run)\n",
        "    else:\n",
        "        run = wandb.init(project=project_name, job_type=\"download_dataset\")\n",
        "        artifact_dir = _get_dataset(run)\n",
        "        run.finish()\n",
        "\n",
        "    files = ls(Path(artifact_dir))\n",
        "    return files\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oS2y3URjtbx1"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9LM4PVWQtbx1",
        "outputId": "87d092b2-9c1a-4de2-e6a3-f25ce80daa30"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: diffusers in /usr/local/lib/python3.10/dist-packages (0.16.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from diffusers) (8.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from diffusers) (3.12.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from diffusers) (0.14.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.10/dist-packages (from diffusers) (6.6.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from diffusers) (1.22.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from diffusers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from diffusers) (2.27.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.13.2->diffusers) (2023.4.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.13.2->diffusers) (4.65.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.13.2->diffusers) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.13.2->diffusers) (4.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.13.2->diffusers) (23.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata->diffusers) (3.15.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers) (3.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install diffusers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qf1vzcXYtbx2",
        "outputId": "41d99e1c-b189-4cfb-e497-afd84a750af9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: denoising_diffusion_pytorch in /usr/local/lib/python3.10/dist-packages (1.6.4)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (from denoising_diffusion_pytorch) (0.19.0)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from denoising_diffusion_pytorch) (0.6.1)\n",
            "Requirement already satisfied: ema-pytorch in /usr/local/lib/python3.10/dist-packages (from denoising_diffusion_pytorch) (0.2.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from denoising_diffusion_pytorch) (1.22.4)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from denoising_diffusion_pytorch) (8.4.0)\n",
            "Requirement already satisfied: pytorch-fid in /usr/local/lib/python3.10/dist-packages (from denoising_diffusion_pytorch) (0.3.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from denoising_diffusion_pytorch) (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from denoising_diffusion_pytorch) (0.15.2+cu118)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from denoising_diffusion_pytorch) (4.65.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate->denoising_diffusion_pytorch) (23.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate->denoising_diffusion_pytorch) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate->denoising_diffusion_pytorch) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->denoising_diffusion_pytorch) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->denoising_diffusion_pytorch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->denoising_diffusion_pytorch) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->denoising_diffusion_pytorch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->denoising_diffusion_pytorch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->denoising_diffusion_pytorch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->denoising_diffusion_pytorch) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->denoising_diffusion_pytorch) (16.0.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from pytorch-fid->denoising_diffusion_pytorch) (1.10.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision->denoising_diffusion_pytorch) (2.27.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->denoising_diffusion_pytorch) (2.1.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->denoising_diffusion_pytorch) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->denoising_diffusion_pytorch) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->denoising_diffusion_pytorch) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->denoising_diffusion_pytorch) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->denoising_diffusion_pytorch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install denoising_diffusion_pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3n2Q5qp_J05F",
        "outputId": "60a04e54-2e5e-41e4-e943-0b3ac7db554f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: fastcore in /usr/local/lib/python3.10/dist-packages (1.5.29)\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (from fastcore) (23.1.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from fastcore) (23.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install fastcore "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.autograd import Variable\n",
        "\n",
        "\n",
        "class ConvGRUCell(nn.Module):\n",
        "    def __init__(self, input_size, input_dim, hidden_dim, kernel_size, bias, dtype):\n",
        "        \"\"\"\n",
        "        Initialize the ConvLSTM cell\n",
        "        :param input_size: (int, int)\n",
        "            Height and width of input tensor as (height, width).\n",
        "        :param input_dim: int\n",
        "            Number of channels of input tensor.\n",
        "        :param hidden_dim: int\n",
        "            Number of channels of hidden state.\n",
        "        :param kernel_size: (int, int)\n",
        "            Size of the convolutional kernel.\n",
        "        :param bias: bool\n",
        "            Whether or not to add the bias.\n",
        "        :param dtype: torch.cuda.FloatTensor or torch.FloatTensor\n",
        "            Whether or not to use cuda.\n",
        "        \"\"\"\n",
        "        super(ConvGRUCell, self).__init__()\n",
        "        self.height, self.width = input_size\n",
        "        self.padding = kernel_size[0] // 2, kernel_size[1] // 2\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.bias = bias\n",
        "        self.dtype = dtype\n",
        "\n",
        "        self.conv_gates = nn.Conv2d(in_channels=input_dim + hidden_dim,\n",
        "                                    out_channels=2*self.hidden_dim,  # for update_gate,reset_gate respectively\n",
        "                                    kernel_size=kernel_size,\n",
        "                                    padding=self.padding,\n",
        "                                    bias=self.bias)\n",
        "\n",
        "        self.conv_can = nn.Conv2d(in_channels=input_dim+hidden_dim,\n",
        "                              out_channels=self.hidden_dim, # for candidate neural memory\n",
        "                              kernel_size=kernel_size,\n",
        "                              padding=self.padding,\n",
        "                              bias=self.bias)\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        return (Variable(torch.zeros(batch_size, self.hidden_dim, self.height, self.width)).type(self.dtype))\n",
        "\n",
        "    def forward(self, input_tensor, h_cur):\n",
        "        \"\"\"\n",
        "\n",
        "        :param self:\n",
        "        :param input_tensor: (b, c, h, w)\n",
        "            input is actually the target_model\n",
        "        :param h_cur: (b, c_hidden, h, w)\n",
        "            current hidden and cell states respectively\n",
        "        :return: h_next,\n",
        "            next hidden state\n",
        "        \"\"\"\n",
        "        combined = torch.cat([input_tensor, h_cur], dim=1)\n",
        "        combined_conv = self.conv_gates(combined)\n",
        "\n",
        "        gamma, beta = torch.split(combined_conv, self.hidden_dim, dim=1)\n",
        "        reset_gate = torch.sigmoid(gamma)\n",
        "        update_gate = torch.sigmoid(beta)\n",
        "\n",
        "        \n",
        "        combined = torch.cat([input_tensor, reset_gate*h_cur], dim=1)\n",
        "        cc_cnm = self.conv_can(combined)\n",
        "        cnm = torch.tanh(cc_cnm)\n",
        "\n",
        "        h_next = (1 - update_gate) * h_cur + update_gate * cnm\n",
        "        return h_next\n",
        "\n",
        "\n",
        "class ConvGRU(nn.Module):\n",
        "    def __init__(self, input_size, input_dim, hidden_dim, kernel_size, num_layers,\n",
        "                 dtype, batch_first=False, bias=True, return_all_layers=False):\n",
        "        \"\"\"\n",
        "\n",
        "        :param input_size: (int, int)\n",
        "            Height and width of input tensor as (height, width).\n",
        "        :param input_dim: int e.g. 256\n",
        "            Number of channels of input tensor.\n",
        "        :param hidden_dim: int e.g. 1024\n",
        "            Number of channels of hidden state.\n",
        "        :param kernel_size: (int, int)\n",
        "            Size of the convolutional kernel.\n",
        "        :param num_layers: int\n",
        "            Number of ConvLSTM layers\n",
        "        :param dtype: torch.cuda.FloatTensor or torch.FloatTensor\n",
        "            Whether or not to use cuda.\n",
        "        :param alexnet_path: str\n",
        "            pretrained alexnet parameters\n",
        "        :param batch_first: bool\n",
        "            if the first position of array is batch or not\n",
        "        :param bias: bool\n",
        "            Whether or not to add the bias.\n",
        "        :param return_all_layers: bool\n",
        "            if return hidden and cell states for all layers\n",
        "        \"\"\"\n",
        "        super(ConvGRU, self).__init__()\n",
        "\n",
        "        # Make sure that both `kernel_size` and `hidden_dim` are lists having len == num_layers\n",
        "        kernel_size = self._extend_for_multilayer(kernel_size, num_layers)\n",
        "        hidden_dim  = self._extend_for_multilayer(hidden_dim, num_layers)\n",
        "        if not len(kernel_size) == len(hidden_dim) == num_layers:\n",
        "            raise ValueError('Inconsistent list length.')\n",
        "\n",
        "        self.height, self.width = input_size\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.kernel_size = kernel_size\n",
        "        self.dtype = dtype\n",
        "        self.num_layers = num_layers\n",
        "        self.batch_first = batch_first\n",
        "        self.bias = bias\n",
        "        self.return_all_layers = return_all_layers\n",
        "\n",
        "        cell_list = []\n",
        "        for i in range(0, self.num_layers):\n",
        "            cur_input_dim = input_dim if i == 0 else hidden_dim[i - 1]\n",
        "            cell_list.append(ConvGRUCell(input_size=(self.height, self.width),\n",
        "                                         input_dim=cur_input_dim,\n",
        "                                         hidden_dim=self.hidden_dim[i],\n",
        "                                         kernel_size=self.kernel_size[i],\n",
        "                                         bias=self.bias,\n",
        "                                         dtype=self.dtype))\n",
        "\n",
        "        # convert python list to pytorch module\n",
        "        self.cell_list = nn.ModuleList(cell_list)\n",
        "\n",
        "    def forward(self, input_tensor, hidden_state=None):\n",
        "        \"\"\"\n",
        "\n",
        "        :param input_tensor: (b, t, c, h, w) or (t,b,c,h,w) depends on if batch first or not\n",
        "            extracted features from alexnet\n",
        "        :param hidden_state:\n",
        "        :return: layer_output_list, last_state_list\n",
        "        \"\"\"\n",
        "        if not self.batch_first:\n",
        "            # (t, b, c, h, w) -> (b, t, c, h, w)\n",
        "            input_tensor = input_tensor.permute(1, 0, 2, 3, 4)\n",
        "\n",
        "        # Implement stateful ConvLSTM\n",
        "        if hidden_state is not None:\n",
        "            raise NotImplementedError()\n",
        "        else:\n",
        "            hidden_state = self._init_hidden(batch_size=input_tensor.size(0))\n",
        "\n",
        "        layer_output_list = []\n",
        "        last_state_list   = []\n",
        "\n",
        "        seq_len = input_tensor.size(1)\n",
        "        cur_layer_input = input_tensor\n",
        "\n",
        "        for layer_idx in range(self.num_layers):\n",
        "            h = hidden_state[layer_idx]\n",
        "            output_inner = []\n",
        "            for t in range(seq_len):\n",
        "                # input current hidden and cell state then compute the next hidden and cell state through ConvLSTMCell forward function\n",
        "                h = self.cell_list[layer_idx](input_tensor=cur_layer_input[:, t, :, :, :], # (b,t,c,h,w)\n",
        "                                              h_cur=h)\n",
        "                output_inner.append(h)\n",
        "\n",
        "            layer_output = torch.stack(output_inner, dim=1)\n",
        "            cur_layer_input = layer_output\n",
        "\n",
        "            layer_output_list.append(layer_output)\n",
        "            last_state_list.append([h])\n",
        "\n",
        "        if not self.return_all_layers:\n",
        "            layer_output_list = layer_output_list[-1:]\n",
        "            last_state_list   = last_state_list[-1:]\n",
        "\n",
        "        return layer_output_list, last_state_list\n",
        "\n",
        "    def _init_hidden(self, batch_size):\n",
        "        init_states = []\n",
        "        for i in range(self.num_layers):\n",
        "            init_states.append(self.cell_list[i].init_hidden(batch_size))\n",
        "        return init_states\n",
        "\n",
        "    @staticmethod\n",
        "    def _check_kernel_size_consistency(kernel_size):\n",
        "        if not (isinstance(kernel_size, tuple) or\n",
        "                    (isinstance(kernel_size, list) and all([isinstance(elem, tuple) for elem in kernel_size]))):\n",
        "            raise ValueError('`kernel_size` must be tuple or list of tuples')\n",
        "\n",
        "    @staticmethod\n",
        "    def _extend_for_multilayer(param, num_layers):\n",
        "        if not isinstance(param, list):\n",
        "            param = [param] * num_layers\n",
        "        return param"
      ],
      "metadata": {
        "id": "xi_MUZcOXMcn"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "XotWTQaPJ05G"
      },
      "outputs": [],
      "source": [
        "from typing import Tuple\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "class TemporalEncoder(nn.Module):\n",
        "    def __init__(\n",
        "        self, \n",
        "        input_size: Tuple[int, int], \n",
        "        hidden_size: int,\n",
        "        num_images: int, \n",
        "        device: str\n",
        "        ) -> None:\n",
        "        super().__init__()\n",
        "        # Set the input size of the image.\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        # Set the size of the flattened image.\n",
        "        self.flatten_size = input_size[0] * input_size[1]\n",
        "        # Set a list of GRUs, one for each image.\n",
        "        self.gru = nn.GRU(\n",
        "            self.flatten_size, hidden_size, batch_first=True)\n",
        "        #self.grus = nn.ModuleList(\n",
        "        #    [nn.GRU(self.flatten_size, self.flatten_size)\n",
        "        #     for _ in range(num_images)])\n",
        "        # Set the device used for the computations.\n",
        "        self.to(device)\n",
        "        self.device = device\n",
        "\n",
        "    def to(self, device: str) -> None:\n",
        "        super().to(device)\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, x: torch.FloatTensor) -> torch.FloatTensor:\n",
        "        batch_size = x.shape[0]\n",
        "        n_channels = x.shape[1]\n",
        "        # Set the initial hidden states \n",
        "        initial_hidden_state = torch.zeros(\n",
        "            batch_size, n_channels, self.hidden_size, dtype=torch.float32,\n",
        "            device=self.device)\n",
        "        print(x.shape)\n",
        "        print(initial_hidden_state.shape)\n",
        "\n",
        "        _, out = self.gru(x.flatten(start_dim=2), initial_hidden_state)\n",
        "        # Iterate over the images and pass them through the GRUs.\n",
        "        '''for i, gru in enumerate(self.grus):\n",
        "            # Flatten the image.\n",
        "            img = x[:, i].flatten(start_dim=1)\n",
        "            # If it is the first image, use the initial hidden state.\n",
        "            if i == 0:\n",
        "                h = initial_hidden_state\n",
        "            # Get the forward pass of the GRU.\n",
        "            h, _ = gru(img, h)''';\n",
        "        \n",
        "        \"\"\"# Turn the hidden state to the original shape.\n",
        "        out = out.view(batch_size, n_channels, self.input_size[0],\n",
        "                       self.input_size[1])\"\"\"\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "yQfuFLG5tbx2"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "import wandb\n",
        "import fastcore.all as fc\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from diffusers import UNet2DConditionModel\n",
        "\n",
        "try:\n",
        "    from denoising_diffusion_pytorch.simple_diffusion import UViT\n",
        "except:\n",
        "    raise ImportError(\"Please install denoising_diffusion_pytorch with `pip install denoising_diffusion_pytorch`\")\n",
        "\n",
        "def init_unet(model):\n",
        "    \"From Jeremy's bag of tricks on fastai V2 2023\"\n",
        "    for o in model.down_blocks:\n",
        "        for p in o.resnets:\n",
        "            p.conv2.weight.data.zero_()\n",
        "            for p in fc.L(o.downsamplers): nn.init.orthogonal_(p.conv.weight)\n",
        "\n",
        "    for o in model.up_blocks:\n",
        "        for p in o.resnets: p.conv2.weight.data.zero_()\n",
        "\n",
        "    model.conv_out.weight.data.zero_()\n",
        "\n",
        "class WandbModel:\n",
        "    \"A model that can be saved to wandb\"\n",
        "    @classmethod\n",
        "    def from_checkpoint(cls, model_params, checkpoint_file):\n",
        "        \"Load a UNet2D model from a checkpoint file\"\n",
        "        model = cls(**model_params)\n",
        "        print(f\"Loading model from: {checkpoint_file}\")\n",
        "        model.load_state_dict(torch.load(checkpoint_file))\n",
        "        return model\n",
        "\n",
        "    @classmethod\n",
        "    def from_artifact(cls, model_params, artifact_name):\n",
        "        \"Load a UNet2D model from a wandb.Artifact, need to be run in a wandb run\"\n",
        "        artifact = wandb.use_artifact(artifact_name, type='model')\n",
        "        artifact_dir = Path(artifact.download())\n",
        "        chpt_file = list(artifact_dir.glob(\"*.pth\"))[0]\n",
        "        return cls.from_checkpoint(model_params, chpt_file)\n",
        "\n",
        "def get_unet_params(model_name=\"unet_small\", num_frames=4):\n",
        "    \"Return the parameters for the diffusers UNet2d model\"\n",
        "    if model_name == \"unet_small\":\n",
        "        return dict(\n",
        "            block_out_channels=(16, 32, 64, 128), # number of channels for each block\n",
        "            norm_num_groups=8, # number of groups for the normalization layer\n",
        "            in_channels=num_frames, # number of input channels\n",
        "            out_channels=1, # number of output channels\n",
        "            input_size=(64, 64),\n",
        "            hidden_size=128,\n",
        "            num_images=3,\n",
        "            )\n",
        "    elif model_name == \"unet_big\":\n",
        "        return dict(\n",
        "            block_out_channels=(32, 64, 128, 256), # number of channels for each block\n",
        "            norm_num_groups=8, # number of groups for the normalization layer\n",
        "            in_channels=num_frames, # number of input channels\n",
        "            out_channels=1, # number of output channels\n",
        "            )\n",
        "    else:\n",
        "        raise(f\"Model name not found: {model_name}, choose between 'unet_small' or 'unet_big'\")\n",
        "\n",
        "class UNet2DTemporalCondition(UNet2DConditionModel, WandbModel):\n",
        "    def __init__(self, \n",
        "                 *x, \n",
        "                 input_size: Tuple[int, int], \n",
        "                 hidden_size: Tuple[int, int], \n",
        "                 num_images: int, \n",
        "                 device: str = \"cuda\",\n",
        "                 **kwargs):\n",
        "        super().__init__(*x, **kwargs)\n",
        "        init_unet(self)\n",
        "        self.temporal_encoder = ConvGRU(input_size=input_size,\n",
        "                                        input_dim=1,\n",
        "                                        hidden_dim=1024,\n",
        "                                        kernel_size=(3, 3),\n",
        "                                        num_layers=2,\n",
        "                                        dtype=torch.cuda.FloatTensor,\n",
        "                                        batch_first=True,\n",
        "                                        bias = True,\n",
        "                                        return_all_layers = False).to(device)\n",
        "\n",
        "    def forward(self, *x, **kwargs):\n",
        "        temporal_input = x[0][0][:,:-1] # first three images\n",
        "        _, encoder_hidden_states = self.temporal_encoder(temporal_input.unsqueeze(2).to(self.device))\n",
        "        print(type(encoder_hidden_states))\n",
        "        return super().forward(*x, timestep=x[0][1], encoder_hidden_states=encoder_hidden_states, **kwargs).sample ## Diffusers's UNet2DConditionModel class\n",
        "\n",
        "## Simple Diffusion paper\n",
        "\n",
        "def get_uvit_params(model_name=\"uvit_small\", num_frames=4):\n",
        "    \"Return the parameters for the diffusers UViT model\"\n",
        "    if model_name == \"uvit_small\":\n",
        "        return dict(\n",
        "            dim=512,\n",
        "            ff_mult=2,\n",
        "            vit_depth=4,\n",
        "            channels=4, \n",
        "            patch_size=4,\n",
        "            final_img_itransform=nn.Conv2d(num_frames,1,1)\n",
        "            )\n",
        "    elif model_name == \"uvit_big\":\n",
        "        return dict(\n",
        "            dim=1024,\n",
        "            ff_mult=4,\n",
        "            vit_depth=8,\n",
        "            channels=4, \n",
        "            patch_size=4,\n",
        "            final_img_itransform=nn.Conv2d(num_frames,1,1)\n",
        "            )\n",
        "    else:\n",
        "        raise(f\"Model name not found: {model_name}, choose between 'uvit_small' or 'uvit_big'\")\n",
        "\n",
        "class UViTModel(UViT, WandbModel): pass"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "files = download_dataset(DATASET_ARTIFACT, PROJECT_NAME)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307,
          "referenced_widgets": [
            "cd011c93e27b4a39a62667b288074182",
            "70297a3be6b04a459c4a52798b019815",
            "279c61eb722d4a439de011ab7209cd1a",
            "de1381eec0054721af585754cf3e5457",
            "c723444a2d924fd39ab398a9fd892133",
            "2cce6c90e8bc44de9b213fe17bfe2db0",
            "182363a3ef0c4051953664d1558eaa67",
            "da922874df7c4bc6ab1f68d237a2a9e2"
          ]
        },
        "id": "PQDj5B_WMcrP",
        "outputId": "aecb9cf6-47d9-472f-a7f1-9617c91c6880"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmaidacundo\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.15.3"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230526_104959-28hdy30f</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/maidacundo/cloud_diffusion/runs/28hdy30f' target=\"_blank\">fancy-capybara-20</a></strong> to <a href='https://wandb.ai/maidacundo/cloud_diffusion' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/maidacundo/cloud_diffusion' target=\"_blank\">https://wandb.ai/maidacundo/cloud_diffusion</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/maidacundo/cloud_diffusion/runs/28hdy30f' target=\"_blank\">https://wandb.ai/maidacundo/cloud_diffusion/runs/28hdy30f</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact sample_dataset:v0, 360.00MB. 30 files... \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   30 of 30 files downloaded.  \n",
            "Done. 0:0:0.6\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cd011c93e27b4a39a62667b288074182"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">fancy-capybara-20</strong> at: <a href='https://wandb.ai/maidacundo/cloud_diffusion/runs/28hdy30f' target=\"_blank\">https://wandb.ai/maidacundo/cloud_diffusion/runs/28hdy30f</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20230526_104959-28hdy30f/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "rJzdvyzMx-MI"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "import wandb\n",
        "import numpy as np\n",
        "\n",
        "## For Training\n",
        "\n",
        "def to_wandb_image(img):\n",
        "    \"Convert a tensor to a wandb.Image\"\n",
        "    return wandb.Image(torch.cat(img.split(1), dim=-1).cpu().numpy())\n",
        "\n",
        "def log_images(xt, samples):\n",
        "    \"Log sampled images to wandb\"\n",
        "    device = samples.device\n",
        "    frames = torch.cat([xt[:, :-1,...].to(device), samples], dim=1)\n",
        "    wandb.log({\"sampled_images\": [to_wandb_image(img) for img in frames]})\n",
        "\n",
        "def save_model(model, model_name):\n",
        "    \"Save the model to wandb\"\n",
        "    model_name = f\"{wandb.run.id}_{model_name}\"\n",
        "    models_folder = Path(\"models\")\n",
        "    if not models_folder.exists():\n",
        "        models_folder.mkdir()\n",
        "    torch.save(model.state_dict(), models_folder/f\"{model_name}.pth\")\n",
        "    at = wandb.Artifact(model_name, type=\"model\")\n",
        "    at.add_file(f\"models/{model_name}.pth\")\n",
        "    wandb.log_artifact(at)\n",
        "\n",
        "\n",
        "## For Inference\n",
        "def htile(img):\n",
        "    \"Horizontally tile a batch of images.\"\n",
        "    return torch.cat(img.split(1), dim=-1)\n",
        "\n",
        "def vtile(img):\n",
        "    \"Vertically tile a batch of images.\"\n",
        "    return torch.cat(img.split(1), dim=-2)\n",
        "\n",
        "def vhtile(*imgs):\n",
        "    \"Vertically and horizontally tile a batch of images.\"\n",
        "    return vtile(torch.cat([htile(img) for img in imgs], dim=0))\n",
        "\n",
        "def scale(arr):\n",
        "    \"Scales values of array in [0,1]\"\n",
        "    m, M = arr.min(), arr.max()\n",
        "    return (arr - m) / (M - m)\n",
        "\n",
        "def preprocess_frames(data):\n",
        "    \"Preprocess frames for wandb.Video\"\n",
        "    sdata = scale(data.squeeze())\n",
        "    # print(sdata.shape)\n",
        "    def tfm(frame):\n",
        "        rframe = 255 * frame\n",
        "        return rframe.cpu().numpy().astype(np.uint8)\n",
        "    return [tfm(frame) for frame in sdata]\n",
        "\n",
        "def to_video(data):\n",
        "    \"create wandb.Video container\"\n",
        "    frames = preprocess_frames(data)\n",
        "    vid = np.stack(frames)[:, None, ...]\n",
        "    return wandb.Video(vid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "103WzikeJ05H",
        "outputId": "1f4149ea-33be-45b5-ef97-720ae92dc5bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: fastprogress in /usr/local/lib/python3.10/dist-packages (1.0.3)\n",
            "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.10/dist-packages (0.11.4)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (1.22.4)\n",
            "Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.0.1+cu118)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (23.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8.1->torchmetrics) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8.1->torchmetrics) (16.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.1->torchmetrics) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.1->torchmetrics) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install fastprogress torchmetrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "z1l_534VxxaQ",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import random, argparse\n",
        "from pathlib import Path\n",
        "\n",
        "import wandb\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.optim import AdamW\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim.lr_scheduler import OneCycleLR\n",
        "from torch.utils.data.dataloader import default_collate\n",
        "from torchmetrics import PeakSignalNoiseRatio, StructuralSimilarityIndexMeasure\n",
        "\n",
        "from fastprogress import progress_bar\n",
        "\n",
        "#from cloud_diffusion.wandb import log_images, save_model\n",
        "\n",
        "def noisify_last_frame(frames, noise_func):\n",
        "    \"Noisify the last frame of a sequence\"\n",
        "    past_frames = frames[:,:-1]\n",
        "    last_frame  = frames[:,-1:]\n",
        "    noise, t, e = noise_func(last_frame)\n",
        "    return torch.cat([past_frames, noise], dim=1), t, e\n",
        "\n",
        "def noisify_collate(noise_func): \n",
        "    def _inner(b): \n",
        "        \"Collate function that noisifies the last frame\"\n",
        "        return noisify_last_frame(default_collate(b), noise_func)\n",
        "    return _inner\n",
        "\n",
        "class NoisifyDataloader(DataLoader):\n",
        "    \"\"\"Noisify the last frame of a dataloader by applying \n",
        "    a noise function, after collating the batch\"\"\"\n",
        "    def __init__(self, dataset, *args, noise_func=None, **kwargs):\n",
        "        super().__init__(dataset, *args, collate_fn=noisify_collate(noise_func), **kwargs)\n",
        "\n",
        "class MiniTrainer:\n",
        "    \"A mini trainer for the diffusion process\"\n",
        "    def __init__(self, \n",
        "                 train_dataloader, \n",
        "                 valid_dataloader, \n",
        "                 model, \n",
        "                 sampler, \n",
        "                 device=\"cuda\", \n",
        "                 loss_func=nn.MSELoss(), \n",
        "                 ):\n",
        "        self.train_dataloader = train_dataloader\n",
        "        self.valid_dataloader = valid_dataloader\n",
        "        self.loss_func = loss_func\n",
        "        self.psnr = PeakSignalNoiseRatio().to(device)\n",
        "        self.ssim = StructuralSimilarityIndexMeasure().to(device)\n",
        "        self.model = model.to(device)\n",
        "        self.scaler = torch.cuda.amp.GradScaler()\n",
        "        self.device = device\n",
        "        self.sampler = sampler\n",
        "        self.val_batch = next(iter(valid_dataloader))[0].to(device)  # grab a fixed batch to log predictions\n",
        "    \n",
        "    def train_step(self, loss):\n",
        "        \"Train for one step\"\n",
        "        self.optimizer.zero_grad()\n",
        "        self.scaler.scale(loss).backward()\n",
        "        self.scaler.step(self.optimizer)\n",
        "        self.scaler.update()\n",
        "        self.scheduler.step()\n",
        "\n",
        "    def one_epoch(self, epoch=None):\n",
        "        \"Train for one epoch, log metrics and save model\"\n",
        "        self.model.train()\n",
        "        pbar = progress_bar(self.train_dataloader, leave=False)\n",
        "        for batch in pbar:\n",
        "            frames, t, noise = to_device(batch, device=self.device)\n",
        "            with torch.autocast(\"cuda\"):\n",
        "                predicted_noise = self.model(frames, t)\n",
        "                loss = self.loss_func(noise, predicted_noise)\n",
        "            self.train_step(loss)\n",
        "            wandb.log({\"train_mse\": loss.item(),\n",
        "                       \"learning_rate\": self.scheduler.get_last_lr()[0]})\n",
        "            pbar.comment = f\"epoch={epoch}, MSE={loss.item():2.3f}\"\n",
        "\n",
        "    def one_epoch_validation(self, epoch=None):\n",
        "        \"Validates on val set\"\n",
        "        pbar = progress_bar(self.valid_dataloader, leave=False)\n",
        "        psnr_metric = 0\n",
        "        mse_metric = 0\n",
        "        ssmi_metric = 0\n",
        "        for val_batch in pbar:\n",
        "            frames = val_batch[0].to(self.device)\n",
        "            samples = self.sampler(self.model, past_frames=frames[:,:-1]).to(self.device)\n",
        "            psnr_metric += self.psnr(samples, frames[:,-1]).float().cpu()\n",
        "            ssmi_metric += self.ssim(samples, frames[:,-1]).float().cpu()\n",
        "            mse_metric += self.loss_func(samples, frames[:,-1]).float().cpu()\n",
        "        psnr_metric = psnr_metric / len(self.valid_dataloader)\n",
        "        ssmi_metric = ssmi_metric / len(self.valid_dataloader)\n",
        "        mse_metric = mse_metric / len(self.valid_dataloader)\n",
        "        wandb.log({\"val_psnr\": psnr_metric,\n",
        "                   \"val_ssmi\": ssmi_metric,\n",
        "                   \"val_mse\": mse_metric})\n",
        "\n",
        "    def prepare(self, config):\n",
        "        wandb.config.update(config)\n",
        "        config.total_train_steps = config.epochs * len(self.train_dataloader)\n",
        "        self.optimizer = AdamW(self.model.parameters(), lr=config.lr, eps=1e-5)\n",
        "        self.scheduler = OneCycleLR(self.optimizer, max_lr=config.lr, total_steps=config.total_train_steps)\n",
        "\n",
        "    def fit(self, config):\n",
        "        self.prepare(config)\n",
        "        self.val_batch = self.val_batch[:min(config.n_preds, 8)]  # log first 8 predictions\n",
        "        for epoch in progress_bar(range(config.epochs), total=config.epochs, leave=True):\n",
        "            self.one_epoch(epoch)\n",
        "            self.one_epoch_validation(epoch)\n",
        "            \n",
        "            # log predictions\n",
        "            if epoch % config.log_every_epoch == 0:\n",
        "                samples = self.sampler(self.model, past_frames=self.val_batch[:,:-1])\n",
        "                log_images(self.val_batch, samples)\n",
        "\n",
        "        save_model(self.model, config.model_name)\n",
        "\n",
        "\n",
        "def set_seed(s, reproducible=False):\n",
        "    \"Set random seed for `random`, `torch`, and `numpy` (where available)\"\n",
        "    try: torch.manual_seed(s)\n",
        "    except NameError: pass\n",
        "    try: torch.cuda.manual_seed_all(s)\n",
        "    except NameError: pass\n",
        "    try: np.random.seed(s%(2**32-1))\n",
        "    except NameError: pass\n",
        "    random.seed(s)\n",
        "    if reproducible:\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "\n",
        "def to_device(t, device=\"cpu\"):\n",
        "    if isinstance(t, (tuple, list)):\n",
        "        return [_t.to(device) for _t in t]\n",
        "    elif isinstance(t, torch.Tensor):\n",
        "        return t.to(device)\n",
        "    else:\n",
        "        raise(\"Not a Tensor or list of Tensors\")\n",
        "\n",
        "\n",
        "def ls(path: Path): \n",
        "    \"Return files on Path, sorted\"\n",
        "    return sorted(list(path.iterdir()))\n",
        "\n",
        "\n",
        "def parse_args(config):\n",
        "    \"A brute force way to parse arguments, it is probably not a good idea to use it\"\n",
        "    parser = argparse.ArgumentParser(description='Run training baseline')\n",
        "    for k,v in config.__dict__.items():\n",
        "        parser.add_argument('--'+k, type=type(v), default=v)\n",
        "    args = vars(parser.parse_args())\n",
        "    \n",
        "    # update config with parsed args\n",
        "    for k, v in args.items():\n",
        "        setattr(config, k, v)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "XfTZnkBpyM-O"
      },
      "outputs": [],
      "source": [
        "from functools import partial\n",
        "\n",
        "import torch\n",
        "from fastprogress import progress_bar\n",
        "\n",
        "from diffusers.schedulers import DDIMScheduler\n",
        "\n",
        "\n",
        "## DDPM params\n",
        "## From fastai V2 Course DDPM notebooks\n",
        "betamin,betamax,n_steps = 0.0001,0.02, 1000\n",
        "beta = torch.linspace(betamin, betamax, n_steps)\n",
        "alpha = 1.-beta\n",
        "alphabar = alpha.cumprod(dim=0)\n",
        "sigma = beta.sqrt()\n",
        "\n",
        "def noisify_ddpm(x0):\n",
        "    \"Noise by ddpm\"\n",
        "    device = x0.device\n",
        "    n = len(x0)\n",
        "    t = torch.randint(0, n_steps, (n,), dtype=torch.long)\n",
        "    ε = torch.randn(x0.shape, device=device)\n",
        "    ᾱ_t = alphabar[t].reshape(-1, 1, 1, 1).to(device)\n",
        "    xt = ᾱ_t.sqrt()*x0 + (1-ᾱ_t).sqrt()*ε\n",
        "    return xt, t.to(device), ε\n",
        "\n",
        "@torch.no_grad()\n",
        "def diffusers_sampler(model, past_frames, sched, **kwargs):\n",
        "    \"Using Diffusers built-in samplers\"\n",
        "    model.eval()\n",
        "    device = next(model.parameters()).device\n",
        "    new_frame = torch.randn_like(past_frames[:,-1:], dtype=past_frames.dtype, device=device)\n",
        "    preds = []\n",
        "    pbar = progress_bar(sched.timesteps, leave=False)\n",
        "    for t in pbar:\n",
        "        pbar.comment = f\"DDIM Sampler: frame {t}\"\n",
        "        noise = model(torch.cat([past_frames, new_frame], dim=1), t)\n",
        "        new_frame = sched.step(noise, t, new_frame, **kwargs).prev_sample\n",
        "        preds.append(new_frame.float().cpu())\n",
        "    return preds[-1]\n",
        "\n",
        "def ddim_sampler(steps=350, eta=1.):\n",
        "    \"DDIM sampler, faster and a bit better than the built-in sampler\"\n",
        "    ddim_sched = DDIMScheduler()\n",
        "    ddim_sched.set_timesteps(steps)\n",
        "    return partial(diffusers_sampler, sched=ddim_sched, eta=eta)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "-uNhPHz3wpqD"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "from types import SimpleNamespace\n",
        "\n",
        "import wandb\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# from cloud_diffusion.dataset import download_dataset, CloudDataset\n",
        "#from cloud_diffusion.utils import NoisifyDataloader, MiniTrainer, set_seed, parse_args\n",
        "#from cloud_diffusion.ddpm import noisify_ddpm, ddim_sampler\n",
        "# from cloud_diffusion.models import UNet2D, get_unet_params\n",
        "\n",
        "\n",
        "PROJECT_NAME = \"cloud_diffusion\"\n",
        "DATASET_ARTIFACT = 'maidacundo/cloud_diffusion/sample_dataset:v0'\n",
        "\n",
        "config = SimpleNamespace(    \n",
        "    epochs=50, # number of epochs\n",
        "    model_name=\"unet_small\", # model name to save [unet_small, unet_big]\n",
        "    strategy=\"ddpm\", # strategy to use ddpm\n",
        "    noise_steps=1000, # number of noise steps on the diffusion process\n",
        "    sampler_steps=333, # number of sampler steps on the diffusion process\n",
        "    seed=42, # random seed\n",
        "    batch_size=128, # batch size\n",
        "    img_size=64, # image size\n",
        "    device=\"cuda\", # device\n",
        "    num_workers=8, # number of workers for dataloader\n",
        "    num_frames=4, # number of frames to use as input\n",
        "    lr=5e-4, # learning rate\n",
        "    validation_days=3, # number of days to use for validation\n",
        "    log_every_epoch=5, # log every n epochs to wandb\n",
        "    n_preds=8, # number of predictions to make\n",
        "    )\n",
        "\n",
        "def train_func(config):\n",
        "    config.model_params = get_unet_params(config.model_name, config.num_frames)\n",
        "\n",
        "    set_seed(config.seed)\n",
        "    device = torch.device(config.device)\n",
        "\n",
        "    # downlaod the dataset from the wandb.Artifact\n",
        "    files = download_dataset(DATASET_ARTIFACT, PROJECT_NAME)\n",
        "    train_days, valid_days = files[:-config.validation_days], files[-config.validation_days:]\n",
        "    train_ds = CloudDataset(files=files[:1], num_frames=config.num_frames, img_size=config.img_size)\n",
        "    valid_ds = CloudDataset(files=files[:1], num_frames=config.num_frames, img_size=config.img_size).shuffle()\n",
        "\n",
        "    # DDPM dataloaders\n",
        "    train_dataloader = NoisifyDataloader(train_ds, config.batch_size, shuffle=True, \n",
        "                                         noise_func=noisify_ddpm,  num_workers=config.num_workers)\n",
        "    valid_dataloader = NoisifyDataloader(valid_ds, config.batch_size, shuffle=False, \n",
        "                                          noise_func=noisify_ddpm,  num_workers=config.num_workers)\n",
        "\n",
        "    # model setup\n",
        "    model = UNet2DTemporalCondition(**config.model_params)\n",
        "\n",
        "    # sampler\n",
        "    sampler = ddim_sampler(steps=config.sampler_steps)\n",
        "\n",
        "    # A simple training loop\n",
        "    trainer = MiniTrainer(train_dataloader, valid_dataloader, model, sampler, device)\n",
        "    trainer.fit(config)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config = SimpleNamespace(\n",
        "    epochs=50, # number of epochs\n",
        "    model_name=\"unet_small\", # model name to save [unet_small, unet_big]\n",
        "    strategy=\"ddpm\", # strategy to use ddpm\n",
        "    noise_steps=1000, # number of noise steps on the diffusion process\n",
        "    sampler_steps=333, # number of sampler steps on the diffusion process\n",
        "    seed=42, # random seed\n",
        "    batch_size=8, # batch size\n",
        "    img_size=64, # image size\n",
        "    device=\"cuda\", # device\n",
        "    num_workers=8, # number of workers for dataloader\n",
        "    num_frames=4, # number of frames to use as input\n",
        "    lr=5e-4, # learning rate\n",
        "    validation_days=3, # number of days to use for validation\n",
        "    log_every_epoch=5, # log every n epochs to wandb\n",
        "    n_preds=8, # number of predictions to make\n",
        "    )\n",
        "config.model_params = get_unet_params(config.model_name, config.num_frames)\n",
        "\n",
        "train_ds = CloudDataset(files=files[:1], num_frames=config.num_frames, img_size=config.img_size)\n",
        "train_dataloader = NoisifyDataloader(train_ds, config.batch_size, shuffle=True, \n",
        "                                      noise_func=noisify_ddpm,  num_workers=config.num_workers)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "Bx_cNEulMI4e",
        "outputId": "5dff6e52-9786-4e5c-9ec9-3cd3ff16d5b6"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "next(iter(train_dataloader))[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K55zYeLCqfwB",
        "outputId": "e43273c8-57fc-4b99-8bac-ecf9231a5aec"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([129, 906, 566,  47, 783, 907, 712, 668])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = UNet2DTemporalCondition(**config.model_params).to('cuda')\n",
        "out = model(next(iter(train_dataloader)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "id": "4BdYXDVSL4K9",
        "outputId": "5489e9b1-bdbb-40ee-cb36-99b57ea1e88f"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
              "\u001b[31m│\u001b[0m in \u001b[92m<cell line: 2>\u001b[0m:\u001b[94m2\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1501\u001b[0m in \u001b[92m_call_impl\u001b[0m            \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1498 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._backward_pre_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks   \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1499 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_backward_pre_hooks \u001b[95mor\u001b[0m _global_backward_hooks                   \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1500 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1501 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*args, **kwargs)                                          \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1502 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                          \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1503 \u001b[0m\u001b[2m│   │   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1504 \u001b[0m\u001b[2m│   │   \u001b[0mbackward_pre_hooks = []                                                           \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m in \u001b[92mforward\u001b[0m:\u001b[94m91\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/diffusers/models/\u001b[0m\u001b[1;33munet_2d_condition.py\u001b[0m:\u001b[94m650\u001b[0m in \u001b[92mforward\u001b[0m     \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m647 \u001b[0m\u001b[2m│   │   \u001b[0mforward_upsample_size = \u001b[94mFalse\u001b[0m                                                      \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m648 \u001b[0m\u001b[2m│   │   \u001b[0mupsample_size = \u001b[94mNone\u001b[0m                                                               \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m649 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m650 \u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96many\u001b[0m(s % default_overall_up_factor != \u001b[94m0\u001b[0m \u001b[94mfor\u001b[0m s \u001b[95min\u001b[0m sample.shape[-\u001b[94m2\u001b[0m:]):             \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m651 \u001b[0m\u001b[2m│   │   │   \u001b[0mlogger.info(\u001b[33m\"\u001b[0m\u001b[33mForward upsample size to force interpolation output size.\u001b[0m\u001b[33m\"\u001b[0m)       \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m652 \u001b[0m\u001b[2m│   │   │   \u001b[0mforward_upsample_size = \u001b[94mTrue\u001b[0m                                                   \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m653 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
              "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
              "\u001b[1;91mAttributeError: \u001b[0m\u001b[32m'tuple'\u001b[0m object has no attribute \u001b[32m'shape'\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;cell line: 2&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2</span>                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1501</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1498 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1499 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1500 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1501 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*args, **kwargs)                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1502 │   │   # Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1503 │   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1504 │   │   </span>backward_pre_hooks = []                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">91</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/diffusers/models/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">unet_2d_condition.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">650</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">647 │   │   </span>forward_upsample_size = <span style=\"color: #0000ff; text-decoration-color: #0000ff\">False</span>                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">648 │   │   </span>upsample_size = <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">649 │   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>650 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">any</span>(s % default_overall_up_factor != <span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> s <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> sample.shape[-<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2</span>:]):             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">651 │   │   │   </span>logger.info(<span style=\"color: #808000; text-decoration-color: #808000\">\"Forward upsample size to force interpolation output size.\"</span>)       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">652 │   │   │   </span>forward_upsample_size = <span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">653 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
              "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">AttributeError: </span><span style=\"color: #008000; text-decoration-color: #008000\">'tuple'</span> object has no attribute <span style=\"color: #008000; text-decoration-color: #008000\">'shape'</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 799,
          "referenced_widgets": [
            "50c1afb835e14bf09ba4fdbcaffc4a52",
            "c800c70b4b3547d6814d7c4c3cfae5e4",
            "5d601f04678f4ca0b1df5536d2e11314",
            "58d2068cfbc04680b5e071d69d17ce49",
            "9742e31f46f6460dbf24def46f23639d",
            "54a14673a5b341c29e15671f2b75ef85",
            "e0cf8dc8837640389da3b93cdada4878",
            "aed3236933374384990732149cb45ff7"
          ]
        },
        "id": "5XPIJlC0xO0T",
        "outputId": "f5a1ada4-4bc5-42d0-c66f-80a2ab4c454a",
        "tags": []
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.15.3"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230526_083049-zf8h1jw6</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/ai-industry/cloud_diffusion/runs/zf8h1jw6' target=\"_blank\">cool-elevator-21</a></strong> to <a href='https://wandb.ai/ai-industry/cloud_diffusion' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/ai-industry/cloud_diffusion' target=\"_blank\">https://wandb.ai/ai-industry/cloud_diffusion</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/ai-industry/cloud_diffusion/runs/zf8h1jw6' target=\"_blank\">https://wandb.ai/ai-industry/cloud_diffusion/runs/zf8h1jw6</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact sample_dataset:v0, 360.00MB. 30 files... \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   30 of 30 files downloaded.  \n",
            "Done. 0:0:0.8\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      <progress value='0' class='' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      0.00% [0/50 00:00&lt;?]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      <progress value='0' class='' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      0.00% [0/1 00:00&lt;?]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'tuple'>\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:red\">(failed 1).</strong> Press Control-C to abort syncing."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "50c1afb835e14bf09ba4fdbcaffc4a52"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">cool-elevator-21</strong> at: <a href='https://wandb.ai/ai-industry/cloud_diffusion/runs/zf8h1jw6' target=\"_blank\">https://wandb.ai/ai-industry/cloud_diffusion/runs/zf8h1jw6</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20230526_083049-zf8h1jw6/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
              "\u001b[31m│\u001b[0m in \u001b[92m<cell line: 1>\u001b[0m:\u001b[94m2\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m in \u001b[92mtrain_func\u001b[0m:\u001b[94m61\u001b[0m                                                                                 \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m in \u001b[92mfit\u001b[0m:\u001b[94m109\u001b[0m                                                                                       \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m in \u001b[92mone_epoch\u001b[0m:\u001b[94m73\u001b[0m                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1501\u001b[0m in \u001b[92m_call_impl\u001b[0m            \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1498 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._backward_pre_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks   \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1499 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_backward_pre_hooks \u001b[95mor\u001b[0m _global_backward_hooks                   \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1500 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1501 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*args, **kwargs)                                          \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1502 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                          \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1503 \u001b[0m\u001b[2m│   │   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1504 \u001b[0m\u001b[2m│   │   \u001b[0mbackward_pre_hooks = []                                                           \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m in \u001b[92mforward\u001b[0m:\u001b[94m82\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1501\u001b[0m in \u001b[92m_call_impl\u001b[0m            \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1498 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._backward_pre_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks   \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1499 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_backward_pre_hooks \u001b[95mor\u001b[0m _global_backward_hooks                   \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1500 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1501 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*args, **kwargs)                                          \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1502 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                          \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1503 \u001b[0m\u001b[2m│   │   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1504 \u001b[0m\u001b[2m│   │   \u001b[0mbackward_pre_hooks = []                                                           \u001b[31m│\u001b[0m\n",
              "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
              "\u001b[1;91mTypeError: \u001b[0m\u001b[1;35mTemporalEncoder.forward\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m takes \u001b[1;36m2\u001b[0m positional arguments but \u001b[1;36m3\u001b[0m were given\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;cell line: 1&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2</span>                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">train_func</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">61</span>                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">fit</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">109</span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">one_epoch</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">73</span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1501</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1498 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1499 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1500 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1501 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*args, **kwargs)                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1502 │   │   # Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1503 │   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1504 │   │   </span>backward_pre_hooks = []                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">82</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1501</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1498 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1499 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1500 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1501 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*args, **kwargs)                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1502 │   │   # Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1503 │   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1504 │   │   </span>backward_pre_hooks = []                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
              "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">TypeError: </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">TemporalEncoder.forward</span><span style=\"font-weight: bold\">()</span> takes <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> positional arguments but <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> were given\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "with wandb.init(project=PROJECT_NAME, entity='ai-industry', config=config, tags=[\"ddpm\", config.model_name]):\n",
        "    train_func(config)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "cd011c93e27b4a39a62667b288074182": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_70297a3be6b04a459c4a52798b019815",
              "IPY_MODEL_279c61eb722d4a439de011ab7209cd1a"
            ],
            "layout": "IPY_MODEL_de1381eec0054721af585754cf3e5457"
          }
        },
        "70297a3be6b04a459c4a52798b019815": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c723444a2d924fd39ab398a9fd892133",
            "placeholder": "​",
            "style": "IPY_MODEL_2cce6c90e8bc44de9b213fe17bfe2db0",
            "value": "0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\r"
          }
        },
        "279c61eb722d4a439de011ab7209cd1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_182363a3ef0c4051953664d1558eaa67",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_da922874df7c4bc6ab1f68d237a2a9e2",
            "value": 1
          }
        },
        "de1381eec0054721af585754cf3e5457": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c723444a2d924fd39ab398a9fd892133": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2cce6c90e8bc44de9b213fe17bfe2db0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "182363a3ef0c4051953664d1558eaa67": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "da922874df7c4bc6ab1f68d237a2a9e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "50c1afb835e14bf09ba4fdbcaffc4a52": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c800c70b4b3547d6814d7c4c3cfae5e4",
              "IPY_MODEL_5d601f04678f4ca0b1df5536d2e11314"
            ],
            "layout": "IPY_MODEL_58d2068cfbc04680b5e071d69d17ce49"
          }
        },
        "c800c70b4b3547d6814d7c4c3cfae5e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9742e31f46f6460dbf24def46f23639d",
            "placeholder": "​",
            "style": "IPY_MODEL_54a14673a5b341c29e15671f2b75ef85",
            "value": "0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\r"
          }
        },
        "5d601f04678f4ca0b1df5536d2e11314": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e0cf8dc8837640389da3b93cdada4878",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_aed3236933374384990732149cb45ff7",
            "value": 1
          }
        },
        "58d2068cfbc04680b5e071d69d17ce49": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9742e31f46f6460dbf24def46f23639d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "54a14673a5b341c29e15671f2b75ef85": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e0cf8dc8837640389da3b93cdada4878": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aed3236933374384990732149cb45ff7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}