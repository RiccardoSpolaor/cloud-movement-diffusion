{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "592KgIBCtbxt"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xn_pBvNhtbxv"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "def ls(path: Path): \n",
        "    \"Return files on Path, sorted\"\n",
        "    return sorted(list(path.iterdir()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "FsmkDobltqUd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bae5e9c2-9fc5-46ba-ba36-0773780a63aa"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.2/519.2 kB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m934.9/934.9 kB\u001b[0m \u001b[31m68.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m206.5/206.5 kB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m219.1/219.1 kB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q wandb tqdm matplotlib fastprogress torchmetrics diffusers denoising_diffusion_pytorch fastcore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ZRHQ37tJ05B",
        "outputId": "bc6f4cc1-bb5b-40e8-c02f-de8f455cfb1d"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import wandb\n",
        "wandb.login(key='6b22cbf359c5924f4500afc1ae572d6827998186', relogin=True, force=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Qzo4ppMBtbxx"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision.transforms as T\n",
        "import wandb\n",
        "from tqdm import tqdm as progress_bar\n",
        "import cv2\n",
        "\n",
        "# from cloud_diffusion.utils import ls\n",
        "\n",
        "PROJECT_NAME = \"cloud_diffusion\"\n",
        "DATASET_ARTIFACT = 'ai-industry/cloud_diffusion/SEVIR:v0'\n",
        "\n",
        "class DummyNextFrameDataset:\n",
        "    \"Dataset that returns random images\"\n",
        "    def __init__(self, num_frames=4, img_size=64, N=1000):\n",
        "        self.img_size = img_size\n",
        "        self.num_frames = num_frames\n",
        "        self.N = N\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.randn(self.num_frames, self.img_size, self.img_size)\n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.N\n",
        "\n",
        "\n",
        "class CloudDataset:\n",
        "    \"\"\"Dataset for cloud images\n",
        "    It loads numpy files from wandb artifact and stacks them into a single array\n",
        "    It also applies some transformations to the images\n",
        "    \"\"\"\n",
        "    def __init__(self, \n",
        "                 files, # list of numpy files to load (they come from the artifact)\n",
        "                 num_frames=4, # how many consecutive frames to stack\n",
        "                 scale=True, # if we images to interval [-0.5, 0.5]\n",
        "                 img_size=64, # resize dim, original images are big (446, 780)\n",
        "                 valid=False, # if True, transforms are deterministic\n",
        "                ):\n",
        "        self.means=[]\n",
        "        self.stds=[]\n",
        "        tfms = [T.Normalize(self.means, self.stds)]\n",
        "        tfms += [T.RandomCrop(img_size)] if not valid else [T.CenterCrop(img_size)]\n",
        "        self.tfms = T.Compose(tfms)\n",
        "        self.load_data(files, num_frames, scale)\n",
        "\n",
        "        \n",
        "    def _scale(self, arr):\n",
        "        \"Scales values of array in [0,1]\"\n",
        "        m, M = arr.min(), arr.max()\n",
        "        return (arr - m) / (M - m)\n",
        "\n",
        "    def _calculate_mean_std(self, arr):\n",
        "        \"Calculate mean and std for normalization\"\n",
        "        mean, std = arr.mean(), arr.std()\n",
        "        self.means.append(mean)\n",
        "        self.stds.append(std)\n",
        "\n",
        "    def _resize(self, arr, img_size):\n",
        "        num_events, num_frames = arr.shape[0], arr.shape[3]\n",
        "        resized_array = np.empty((num_events, img_size, img_size, num_frames))\n",
        "        for event in range(num_events):\n",
        "            resized_array[event] = cv2.resize(arr[event], (img_size, img_size))\n",
        "        return resized_array.transpose((0, 3, 1, 2))\n",
        "    \n",
        "    def load_channel(self, file, scale=True):\n",
        "        one_channel = np.load(file)\n",
        "        one_channel = one_channel.astype('float32')\n",
        "        if scale:\n",
        "            one_day = 0.5 - self._scale(one_channel)\n",
        "            self._calculate_mean_std(one_channel)\n",
        "        return one_channel\n",
        "\n",
        "    def create_windows(self, data, num_frames):\n",
        "        windows = []\n",
        "        for event in data:\n",
        "            wds = np.lib.stride_tricks.sliding_window_view(\n",
        "                        event,\n",
        "                        num_frames,\n",
        "                        axis=0)[::num_frames].transpose(0,4,1,2,3) # (windows, frames, channels, height, width)\n",
        "            windows.append(wds)\n",
        "        windows = np.array(windows)\n",
        "        shape = windows.shape\n",
        "        windows = windows.reshape(shape[0] * shape[1], shape[2], shape[3], shape[4], shape[5])\n",
        "        windows = windows.astype('float32')\n",
        "        return windows # (batch, channels, frames, height, width)\n",
        "\n",
        "    def load_data(self, files, num_frames, scale, img_size=64):\n",
        "        \"Loads all data into a single array self.data\"\n",
        "        channels = []\n",
        "        # Load all channels\n",
        "        for file in progress_bar(files, leave=False):\n",
        "            one_channel = self.load_channel(file, scale)\n",
        "            resized_array = self._resize(one_channel, img_size)\n",
        "            channels.append(resized_array)\n",
        "            del one_channel\n",
        "        all_channels = np.stack(channels, axis=2)\n",
        "        self.data = self.create_windows(all_channels, num_frames)\n",
        "\n",
        "    def shuffle(self):\n",
        "        \"\"\"Shuffles the dataset, useful for getting \n",
        "        interesting samples on the validation dataset\"\"\"\n",
        "        idxs = torch.randperm(len(self.data))\n",
        "        self.data = self.data[idxs]\n",
        "        return self\n",
        "\n",
        "    @staticmethod\n",
        "    def _scale(arr):\n",
        "        \"Scales values of array in [0,1]\"\n",
        "        m, M = arr.min(), arr.max()\n",
        "        return (arr - m) / (M - m)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return self.tfms(torch.from_numpy(self.data[idx]))\n",
        "    \n",
        "    def __len__(self): return len(self.data)\n",
        "\n",
        "    def save(self, fname=\"cloud_frames.npy\"):\n",
        "        np.save(fname, self.data)\n",
        "\n",
        "\n",
        "class CloudDatasetInference(CloudDataset):\n",
        "     def load_data(self, files, num_frames=None, scale=None):\n",
        "        \"Loads all data into a single array self.data\"\n",
        "        data = []\n",
        "        max_length = 100\n",
        "        # TODO: download everything\n",
        "        for file in files:\n",
        "            one_day = self.load_day(file, scale)\n",
        "            data.append(one_day)\n",
        "            max_length = min(max_length, len(one_day))\n",
        "        self.data = np.stack([d[:max_length] for d in data], axis=0).squeeze()\n",
        "\n",
        "\n",
        "def download_dataset(at_name, project_name):\n",
        "    \"Downloads dataset from wandb artifact\"\n",
        "    def _get_dataset(run):\n",
        "        artifact = run.use_artifact(at_name, type='dataset')\n",
        "        return artifact.download()\n",
        "\n",
        "    if wandb.run is not None:\n",
        "        run = wandb.run\n",
        "        artifact_dir = _get_dataset(run)\n",
        "    else:\n",
        "        run = wandb.init(project=project_name, job_type=\"download_dataset\")\n",
        "        artifact_dir = _get_dataset(run)\n",
        "        run.finish()\n",
        "\n",
        "    files = ls(Path(artifact_dir))\n",
        "    return files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oS2y3URjtbx1"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "xi_MUZcOXMcn"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.autograd import Variable\n",
        "\n",
        "\n",
        "class ConvGRUCell(nn.Module):\n",
        "    def __init__(self, input_size, input_dim, hidden_dim, kernel_size, bias, dtype):\n",
        "        \"\"\"\n",
        "        Initialize the ConvLSTM cell\n",
        "        :param input_size: (int, int)\n",
        "            Height and width of input tensor as (height, width).\n",
        "        :param input_dim: int\n",
        "            Number of channels of input tensor.\n",
        "        :param hidden_dim: int\n",
        "            Number of channels of hidden state.\n",
        "        :param kernel_size: (int, int)\n",
        "            Size of the convolutional kernel.\n",
        "        :param bias: bool\n",
        "            Whether or not to add the bias.\n",
        "        :param dtype: torch.cuda.FloatTensor or torch.FloatTensor\n",
        "            Whether or not to use cuda.\n",
        "        \"\"\"\n",
        "        super(ConvGRUCell, self).__init__()\n",
        "        self.height, self.width = input_size\n",
        "        self.padding = kernel_size[0] // 2, kernel_size[1] // 2\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.bias = bias\n",
        "        self.dtype = dtype\n",
        "\n",
        "        self.conv_gates = nn.Conv2d(in_channels=input_dim + hidden_dim,\n",
        "                                    out_channels=2*self.hidden_dim,  # for update_gate,reset_gate respectively\n",
        "                                    kernel_size=kernel_size,\n",
        "                                    padding=self.padding,\n",
        "                                    bias=self.bias)\n",
        "\n",
        "        self.conv_can = nn.Conv2d(in_channels=input_dim+hidden_dim,\n",
        "                              out_channels=self.hidden_dim, # for candidate neural memory\n",
        "                              kernel_size=kernel_size,\n",
        "                              padding=self.padding,\n",
        "                              bias=self.bias)\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        return (Variable(torch.zeros(batch_size, self.hidden_dim, self.height, self.width)).type(self.dtype))\n",
        "\n",
        "    def forward(self, input_tensor, h_cur):\n",
        "        \"\"\"\n",
        "\n",
        "        :param self:\n",
        "        :param input_tensor: (b, c, h, w)\n",
        "            input is actually the target_model\n",
        "        :param h_cur: (b, c_hidden, h, w)\n",
        "            current hidden and cell states respectively\n",
        "        :return: h_next,\n",
        "            next hidden state\n",
        "        \"\"\"\n",
        "        combined = torch.cat([input_tensor, h_cur], dim=1)\n",
        "        combined_conv = self.conv_gates(combined)\n",
        "\n",
        "        gamma, beta = torch.split(combined_conv, self.hidden_dim, dim=1)\n",
        "        reset_gate = torch.sigmoid(gamma)\n",
        "        update_gate = torch.sigmoid(beta)\n",
        "\n",
        "        \n",
        "        combined = torch.cat([input_tensor, reset_gate*h_cur], dim=1)\n",
        "        cc_cnm = self.conv_can(combined)\n",
        "        cnm = torch.tanh(cc_cnm)\n",
        "\n",
        "        h_next = (1 - update_gate) * h_cur + update_gate * cnm\n",
        "        return h_next\n",
        "\n",
        "\n",
        "class ConvGRU(nn.Module):\n",
        "    def __init__(self, input_size, input_dim, hidden_dim, kernel_size, num_layers,\n",
        "                 dtype, batch_first=False, bias=True, return_all_layers=False):\n",
        "        \"\"\"\n",
        "\n",
        "        :param input_size: (int, int)\n",
        "            Height and width of input tensor as (height, width).\n",
        "        :param input_dim: int e.g. 256\n",
        "            Number of channels of input tensor.\n",
        "        :param hidden_dim: int e.g. 1024\n",
        "            Number of channels of hidden state.\n",
        "        :param kernel_size: (int, int)\n",
        "            Size of the convolutional kernel.\n",
        "        :param num_layers: int\n",
        "            Number of ConvLSTM layers\n",
        "        :param dtype: torch.cuda.FloatTensor or torch.FloatTensor\n",
        "            Whether or not to use cuda.\n",
        "        :param alexnet_path: str\n",
        "            pretrained alexnet parameters\n",
        "        :param batch_first: bool\n",
        "            if the first position of array is batch or not\n",
        "        :param bias: bool\n",
        "            Whether or not to add the bias.\n",
        "        :param return_all_layers: bool\n",
        "            if return hidden and cell states for all layers\n",
        "        \"\"\"\n",
        "        super(ConvGRU, self).__init__()\n",
        "\n",
        "        # Make sure that both `kernel_size` and `hidden_dim` are lists having len == num_layers\n",
        "        kernel_size = self._extend_for_multilayer(kernel_size, num_layers)\n",
        "        hidden_dim  = self._extend_for_multilayer(hidden_dim, num_layers)\n",
        "        if not len(kernel_size) == len(hidden_dim) == num_layers:\n",
        "            raise ValueError('Inconsistent list length.')\n",
        "\n",
        "        self.height, self.width = input_size\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.kernel_size = kernel_size\n",
        "        self.dtype = dtype\n",
        "        self.num_layers = num_layers\n",
        "        self.batch_first = batch_first\n",
        "        self.bias = bias\n",
        "        self.return_all_layers = return_all_layers\n",
        "\n",
        "        cell_list = []\n",
        "        for i in range(0, self.num_layers):\n",
        "            cur_input_dim = input_dim if i == 0 else hidden_dim[i - 1]\n",
        "            cell_list.append(ConvGRUCell(input_size=(self.height, self.width),\n",
        "                                         input_dim=cur_input_dim,\n",
        "                                         hidden_dim=self.hidden_dim[i],\n",
        "                                         kernel_size=self.kernel_size[i],\n",
        "                                         bias=self.bias,\n",
        "                                         dtype=self.dtype))\n",
        "\n",
        "        # convert python list to pytorch module\n",
        "        self.cell_list = nn.ModuleList(cell_list)\n",
        "\n",
        "    def forward(self, input_tensor, hidden_state=None):\n",
        "        \"\"\"\n",
        "\n",
        "        :param input_tensor: (b, t, c, h, w) or (t,b,c,h,w) depends on if batch first or not\n",
        "            extracted features from alexnet\n",
        "        :param hidden_state:\n",
        "        :return: layer_output_list, last_state_list\n",
        "        \"\"\"\n",
        "        if not self.batch_first:\n",
        "            # (t, b, c, h, w) -> (b, t, c, h, w)\n",
        "            input_tensor = input_tensor.permute(1, 0, 2, 3, 4)\n",
        "\n",
        "        # Implement stateful ConvLSTM\n",
        "        if hidden_state is not None:\n",
        "            raise NotImplementedError()\n",
        "        else:\n",
        "            hidden_state = self._init_hidden(batch_size=input_tensor.size(0))\n",
        "\n",
        "        layer_output_list = []\n",
        "        last_state_list   = []\n",
        "\n",
        "        seq_len = input_tensor.size(1)\n",
        "        cur_layer_input = input_tensor\n",
        "\n",
        "        for layer_idx in range(self.num_layers):\n",
        "            h = hidden_state[layer_idx]\n",
        "            output_inner = []\n",
        "            for t in range(seq_len):\n",
        "                # input current hidden and cell state then compute the next hidden and cell state through ConvLSTMCell forward function\n",
        "                h = self.cell_list[layer_idx](input_tensor=cur_layer_input[:, t, :, :, :], # (b,t,c,h,w)\n",
        "                                              h_cur=h)\n",
        "                output_inner.append(h)\n",
        "\n",
        "            layer_output = torch.stack(output_inner, dim=1)\n",
        "            cur_layer_input = layer_output\n",
        "\n",
        "            layer_output_list.append(layer_output)\n",
        "            last_state_list.append([h])\n",
        "\n",
        "        if not self.return_all_layers:\n",
        "            layer_output_list = layer_output_list[-1:]\n",
        "            last_state_list   = last_state_list[-1:]\n",
        "\n",
        "        return layer_output_list, last_state_list\n",
        "\n",
        "    def _init_hidden(self, batch_size):\n",
        "        init_states = []\n",
        "        for i in range(self.num_layers):\n",
        "            init_states.append(self.cell_list[i].init_hidden(batch_size))\n",
        "        return init_states\n",
        "\n",
        "    @staticmethod\n",
        "    def _check_kernel_size_consistency(kernel_size):\n",
        "        if not (isinstance(kernel_size, tuple) or\n",
        "                    (isinstance(kernel_size, list) and all([isinstance(elem, tuple) for elem in kernel_size]))):\n",
        "            raise ValueError('`kernel_size` must be tuple or list of tuples')\n",
        "\n",
        "    @staticmethod\n",
        "    def _extend_for_multilayer(param, num_layers):\n",
        "        if not isinstance(param, list):\n",
        "            param = [param] * num_layers\n",
        "        return param"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "XotWTQaPJ05G"
      },
      "outputs": [],
      "source": [
        "from typing import Tuple\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "class TemporalEncoder(nn.Module):\n",
        "    def __init__(\n",
        "        self, \n",
        "        input_size: Tuple[int, int], \n",
        "        hidden_size: int,\n",
        "        num_images: int, \n",
        "        device: str\n",
        "        ) -> None:\n",
        "        super().__init__()\n",
        "        # Set the input size of the image.\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        # Set the size of the flattened image.\n",
        "        self.flatten_size = input_size[0] * input_size[1]\n",
        "        # Set a list of GRUs, one for each image.\n",
        "        self.gru = nn.GRU(\n",
        "            self.flatten_size, hidden_size, batch_first=True)\n",
        "        #self.grus = nn.ModuleList(\n",
        "        #    [nn.GRU(self.flatten_size, self.flatten_size)\n",
        "        #     for _ in range(num_images)])\n",
        "        # Set the device used for the computations.\n",
        "        self.to(device)\n",
        "        self.device = device\n",
        "\n",
        "    def to(self, device: str) -> None:\n",
        "        super().to(device)\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, x: torch.FloatTensor) -> torch.FloatTensor:\n",
        "        batch_size = x.shape[0]\n",
        "        n_channels = x.shape[1]\n",
        "        # Set the initial hidden states \n",
        "        initial_hidden_state = torch.zeros(\n",
        "            batch_size, n_channels, self.hidden_size, dtype=torch.float32,\n",
        "            device=self.device)\n",
        "\n",
        "        _, out = self.gru(x.flatten(start_dim=2), initial_hidden_state)\n",
        "        # Iterate over the images and pass them through the GRUs.\n",
        "        '''for i, gru in enumerate(self.grus):\n",
        "            # Flatten the image.\n",
        "            img = x[:, i].flatten(start_dim=1)\n",
        "            # If it is the first image, use the initial hidden state.\n",
        "            if i == 0:\n",
        "                h = initial_hidden_state\n",
        "            # Get the forward pass of the GRU.\n",
        "            h, _ = gru(img, h)''';\n",
        "        \n",
        "        \"\"\"# Turn the hidden state to the original shape.\n",
        "        out = out.view(batch_size, n_channels, self.input_size[0],\n",
        "                       self.input_size[1])\"\"\"\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "yQfuFLG5tbx2"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "import wandb\n",
        "import fastcore.all as fc\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from diffusers import UNet2DModel\n",
        "\n",
        "try:\n",
        "    from denoising_diffusion_pytorch.simple_diffusion import UViT\n",
        "except:\n",
        "    raise ImportError(\"Please install denoising_diffusion_pytorch with `pip install denoising_diffusion_pytorch`\")\n",
        "\n",
        "def init_unet(model):\n",
        "    \"From Jeremy's bag of tricks on fastai V2 2023\"\n",
        "    for o in model.down_blocks:\n",
        "        for p in o.resnets:\n",
        "            p.conv2.weight.data.zero_()\n",
        "            for p in fc.L(o.downsamplers): nn.init.orthogonal_(p.conv.weight)\n",
        "\n",
        "    for o in model.up_blocks:\n",
        "        for p in o.resnets: p.conv2.weight.data.zero_()\n",
        "\n",
        "    model.conv_out.weight.data.zero_()\n",
        "\n",
        "class WandbModel:\n",
        "    \"A model that can be saved to wandb\"\n",
        "    @classmethod\n",
        "    def from_checkpoint(cls, model_params, checkpoint_file):\n",
        "        \"Load a UNet2D model from a checkpoint file\"\n",
        "        model = cls(**model_params)\n",
        "        print(f\"Loading model from: {checkpoint_file}\")\n",
        "        model.load_state_dict(torch.load(checkpoint_file))\n",
        "        return model\n",
        "\n",
        "    @classmethod\n",
        "    def from_artifact(cls, model_params, artifact_name):\n",
        "        \"Load a UNet2D model from a wandb.Artifact, need to be run in a wandb run\"\n",
        "        artifact = wandb.use_artifact(artifact_name, type='model')\n",
        "        artifact_dir = Path(artifact.download())\n",
        "        chpt_file = list(artifact_dir.glob(\"*.pth\"))[0]\n",
        "        return cls.from_checkpoint(model_params, chpt_file)\n",
        "\n",
        "def get_unet_params(model_name=\"unet_small\", num_frames=4):\n",
        "    \"Return the parameters for the diffusers UNet2d model\"\n",
        "    if model_name == \"unet_small\":\n",
        "        return dict(\n",
        "            block_out_channels=(16, 32, 64, 128), # number of channels for each block\n",
        "            norm_num_groups=8, # number of groups for the normalization layer\n",
        "            in_channels=4, # number of input channels\n",
        "            out_channels=1, # number of output channels\n",
        "            input_size=(64, 64),\n",
        "            hidden_size=3,\n",
        "            num_images=3,\n",
        "            )\n",
        "    elif model_name == \"unet_big\":\n",
        "        return dict(\n",
        "            block_out_channels=(32, 64, 128, 256), # number of channels for each block\n",
        "            norm_num_groups=8, # number of groups for the normalization layer\n",
        "            in_channels=num_frames, # number of input channels\n",
        "            out_channels=1, # number of output channels\n",
        "            )\n",
        "    else:\n",
        "        raise(f\"Model name not found: {model_name}, choose between 'unet_small' or 'unet_big'\")\n",
        "\n",
        "class UNet2DTemporalCondition(UNet2DModel, WandbModel):\n",
        "    def __init__(self, \n",
        "                 *x, \n",
        "                 input_size: Tuple[int, int, int], \n",
        "                 hidden_size: Tuple[int, int], \n",
        "                 num_images: int, \n",
        "                 device: str = \"cuda\",\n",
        "                 **kwargs):\n",
        "        super().__init__(*x, **kwargs)\n",
        "        init_unet(self)\n",
        "        self.temporal_encoder = ConvGRU(input_size=(input_size[1, 2]),\n",
        "                                        input_dim=input_size[0],\n",
        "                                        hidden_dim=hidden_size,\n",
        "                                        kernel_size=(3, 3),\n",
        "                                        num_layers=2,\n",
        "                                        dtype=torch.cuda.FloatTensor,\n",
        "                                        batch_first=True,\n",
        "                                        bias = True,\n",
        "                                        return_all_layers = False).to(device)\n",
        "\n",
        "    def forward(self, *x, **kwargs):\n",
        "        temporal_input = x[0] # first three images\n",
        "        _, encoder_hidden_states = self.temporal_encoder(temporal_input.to(self.device))\n",
        "        \n",
        "        conv_lstm_features = encoder_hidden_states[0][0].to(self.device)\n",
        "        noisy_frame = x[1]\n",
        "        noise_hidden_state = torch.cat([conv_lstm_features, noisy_frame], dim=1)\n",
        "\n",
        "        return super().forward(noise_hidden_state, timestep=x[2], **kwargs).sample ## Diffusers's UNet2DConditionModel class\n",
        "\n",
        "## Simple Diffusion paper\n",
        "\n",
        "def get_uvit_params(model_name=\"uvit_small\", num_frames=4):\n",
        "    \"Return the parameters for the diffusers UViT model\"\n",
        "    if model_name == \"uvit_small\":\n",
        "        return dict(\n",
        "            dim=512,\n",
        "            ff_mult=2,\n",
        "            vit_depth=4,\n",
        "            channels=4, \n",
        "            patch_size=4,\n",
        "            final_img_itransform=nn.Conv2d(num_frames,1,1)\n",
        "            )\n",
        "    elif model_name == \"uvit_big\":\n",
        "        return dict(\n",
        "            dim=1024,\n",
        "            ff_mult=4,\n",
        "            vit_depth=8,\n",
        "            channels=4, \n",
        "            patch_size=4,\n",
        "            final_img_itransform=nn.Conv2d(num_frames,1,1)\n",
        "            )\n",
        "    else:\n",
        "        raise(f\"Model name not found: {model_name}, choose between 'uvit_small' or 'uvit_big'\")\n",
        "\n",
        "class UViTModel(UViT, WandbModel): pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "rJzdvyzMx-MI"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "import wandb\n",
        "import numpy as np\n",
        "\n",
        "## For Training\n",
        "\n",
        "def to_wandb_image(img):\n",
        "    \"Convert a tensor to a wandb.Image\"\n",
        "    return wandb.Image(torch.cat(img.split(1), dim=-1).cpu().numpy())\n",
        "\n",
        "def log_images(xt, samples):\n",
        "    \"Log sampled images to wandb\"\n",
        "    device = samples.device\n",
        "    frames = torch.cat([xt[:, :-1,...].to(device), samples], dim=1)\n",
        "    wandb.log({\"sampled_images\": [to_wandb_image(img) for img in frames]})\n",
        "\n",
        "def save_model(model, model_name):\n",
        "    \"Save the model to wandb\"\n",
        "    model_name = f\"{wandb.run.id}_{model_name}\"\n",
        "    models_folder = Path(\"models\")\n",
        "    if not models_folder.exists():\n",
        "        models_folder.mkdir()\n",
        "    torch.save(model.state_dict(), models_folder/f\"{model_name}.pth\")\n",
        "    at = wandb.Artifact(model_name, type=\"model\")\n",
        "    at.add_file(f\"models/{model_name}.pth\")\n",
        "    wandb.log_artifact(at)\n",
        "\n",
        "\n",
        "## For Inference\n",
        "def htile(img):\n",
        "    \"Horizontally tile a batch of images.\"\n",
        "    return torch.cat(img.split(1), dim=-1)\n",
        "\n",
        "def vtile(img):\n",
        "    \"Vertically tile a batch of images.\"\n",
        "    return torch.cat(img.split(1), dim=-2)\n",
        "\n",
        "def vhtile(*imgs):\n",
        "    \"Vertically and horizontally tile a batch of images.\"\n",
        "    return vtile(torch.cat([htile(img) for img in imgs], dim=0))\n",
        "\n",
        "def scale(arr):\n",
        "    \"Scales values of array in [0,1]\"\n",
        "    m, M = arr.min(), arr.max()\n",
        "    return (arr - m) / (M - m)\n",
        "\n",
        "def preprocess_frames(data):\n",
        "    \"Preprocess frames for wandb.Video\"\n",
        "    sdata = scale(data.squeeze())\n",
        "    # print(sdata.shape)\n",
        "    def tfm(frame):\n",
        "        rframe = 255 * frame\n",
        "        return rframe.cpu().numpy().astype(np.uint8)\n",
        "    return [tfm(frame) for frame in sdata]\n",
        "\n",
        "def to_video(data):\n",
        "    \"create wandb.Video container\"\n",
        "    frames = preprocess_frames(data)\n",
        "    vid = np.stack(frames)[:, None, ...]\n",
        "    return wandb.Video(vid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "z1l_534VxxaQ",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import random, argparse\n",
        "from pathlib import Path\n",
        "\n",
        "import wandb\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.optim import AdamW\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim.lr_scheduler import OneCycleLR\n",
        "from torch.utils.data.dataloader import default_collate\n",
        "from torchmetrics import PeakSignalNoiseRatio, StructuralSimilarityIndexMeasure\n",
        "\n",
        "from fastprogress import progress_bar\n",
        "\n",
        "#from cloud_diffusion.wandb import log_images, save_model\n",
        "\n",
        "def noisify_last_frame(frames, noise_func):\n",
        "    \"Noisify the last frame of a sequence\"\n",
        "    past_frames = frames[:,:-1]\n",
        "    last_frame  = frames[:,-1:,0]\n",
        "    noise, t, e = noise_func(last_frame)\n",
        "    # noise = torch.cat([noise] * 3, dim=2)\n",
        "    return past_frames, noise, t, e\n",
        "\n",
        "def noisify_collate(noise_func): \n",
        "    def _inner(b): \n",
        "        \"Collate function that noisifies the last frame\"\n",
        "        return noisify_last_frame(default_collate(b), noise_func)\n",
        "    return _inner\n",
        "\n",
        "class NoisifyDataloader(DataLoader):\n",
        "    \"\"\"Noisify the last frame of a dataloader by applying \n",
        "    a noise function, after collating the batch\"\"\"\n",
        "    def __init__(self, dataset, *args, noise_func=None, **kwargs):\n",
        "        super().__init__(dataset, *args, collate_fn=noisify_collate(noise_func), **kwargs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "XfTZnkBpyM-O"
      },
      "outputs": [],
      "source": [
        "from functools import partial\n",
        "\n",
        "import torch\n",
        "from fastprogress import progress_bar\n",
        "\n",
        "from diffusers.schedulers import DDIMScheduler\n",
        "\n",
        "\n",
        "## DDPM params\n",
        "## From fastai V2 Course DDPM notebooks\n",
        "betamin,betamax,n_steps = 0.0001,0.02, 1000\n",
        "beta = torch.linspace(betamin, betamax, n_steps)\n",
        "alpha = 1.-beta\n",
        "alphabar = alpha.cumprod(dim=0)\n",
        "sigma = beta.sqrt()\n",
        "\n",
        "def noisify_ddpm(x0):\n",
        "    \"Noise by ddpm\"\n",
        "    device = x0.device\n",
        "    n = len(x0)\n",
        "    t = torch.randint(0, n_steps, (n,), dtype=torch.long)\n",
        "    ε = torch.randn(x0.shape, device=device)\n",
        "    ᾱ_t = alphabar[t].reshape(-1, 1, 1, 1).to(device)\n",
        "    xt = ᾱ_t.sqrt()*x0 + (1-ᾱ_t).sqrt()*ε\n",
        "    return xt, t.to(device), ε\n",
        "\n",
        "@torch.no_grad()\n",
        "def diffusers_sampler(model, past_frames, sched, **kwargs):\n",
        "    \"Using Diffusers built-in samplers\"\n",
        "    model.eval()\n",
        "    device = next(model.parameters()).device\n",
        "    new_frame = torch.randn_like(past_frames[:,-1:], dtype=past_frames.dtype, device=device)\n",
        "    preds = []\n",
        "    pbar = progress_bar(sched.timesteps, leave=False)\n",
        "    for t in pbar:\n",
        "        pbar.comment = f\"DDIM Sampler: frame {t}\"\n",
        "        noise = model(past_frames, new_frame, t)\n",
        "        new_frame = sched.step(noise, t, new_frame, **kwargs).prev_sample\n",
        "        preds.append(new_frame.float().cpu())\n",
        "    return preds[-1]\n",
        "\n",
        "def ddim_sampler(steps=350, eta=1.):\n",
        "    \"DDIM sampler, faster and a bit better than the built-in sampler\"\n",
        "    ddim_sched = DDIMScheduler()\n",
        "    ddim_sched.set_timesteps(steps)\n",
        "    return partial(diffusers_sampler, sched=ddim_sched, eta=eta)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MiniTrainer:\n",
        "    \"A mini trainer for the diffusion process\"\n",
        "    def __init__(self, \n",
        "                 train_dataloader, \n",
        "                 valid_dataloader, \n",
        "                 model, \n",
        "                 sampler, \n",
        "                 device=\"cuda\", \n",
        "                 loss_func=nn.MSELoss(), \n",
        "                 ):\n",
        "        self.train_dataloader = train_dataloader\n",
        "        self.valid_dataloader = valid_dataloader\n",
        "        self.loss_func = loss_func\n",
        "        self.psnr = PeakSignalNoiseRatio().to(device)\n",
        "        self.ssim = StructuralSimilarityIndexMeasure().to(device)\n",
        "        self.model = model.to(device)\n",
        "        self.scaler = torch.cuda.amp.GradScaler()\n",
        "        self.device = device\n",
        "        self.sampler = sampler\n",
        "        self.val_batch = next(iter(valid_dataloader))[0].to(device)  # grab a fixed batch to log predictions\n",
        "    \n",
        "    def train_step(self, loss):\n",
        "        \"Train for one step\"\n",
        "        self.optimizer.zero_grad()\n",
        "        self.scaler.scale(loss).backward()\n",
        "        self.scaler.step(self.optimizer)\n",
        "        self.scaler.update()\n",
        "        self.scheduler.step()\n",
        "\n",
        "    def one_epoch(self, epoch=None):\n",
        "        \"Train for one epoch, log metrics and save model\"\n",
        "        self.model.train()\n",
        "        pbar = progress_bar(self.train_dataloader, leave=False)\n",
        "        for batch in pbar:\n",
        "            past_frames, noise_frame, t, noise = to_device(batch, device=self.device)\n",
        "            with torch.autocast(\"cuda\"):\n",
        "                predicted_noise = self.model(past_frames, noise_frame, t)\n",
        "                loss = self.loss_func(noise[:,:], predicted_noise)\n",
        "            self.train_step(loss)\n",
        "            wandb.log({\"train_mse\": loss.item(),\n",
        "                       \"learning_rate\": self.scheduler.get_last_lr()[0]})\n",
        "            pbar.comment = f\"epoch={epoch}, MSE={loss.item():2.3f}\"\n",
        "\n",
        "    def one_epoch_validation(self, epoch=None):\n",
        "        \"Validates on val set\"\n",
        "        pbar = progress_bar(self.valid_dataloader, leave=False)\n",
        "        psnr_metric = 0\n",
        "        mse_metric = 0\n",
        "        ssim_metric = 0\n",
        "        for val_batch in pbar:\n",
        "            frames = val_batch[0].to(self.device)\n",
        "            samples = self.sampler(self.model, past_frames=frames[:,:-1]).to(self.device)\n",
        "            target = frames[:,-1].unsqueeze(1)\n",
        "            psnr_metric += self.psnr(samples, target).float().cpu()\n",
        "            ssim_metric += self.ssim(samples, target).float().cpu()\n",
        "            mse_metric += self.loss_func(samples, target).float().cpu()\n",
        "        psnr_metric = psnr_metric / len(self.valid_dataloader)\n",
        "        ssim_metric = ssim_metric / len(self.valid_dataloader)\n",
        "        mse_metric = mse_metric / len(self.valid_dataloader)\n",
        "        wandb.log({\"val_psnr\": psnr_metric,\n",
        "                   \"val_ssim\": ssim_metric,\n",
        "                   \"val_mse\": mse_metric})\n",
        "\n",
        "    def prepare(self, config):\n",
        "        wandb.config.update(config)\n",
        "        config.total_train_steps = config.epochs * len(self.train_dataloader)\n",
        "        self.optimizer = AdamW(self.model.parameters(), lr=config.lr, eps=1e-5)\n",
        "        self.scheduler = OneCycleLR(self.optimizer, max_lr=config.lr, total_steps=config.total_train_steps)\n",
        "\n",
        "    def fit(self, config):\n",
        "        self.prepare(config)\n",
        "        self.val_batch = self.val_batch[:min(config.n_preds, 8)]  # log first 8 predictions\n",
        "        for epoch in progress_bar(range(config.epochs), total=config.epochs, leave=True):\n",
        "            self.one_epoch(epoch)\n",
        "            if config.validate_epochs:\n",
        "                self.one_epoch_validation(epoch)\n",
        "            \n",
        "            # log predictions\n",
        "            if epoch % config.log_every_epoch == 0:\n",
        "                pass\n",
        "                # samples = self.sampler(self.model, past_frames=self.val_batch[:,:-1])\n",
        "                # print('samples:', samples.shape)\n",
        "                # self.one_epoch_validation(epoch)\n",
        "                # log_images(self.val_batch, samples)\n",
        "\n",
        "        save_model(self.model, config.model_name)\n",
        "\n",
        "\n",
        "def set_seed(s, reproducible=False):\n",
        "    \"Set random seed for `random`, `torch`, and `numpy` (where available)\"\n",
        "    try: torch.manual_seed(s)\n",
        "    except NameError: pass\n",
        "    try: torch.cuda.manual_seed_all(s)\n",
        "    except NameError: pass\n",
        "    try: np.random.seed(s%(2**32-1))\n",
        "    except NameError: pass\n",
        "    random.seed(s)\n",
        "    if reproducible:\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "\n",
        "def to_device(t, device=\"cpu\"):\n",
        "    if isinstance(t, (tuple, list)):\n",
        "        return [_t.to(device) for _t in t]\n",
        "    elif isinstance(t, torch.Tensor):\n",
        "        return t.to(device)\n",
        "    else:\n",
        "        raise(\"Not a Tensor or list of Tensors\")\n",
        "\n",
        "\n",
        "def ls(path: Path): \n",
        "    \"Return files on Path, sorted\"\n",
        "    return sorted(list(path.iterdir()))\n",
        "\n",
        "\n",
        "def parse_args(config):\n",
        "    \"A brute force way to parse arguments, it is probably not a good idea to use it\"\n",
        "    parser = argparse.ArgumentParser(description='Run training baseline')\n",
        "    for k,v in config.__dict__.items():\n",
        "        parser.add_argument('--'+k, type=type(v), default=v)\n",
        "    args = vars(parser.parse_args())\n",
        "    \n",
        "    # update config with parsed args\n",
        "    for k, v in args.items():\n",
        "        setattr(config, k, v)"
      ],
      "metadata": {
        "id": "Q4M7BOOS0HAQ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "-uNhPHz3wpqD"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "from types import SimpleNamespace\n",
        "\n",
        "import wandb\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# from cloud_diffusion.dataset import download_dataset, CloudDataset\n",
        "# from cloud_diffusion.utils import NoisifyDataloader, MiniTrainer, set_seed, parse_args\n",
        "# from cloud_diffusion.ddpm import noisify_ddpm, ddim_sampler\n",
        "# from cloud_diffusion.models import UNet2D, get_unet_params\n",
        "\n",
        "\n",
        "PROJECT_NAME = \"cloud_diffusion\"\n",
        "DATASET_ARTIFACT = 'ai-industry/cloud_diffusion/SEVIR:v0'\n",
        "\n",
        "config = SimpleNamespace(    \n",
        "    epochs=50, # number of epochs\n",
        "    model_name=\"unet_small\", # model name to save [unet_small, unet_big]\n",
        "    strategy=\"ddpm\", # strategy to use ddpm\n",
        "    noise_steps=1000, # number of noise steps on the diffusion process\n",
        "    sampler_steps=333, # number of sampler steps on the diffusion process\n",
        "    seed=42, # random seed\n",
        "    batch_size=128, # batch size\n",
        "    img_size=64, # image size\n",
        "    device=\"cuda\", # device\n",
        "    num_workers=8, # number of workers for dataloader\n",
        "    num_frames=4, # number of frames to use as input\n",
        "    lr=5e-4, # learning rate\n",
        "    validation_days=3, # number of days to use for validation\n",
        "    log_every_epoch=5, # log every n epochs to wandb\n",
        "    n_preds=8, # number of predictions to make\n",
        "    validate_epochs=False # compute metrics at each epoch \n",
        "    )\n",
        "\n",
        "def train_func(config, train_ds, val_ds):\n",
        "    config.model_params = get_unet_params(config.model_name, config.num_frames)\n",
        "\n",
        "    set_seed(config.seed)\n",
        "    device = torch.device(config.device)\n",
        "\n",
        "    # downlaod the dataset from the wandb.Artifact\n",
        "\n",
        "    # valid_ds = CloudDataset(files=files, num_frames=config.num_frames, img_size=config.img_size).shuffle()\n",
        "\n",
        "    # DDPM dataloaders\n",
        "    train_dataloader = NoisifyDataloader(train_ds, config.batch_size, shuffle=True, \n",
        "                                         noise_func=noisify_ddpm,  num_workers=config.num_workers)\n",
        "    valid_dataloader = NoisifyDataloader(val_ds, config.batch_size, shuffle=False, \n",
        "                                          noise_func=noisify_ddpm,  num_workers=config.num_workers)\n",
        "\n",
        "    # model setup\n",
        "    model = UNet2DTemporalCondition(**config.model_params)\n",
        "\n",
        "    # sampler\n",
        "    sampler = ddim_sampler(steps=config.sampler_steps)\n",
        "\n",
        "    # A simple training loop\n",
        "    trainer = MiniTrainer(train_dataloader, valid_dataloader, model, sampler, device)\n",
        "    trainer.fit(config)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "files = download_dataset(DATASET_ARTIFACT, PROJECT_NAME)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312,
          "referenced_widgets": [
            "672d992013104a97835cf6ceb52e055e",
            "207f88ab391c41bb89dd07cc4bf2a78a",
            "8204b6c014a24642b77ca3662bd7ce0f",
            "fcedea3ee3b6412cb3341c140daed333",
            "4b78394b4598484fa77eb2f9b0a5c917",
            "b9187e07b0c741da90ca7744edd846e3",
            "b4b0f339207f4147b4076ad7a57294bf",
            "ee020f329a8b49db855cdd79d9c66f9a"
          ]
        },
        "id": "rLDjR4JvO32d",
        "outputId": "70979c6f-b707-46e8-d6fc-29bd4f52a210"
      },
      "execution_count": 13,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmaidacundo\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.3"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230530_105944-jrzlu8me</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/maidacundo/cloud_diffusion/runs/jrzlu8me' target=\"_blank\">morning-glade-35</a></strong> to <a href='https://wandb.ai/maidacundo/cloud_diffusion' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/maidacundo/cloud_diffusion' target=\"_blank\">https://wandb.ai/maidacundo/cloud_diffusion</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/maidacundo/cloud_diffusion/runs/jrzlu8me' target=\"_blank\">https://wandb.ai/maidacundo/cloud_diffusion/runs/jrzlu8me</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact SEVIR:v0, 2756.25MB. 3 files... \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   3 of 3 files downloaded.  \n",
            "Done. 0:0:58.9\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "672d992013104a97835cf6ceb52e055e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">morning-glade-35</strong> at: <a href='https://wandb.ai/maidacundo/cloud_diffusion/runs/jrzlu8me' target=\"_blank\">https://wandb.ai/maidacundo/cloud_diffusion/runs/jrzlu8me</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230530_105944-jrzlu8me/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = CloudDataset(files=files, num_frames=config.num_frames, img_size=config.img_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "u9BqYnJGmDTO",
        "outputId": "be828ff6-b9bb-4c6f-c485-bf312a919c4c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader = NoisifyDataloader(train_ds, config.batch_size, shuffle=True, \n",
        "                                         noise_func=noisify_ddpm,  num_workers=config.num_workers)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R99Jx6FtLJiP",
        "outputId": "45d0afde-f231-4546-a0d9-48ad5ab2b00c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch = next(iter(train_dataloader))"
      ],
      "metadata": {
        "id": "4Zzm3ijeLLR-"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('min', batch[0].min())\n",
        "print('max', batch[0].max())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5f-d3SYjLQYl",
        "outputId": "4d5881ff-082c-448d-daba-deb2b88ec479"
      },
      "execution_count": 17,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "min tensor(-3.8790)\n",
            "max tensor(7.0128)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch[0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gzAnBoEVUpY7",
        "outputId": "7458b600-df1e-4ce7-8503-9d1fdbd605e7"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([128, 3, 3, 64, 64])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "5XPIJlC0xO0T",
        "outputId": "02076608-39d0-441a-c36a-3a6bf33f2651",
        "tags": []
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.15.3"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230530_110442-28thycul</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/ai-industry/cloud_diffusion/runs/28thycul' target=\"_blank\">silvery-disco-105</a></strong> to <a href='https://wandb.ai/ai-industry/cloud_diffusion' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/ai-industry/cloud_diffusion' target=\"_blank\">https://wandb.ai/ai-industry/cloud_diffusion</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/ai-industry/cloud_diffusion/runs/28thycul' target=\"_blank\">https://wandb.ai/ai-industry/cloud_diffusion/runs/28thycul</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      <progress value='18' class='' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      36.00% [18/50 02:44&lt;04:51]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([96, 4, 64, 64])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([96, 4, 64, 64])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([96, 4, 64, 64])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([96, 4, 64, 64])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([96, 4, 64, 64])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([96, 4, 64, 64])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([96, 4, 64, 64])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([96, 4, 64, 64])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([96, 4, 64, 64])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([96, 4, 64, 64])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([96, 4, 64, 64])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([96, 4, 64, 64])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([96, 4, 64, 64])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([96, 4, 64, 64])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([96, 4, 64, 64])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([96, 4, 64, 64])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([96, 4, 64, 64])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([96, 4, 64, 64])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      <progress value='6' class='' max='19' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      31.58% [6/19 00:03&lt;00:07 epoch=18, MSE=0.035]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n",
            "torch.Size([128, 4, 64, 64])\n"
          ]
        }
      ],
      "source": [
        "with wandb.init(project=PROJECT_NAME, entity='ai-industry', config=config, tags=[\"ddpm\", config.model_name]):\n",
        "    train_func(config, train_ds, train_ds)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "672d992013104a97835cf6ceb52e055e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_207f88ab391c41bb89dd07cc4bf2a78a",
              "IPY_MODEL_8204b6c014a24642b77ca3662bd7ce0f"
            ],
            "layout": "IPY_MODEL_fcedea3ee3b6412cb3341c140daed333"
          }
        },
        "207f88ab391c41bb89dd07cc4bf2a78a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4b78394b4598484fa77eb2f9b0a5c917",
            "placeholder": "​",
            "style": "IPY_MODEL_b9187e07b0c741da90ca7744edd846e3",
            "value": ""
          }
        },
        "8204b6c014a24642b77ca3662bd7ce0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b4b0f339207f4147b4076ad7a57294bf",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ee020f329a8b49db855cdd79d9c66f9a",
            "value": 0
          }
        },
        "fcedea3ee3b6412cb3341c140daed333": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4b78394b4598484fa77eb2f9b0a5c917": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b9187e07b0c741da90ca7744edd846e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b4b0f339207f4147b4076ad7a57294bf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ee020f329a8b49db855cdd79d9c66f9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}