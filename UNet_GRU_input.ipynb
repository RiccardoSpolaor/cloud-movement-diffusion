{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "592KgIBCtbxt"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "xn_pBvNhtbxv"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "def ls(path: Path): \n",
        "    \"Return files on Path, sorted\"\n",
        "    return sorted(list(path.iterdir()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "FsmkDobltqUd"
      },
      "outputs": [],
      "source": [
        "!pip install -q wandb tqdm matplotlib fastprogress torchmetrics diffusers denoising_diffusion_pytorch fastcore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ZRHQ37tJ05B",
        "outputId": "b57b3e5d-27f0-40e3-94c7-5f84d01a6f91"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "import wandb\n",
        "wandb.login(key='6b22cbf359c5924f4500afc1ae572d6827998186', relogin=True, force=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Qzo4ppMBtbxx"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision.transforms as T\n",
        "import wandb\n",
        "from tqdm import tqdm as progress_bar\n",
        "import cv2\n",
        "\n",
        "# from cloud_diffusion.utils import ls\n",
        "\n",
        "PROJECT_NAME = \"cloud_diffusion\"\n",
        "DATASET_ARTIFACT = 'ai-industry/cloud_diffusion/SEVIR:v0'\n",
        "\n",
        "class DummyNextFrameDataset:\n",
        "    \"Dataset that returns random images\"\n",
        "    def __init__(self, num_frames=4, img_size=64, N=1000):\n",
        "        self.img_size = img_size\n",
        "        self.num_frames = num_frames\n",
        "        self.N = N\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.randn(self.num_frames, self.img_size, self.img_size)\n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.N\n",
        "\n",
        "\n",
        "class CloudDataset:\n",
        "    \"\"\"Dataset for cloud images\n",
        "    It loads numpy files from wandb artifact and stacks them into a single array\n",
        "    It also applies some transformations to the images\n",
        "    \"\"\"\n",
        "    def __init__(self, \n",
        "                 files, # list of numpy files to load (they come from the artifact)\n",
        "                 num_frames=4, # how many consecutive frames to stack\n",
        "                 scale=True, # if we images to interval [-0.5, 0.5]\n",
        "                 img_size=64, # resize dim, original images are big (446, 780)\n",
        "                 valid=False, # if True, transforms are deterministic\n",
        "                ):\n",
        "\n",
        "        tfms = [T.RandomCrop(img_size)] if not valid else [T.CenterCrop(img_size)]\n",
        "        self.tfms = T.Compose(tfms)\n",
        "        self.load_data(files, num_frames, scale)\n",
        "        \n",
        "    def _scale(self, arr):\n",
        "        \"Scales values of array in [0,1]\"\n",
        "        m, M = arr.min(), arr.max()\n",
        "        return (arr - m) / (M - m)\n",
        "\n",
        "    def _resize(self, arr, img_size):\n",
        "        num_events, num_frames = arr.shape[0], arr.shape[3]\n",
        "        resized_array = np.empty((num_events, img_size, img_size, num_frames))\n",
        "        for event in range(num_events):\n",
        "            resized_array[event] = cv2.resize(arr[event], (img_size, img_size))\n",
        "        return resized_array.transpose((0, 3, 1, 2))\n",
        "    \n",
        "    def load_channel(self, file, scale=True):\n",
        "        one_channel = np.load(file)\n",
        "        one_channel = one_channel.astype('float32')\n",
        "        if scale:\n",
        "            one_day = 0.5 - self._scale(one_channel)\n",
        "        return one_channel\n",
        "\n",
        "    def create_windows(self, data, num_frames):\n",
        "        windows = []\n",
        "        for event in data:\n",
        "            wds = np.lib.stride_tricks.sliding_window_view(\n",
        "                        event,\n",
        "                        num_frames,\n",
        "                        axis=0)[::num_frames].transpose(0,4,1,2,3) # (windows, frames, channels, height, width)\n",
        "            windows.append(wds)\n",
        "        windows = np.array(windows)\n",
        "        shape = windows.shape\n",
        "        windows = windows.reshape(shape[0] * shape[1], shape[2], shape[3], shape[4], shape[5])\n",
        "        return windows # (batch, channels, frames, height, width)\n",
        "\n",
        "    def load_data(self, files, num_frames, scale, img_size=64):\n",
        "        \"Loads all data into a single array self.data\"\n",
        "        channels = []\n",
        "        # Load all channels\n",
        "        for file in progress_bar(files, leave=False):\n",
        "            one_channel = self.load_channel(file, scale)\n",
        "            resized_array = self._resize(one_channel, img_size)\n",
        "            channels.append(resized_array)\n",
        "            del one_channel\n",
        "        all_channels = np.stack(channels, axis=2)\n",
        "        self.data = self.create_windows(all_channels, num_frames)\n",
        "\n",
        "    def shuffle(self):\n",
        "        \"\"\"Shuffles the dataset, useful for getting \n",
        "        interesting samples on the validation dataset\"\"\"\n",
        "        idxs = torch.randperm(len(self.data))\n",
        "        self.data = self.data[idxs]\n",
        "        return self\n",
        "\n",
        "    @staticmethod\n",
        "    def _scale(arr):\n",
        "        \"Scales values of array in [0,1]\"\n",
        "        m, M = arr.min(), arr.max()\n",
        "        return (arr - m) / (M - m)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return self.tfms(torch.from_numpy(self.data[idx]))\n",
        "    \n",
        "    def __len__(self): return len(self.data)\n",
        "\n",
        "    def save(self, fname=\"cloud_frames.npy\"):\n",
        "        np.save(fname, self.data)\n",
        "\n",
        "\n",
        "class CloudDatasetInference(CloudDataset):\n",
        "     def load_data(self, files, num_frames=None, scale=None):\n",
        "        \"Loads all data into a single array self.data\"\n",
        "        data = []\n",
        "        max_length = 100\n",
        "        # TODO: download everything\n",
        "        for file in files:\n",
        "            one_day = self.load_day(file, scale)\n",
        "            data.append(one_day)\n",
        "            max_length = min(max_length, len(one_day))\n",
        "        self.data = np.stack([d[:max_length] for d in data], axis=0).squeeze()\n",
        "\n",
        "\n",
        "def download_dataset(at_name, project_name):\n",
        "    \"Downloads dataset from wandb artifact\"\n",
        "    def _get_dataset(run):\n",
        "        artifact = run.use_artifact(at_name, type='dataset')\n",
        "        return artifact.download()\n",
        "\n",
        "    if wandb.run is not None:\n",
        "        run = wandb.run\n",
        "        artifact_dir = _get_dataset(run)\n",
        "    else:\n",
        "        run = wandb.init(project=project_name, job_type=\"download_dataset\")\n",
        "        artifact_dir = _get_dataset(run)\n",
        "        run.finish()\n",
        "\n",
        "    files = ls(Path(artifact_dir))\n",
        "    return files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oS2y3URjtbx1"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "xi_MUZcOXMcn"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.autograd import Variable\n",
        "\n",
        "\n",
        "class ConvGRUCell(nn.Module):\n",
        "    def __init__(self, input_size, input_dim, hidden_dim, kernel_size, bias, dtype):\n",
        "        \"\"\"\n",
        "        Initialize the ConvLSTM cell\n",
        "        :param input_size: (int, int)\n",
        "            Height and width of input tensor as (height, width).\n",
        "        :param input_dim: int\n",
        "            Number of channels of input tensor.\n",
        "        :param hidden_dim: int\n",
        "            Number of channels of hidden state.\n",
        "        :param kernel_size: (int, int)\n",
        "            Size of the convolutional kernel.\n",
        "        :param bias: bool\n",
        "            Whether or not to add the bias.\n",
        "        :param dtype: torch.cuda.FloatTensor or torch.FloatTensor\n",
        "            Whether or not to use cuda.\n",
        "        \"\"\"\n",
        "        super(ConvGRUCell, self).__init__()\n",
        "        self.height, self.width = input_size\n",
        "        self.padding = kernel_size[0] // 2, kernel_size[1] // 2\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.bias = bias\n",
        "        self.dtype = dtype\n",
        "\n",
        "        self.conv_gates = nn.Conv2d(in_channels=input_dim + hidden_dim,\n",
        "                                    out_channels=2*self.hidden_dim,  # for update_gate,reset_gate respectively\n",
        "                                    kernel_size=kernel_size,\n",
        "                                    padding=self.padding,\n",
        "                                    bias=self.bias)\n",
        "\n",
        "        self.conv_can = nn.Conv2d(in_channels=input_dim+hidden_dim,\n",
        "                              out_channels=self.hidden_dim, # for candidate neural memory\n",
        "                              kernel_size=kernel_size,\n",
        "                              padding=self.padding,\n",
        "                              bias=self.bias)\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        return (Variable(torch.zeros(batch_size, self.hidden_dim, self.height, self.width)).type(self.dtype))\n",
        "\n",
        "    def forward(self, input_tensor, h_cur):\n",
        "        \"\"\"\n",
        "\n",
        "        :param self:\n",
        "        :param input_tensor: (b, c, h, w)\n",
        "            input is actually the target_model\n",
        "        :param h_cur: (b, c_hidden, h, w)\n",
        "            current hidden and cell states respectively\n",
        "        :return: h_next,\n",
        "            next hidden state\n",
        "        \"\"\"\n",
        "        combined = torch.cat([input_tensor, h_cur], dim=1)\n",
        "        combined_conv = self.conv_gates(combined)\n",
        "\n",
        "        gamma, beta = torch.split(combined_conv, self.hidden_dim, dim=1)\n",
        "        reset_gate = torch.sigmoid(gamma)\n",
        "        update_gate = torch.sigmoid(beta)\n",
        "\n",
        "        \n",
        "        combined = torch.cat([input_tensor, reset_gate*h_cur], dim=1)\n",
        "        cc_cnm = self.conv_can(combined)\n",
        "        cnm = torch.tanh(cc_cnm)\n",
        "\n",
        "        h_next = (1 - update_gate) * h_cur + update_gate * cnm\n",
        "        return h_next\n",
        "\n",
        "\n",
        "class ConvGRU(nn.Module):\n",
        "    def __init__(self, input_size, input_dim, hidden_dim, kernel_size, num_layers,\n",
        "                 dtype, batch_first=False, bias=True, return_all_layers=False):\n",
        "        \"\"\"\n",
        "\n",
        "        :param input_size: (int, int)\n",
        "            Height and width of input tensor as (height, width).\n",
        "        :param input_dim: int e.g. 256\n",
        "            Number of channels of input tensor.\n",
        "        :param hidden_dim: int e.g. 1024\n",
        "            Number of channels of hidden state.\n",
        "        :param kernel_size: (int, int)\n",
        "            Size of the convolutional kernel.\n",
        "        :param num_layers: int\n",
        "            Number of ConvLSTM layers\n",
        "        :param dtype: torch.cuda.FloatTensor or torch.FloatTensor\n",
        "            Whether or not to use cuda.\n",
        "        :param alexnet_path: str\n",
        "            pretrained alexnet parameters\n",
        "        :param batch_first: bool\n",
        "            if the first position of array is batch or not\n",
        "        :param bias: bool\n",
        "            Whether or not to add the bias.\n",
        "        :param return_all_layers: bool\n",
        "            if return hidden and cell states for all layers\n",
        "        \"\"\"\n",
        "        super(ConvGRU, self).__init__()\n",
        "\n",
        "        # Make sure that both `kernel_size` and `hidden_dim` are lists having len == num_layers\n",
        "        kernel_size = self._extend_for_multilayer(kernel_size, num_layers)\n",
        "        hidden_dim  = self._extend_for_multilayer(hidden_dim, num_layers)\n",
        "        if not len(kernel_size) == len(hidden_dim) == num_layers:\n",
        "            raise ValueError('Inconsistent list length.')\n",
        "\n",
        "        self.height, self.width = input_size\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.kernel_size = kernel_size\n",
        "        self.dtype = dtype\n",
        "        self.num_layers = num_layers\n",
        "        self.batch_first = batch_first\n",
        "        self.bias = bias\n",
        "        self.return_all_layers = return_all_layers\n",
        "\n",
        "        cell_list = []\n",
        "        for i in range(0, self.num_layers):\n",
        "            cur_input_dim = input_dim if i == 0 else hidden_dim[i - 1]\n",
        "            cell_list.append(ConvGRUCell(input_size=(self.height, self.width),\n",
        "                                         input_dim=cur_input_dim,\n",
        "                                         hidden_dim=self.hidden_dim[i],\n",
        "                                         kernel_size=self.kernel_size[i],\n",
        "                                         bias=self.bias,\n",
        "                                         dtype=self.dtype))\n",
        "\n",
        "        # convert python list to pytorch module\n",
        "        self.cell_list = nn.ModuleList(cell_list)\n",
        "\n",
        "    def forward(self, input_tensor, hidden_state=None):\n",
        "        \"\"\"\n",
        "\n",
        "        :param input_tensor: (b, t, c, h, w) or (t,b,c,h,w) depends on if batch first or not\n",
        "            extracted features from alexnet\n",
        "        :param hidden_state:\n",
        "        :return: layer_output_list, last_state_list\n",
        "        \"\"\"\n",
        "        if not self.batch_first:\n",
        "            # (t, b, c, h, w) -> (b, t, c, h, w)\n",
        "            input_tensor = input_tensor.permute(1, 0, 2, 3, 4)\n",
        "\n",
        "        # Implement stateful ConvLSTM\n",
        "        if hidden_state is not None:\n",
        "            raise NotImplementedError()\n",
        "        else:\n",
        "            hidden_state = self._init_hidden(batch_size=input_tensor.size(0))\n",
        "\n",
        "        layer_output_list = []\n",
        "        last_state_list   = []\n",
        "\n",
        "        seq_len = input_tensor.size(1)\n",
        "        cur_layer_input = input_tensor\n",
        "\n",
        "        for layer_idx in range(self.num_layers):\n",
        "            h = hidden_state[layer_idx]\n",
        "            output_inner = []\n",
        "            for t in range(seq_len):\n",
        "                # input current hidden and cell state then compute the next hidden and cell state through ConvLSTMCell forward function\n",
        "                h = self.cell_list[layer_idx](input_tensor=cur_layer_input[:, t, :, :, :], # (b,t,c,h,w)\n",
        "                                              h_cur=h)\n",
        "                output_inner.append(h)\n",
        "\n",
        "            layer_output = torch.stack(output_inner, dim=1)\n",
        "            cur_layer_input = layer_output\n",
        "\n",
        "            layer_output_list.append(layer_output)\n",
        "            last_state_list.append([h])\n",
        "\n",
        "        if not self.return_all_layers:\n",
        "            layer_output_list = layer_output_list[-1:]\n",
        "            last_state_list   = last_state_list[-1:]\n",
        "\n",
        "        return layer_output_list, last_state_list\n",
        "\n",
        "    def _init_hidden(self, batch_size):\n",
        "        init_states = []\n",
        "        for i in range(self.num_layers):\n",
        "            init_states.append(self.cell_list[i].init_hidden(batch_size))\n",
        "        return init_states\n",
        "\n",
        "    @staticmethod\n",
        "    def _check_kernel_size_consistency(kernel_size):\n",
        "        if not (isinstance(kernel_size, tuple) or\n",
        "                    (isinstance(kernel_size, list) and all([isinstance(elem, tuple) for elem in kernel_size]))):\n",
        "            raise ValueError('`kernel_size` must be tuple or list of tuples')\n",
        "\n",
        "    @staticmethod\n",
        "    def _extend_for_multilayer(param, num_layers):\n",
        "        if not isinstance(param, list):\n",
        "            param = [param] * num_layers\n",
        "        return param"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "XotWTQaPJ05G"
      },
      "outputs": [],
      "source": [
        "from typing import Tuple\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "class TemporalEncoder(nn.Module):\n",
        "    def __init__(\n",
        "        self, \n",
        "        input_size: Tuple[int, int], \n",
        "        hidden_size: int,\n",
        "        num_images: int, \n",
        "        device: str\n",
        "        ) -> None:\n",
        "        super().__init__()\n",
        "        # Set the input size of the image.\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        # Set the size of the flattened image.\n",
        "        self.flatten_size = input_size[0] * input_size[1]\n",
        "        # Set a list of GRUs, one for each image.\n",
        "        self.gru = nn.GRU(\n",
        "            self.flatten_size, hidden_size, batch_first=True)\n",
        "        #self.grus = nn.ModuleList(\n",
        "        #    [nn.GRU(self.flatten_size, self.flatten_size)\n",
        "        #     for _ in range(num_images)])\n",
        "        # Set the device used for the computations.\n",
        "        self.to(device)\n",
        "        self.device = device\n",
        "\n",
        "    def to(self, device: str) -> None:\n",
        "        super().to(device)\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, x: torch.FloatTensor) -> torch.FloatTensor:\n",
        "        batch_size = x.shape[0]\n",
        "        n_channels = x.shape[1]\n",
        "        # Set the initial hidden states \n",
        "        initial_hidden_state = torch.zeros(\n",
        "            batch_size, n_channels, self.hidden_size, dtype=torch.float32,\n",
        "            device=self.device)\n",
        "        print(x.shape)\n",
        "        print(initial_hidden_state.shape)\n",
        "\n",
        "        _, out = self.gru(x.flatten(start_dim=2), initial_hidden_state)\n",
        "        # Iterate over the images and pass them through the GRUs.\n",
        "        '''for i, gru in enumerate(self.grus):\n",
        "            # Flatten the image.\n",
        "            img = x[:, i].flatten(start_dim=1)\n",
        "            # If it is the first image, use the initial hidden state.\n",
        "            if i == 0:\n",
        "                h = initial_hidden_state\n",
        "            # Get the forward pass of the GRU.\n",
        "            h, _ = gru(img, h)''';\n",
        "        \n",
        "        \"\"\"# Turn the hidden state to the original shape.\n",
        "        out = out.view(batch_size, n_channels, self.input_size[0],\n",
        "                       self.input_size[1])\"\"\"\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "yQfuFLG5tbx2"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "import wandb\n",
        "import fastcore.all as fc\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from diffusers import UNet2DModel\n",
        "\n",
        "try:\n",
        "    from denoising_diffusion_pytorch.simple_diffusion import UViT\n",
        "except:\n",
        "    raise ImportError(\"Please install denoising_diffusion_pytorch with `pip install denoising_diffusion_pytorch`\")\n",
        "\n",
        "def init_unet(model):\n",
        "    \"From Jeremy's bag of tricks on fastai V2 2023\"\n",
        "    for o in model.down_blocks:\n",
        "        for p in o.resnets:\n",
        "            p.conv2.weight.data.zero_()\n",
        "            for p in fc.L(o.downsamplers): nn.init.orthogonal_(p.conv.weight)\n",
        "\n",
        "    for o in model.up_blocks:\n",
        "        for p in o.resnets: p.conv2.weight.data.zero_()\n",
        "\n",
        "    model.conv_out.weight.data.zero_()\n",
        "\n",
        "class WandbModel:\n",
        "    \"A model that can be saved to wandb\"\n",
        "    @classmethod\n",
        "    def from_checkpoint(cls, model_params, checkpoint_file):\n",
        "        \"Load a UNet2D model from a checkpoint file\"\n",
        "        model = cls(**model_params)\n",
        "        print(f\"Loading model from: {checkpoint_file}\")\n",
        "        model.load_state_dict(torch.load(checkpoint_file))\n",
        "        return model\n",
        "\n",
        "    @classmethod\n",
        "    def from_artifact(cls, model_params, artifact_name):\n",
        "        \"Load a UNet2D model from a wandb.Artifact, need to be run in a wandb run\"\n",
        "        artifact = wandb.use_artifact(artifact_name, type='model')\n",
        "        artifact_dir = Path(artifact.download())\n",
        "        chpt_file = list(artifact_dir.glob(\"*.pth\"))[0]\n",
        "        return cls.from_checkpoint(model_params, chpt_file)\n",
        "\n",
        "def get_unet_params(model_name=\"unet_small\", num_frames=4):\n",
        "    \"Return the parameters for the diffusers UNet2d model\"\n",
        "    if model_name == \"unet_small\":\n",
        "        return dict(\n",
        "            block_out_channels=(16, 32, 64, 128), # number of channels for each block\n",
        "            norm_num_groups=8, # number of groups for the normalization layer\n",
        "            in_channels=16, # number of input channels\n",
        "            out_channels=1, # number of output channels\n",
        "            input_size=(64, 64),\n",
        "            hidden_size=15,\n",
        "            num_images=3,\n",
        "            )\n",
        "    elif model_name == \"unet_big\":\n",
        "        return dict(\n",
        "            block_out_channels=(32, 64, 128, 256), # number of channels for each block\n",
        "            norm_num_groups=8, # number of groups for the normalization layer\n",
        "            in_channels=num_frames, # number of input channels\n",
        "            out_channels=1, # number of output channels\n",
        "            )\n",
        "    else:\n",
        "        raise(f\"Model name not found: {model_name}, choose between 'unet_small' or 'unet_big'\")\n",
        "\n",
        "class UNet2DTemporalCondition(UNet2DModel, WandbModel):\n",
        "    def __init__(self, \n",
        "                 *x, \n",
        "                 input_size: Tuple[int, int], \n",
        "                 hidden_size: Tuple[int, int], \n",
        "                 num_images: int, \n",
        "                 device: str = \"cuda\",\n",
        "                 **kwargs):\n",
        "        super().__init__(*x, **kwargs)\n",
        "        init_unet(self)\n",
        "        self.temporal_encoder = ConvGRU(input_size=input_size,\n",
        "                                        input_dim=1,\n",
        "                                        hidden_dim=hidden_size,\n",
        "                                        kernel_size=(3, 3),\n",
        "                                        num_layers=2,\n",
        "                                        dtype=torch.cuda.FloatTensor,\n",
        "                                        batch_first=True,\n",
        "                                        bias = True,\n",
        "                                        return_all_layers = False).to(device)\n",
        "\n",
        "    def forward(self, *x, **kwargs):\n",
        "        temporal_input = x[0][:,:-1] # first three images\n",
        "        _, encoder_hidden_states = self.temporal_encoder(temporal_input.unsqueeze(2).to(self.device))\n",
        "        noise_hidden_state = torch.cat([encoder_hidden_states[0][0].to(self.device), x[0][:,-1].unsqueeze(1)], dim=1)\n",
        "        return super().forward(noise_hidden_state, timestep=x[1], **kwargs).sample ## Diffusers's UNet2DConditionModel class\n",
        "\n",
        "## Simple Diffusion paper\n",
        "\n",
        "def get_uvit_params(model_name=\"uvit_small\", num_frames=4):\n",
        "    \"Return the parameters for the diffusers UViT model\"\n",
        "    if model_name == \"uvit_small\":\n",
        "        return dict(\n",
        "            dim=512,\n",
        "            ff_mult=2,\n",
        "            vit_depth=4,\n",
        "            channels=4, \n",
        "            patch_size=4,\n",
        "            final_img_itransform=nn.Conv2d(num_frames,1,1)\n",
        "            )\n",
        "    elif model_name == \"uvit_big\":\n",
        "        return dict(\n",
        "            dim=1024,\n",
        "            ff_mult=4,\n",
        "            vit_depth=8,\n",
        "            channels=4, \n",
        "            patch_size=4,\n",
        "            final_img_itransform=nn.Conv2d(num_frames,1,1)\n",
        "            )\n",
        "    else:\n",
        "        raise(f\"Model name not found: {model_name}, choose between 'uvit_small' or 'uvit_big'\")\n",
        "\n",
        "class UViTModel(UViT, WandbModel): pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "rJzdvyzMx-MI"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "import wandb\n",
        "import numpy as np\n",
        "\n",
        "## For Training\n",
        "\n",
        "def to_wandb_image(img):\n",
        "    \"Convert a tensor to a wandb.Image\"\n",
        "    return wandb.Image(torch.cat(img.split(1), dim=-1).cpu().numpy())\n",
        "\n",
        "def log_images(xt, samples):\n",
        "    \"Log sampled images to wandb\"\n",
        "    device = samples.device\n",
        "    frames = torch.cat([xt[:, :-1,...].to(device), samples], dim=1)\n",
        "    wandb.log({\"sampled_images\": [to_wandb_image(img) for img in frames]})\n",
        "\n",
        "def save_model(model, model_name):\n",
        "    \"Save the model to wandb\"\n",
        "    model_name = f\"{wandb.run.id}_{model_name}\"\n",
        "    models_folder = Path(\"models\")\n",
        "    if not models_folder.exists():\n",
        "        models_folder.mkdir()\n",
        "    torch.save(model.state_dict(), models_folder/f\"{model_name}.pth\")\n",
        "    at = wandb.Artifact(model_name, type=\"model\")\n",
        "    at.add_file(f\"models/{model_name}.pth\")\n",
        "    wandb.log_artifact(at)\n",
        "\n",
        "\n",
        "## For Inference\n",
        "def htile(img):\n",
        "    \"Horizontally tile a batch of images.\"\n",
        "    return torch.cat(img.split(1), dim=-1)\n",
        "\n",
        "def vtile(img):\n",
        "    \"Vertically tile a batch of images.\"\n",
        "    return torch.cat(img.split(1), dim=-2)\n",
        "\n",
        "def vhtile(*imgs):\n",
        "    \"Vertically and horizontally tile a batch of images.\"\n",
        "    return vtile(torch.cat([htile(img) for img in imgs], dim=0))\n",
        "\n",
        "def scale(arr):\n",
        "    \"Scales values of array in [0,1]\"\n",
        "    m, M = arr.min(), arr.max()\n",
        "    return (arr - m) / (M - m)\n",
        "\n",
        "def preprocess_frames(data):\n",
        "    \"Preprocess frames for wandb.Video\"\n",
        "    sdata = scale(data.squeeze())\n",
        "    # print(sdata.shape)\n",
        "    def tfm(frame):\n",
        "        rframe = 255 * frame\n",
        "        return rframe.cpu().numpy().astype(np.uint8)\n",
        "    return [tfm(frame) for frame in sdata]\n",
        "\n",
        "def to_video(data):\n",
        "    \"create wandb.Video container\"\n",
        "    frames = preprocess_frames(data)\n",
        "    vid = np.stack(frames)[:, None, ...]\n",
        "    return wandb.Video(vid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "z1l_534VxxaQ",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import random, argparse\n",
        "from pathlib import Path\n",
        "\n",
        "import wandb\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.optim import AdamW\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim.lr_scheduler import OneCycleLR\n",
        "from torch.utils.data.dataloader import default_collate\n",
        "from torchmetrics import PeakSignalNoiseRatio, StructuralSimilarityIndexMeasure\n",
        "\n",
        "from fastprogress import progress_bar\n",
        "\n",
        "#from cloud_diffusion.wandb import log_images, save_model\n",
        "\n",
        "def noisify_last_frame(frames, noise_func):\n",
        "    \"Noisify the last frame of a sequence\"\n",
        "    past_frames = frames[:,:-1]\n",
        "    last_frame  = frames[:,-1:] # TODO the last frame has to have the same dimension as the others, cant have this shape.\n",
        "    noise, t, e = noise_func(last_frame)\n",
        "    print('noise:', noise.shape)\n",
        "    return torch.cat([past_frames, noise], dim=1), t, e\n",
        "\n",
        "def noisify_collate(noise_func): \n",
        "    def _inner(b): \n",
        "        \"Collate function that noisifies the last frame\"\n",
        "        return noisify_last_frame(default_collate(b), noise_func)\n",
        "    return _inner\n",
        "\n",
        "class NoisifyDataloader(DataLoader):\n",
        "    \"\"\"Noisify the last frame of a dataloader by applying \n",
        "    a noise function, after collating the batch\"\"\"\n",
        "    def __init__(self, dataset, *args, noise_func=None, **kwargs):\n",
        "        super().__init__(dataset, *args, collate_fn=noisify_collate(noise_func), **kwargs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "XfTZnkBpyM-O"
      },
      "outputs": [],
      "source": [
        "from functools import partial\n",
        "\n",
        "import torch\n",
        "from fastprogress import progress_bar\n",
        "\n",
        "from diffusers.schedulers import DDIMScheduler\n",
        "\n",
        "\n",
        "## DDPM params\n",
        "## From fastai V2 Course DDPM notebooks\n",
        "betamin,betamax,n_steps = 0.0001,0.02, 1000\n",
        "beta = torch.linspace(betamin, betamax, n_steps)\n",
        "alpha = 1.-beta\n",
        "alphabar = alpha.cumprod(dim=0)\n",
        "sigma = beta.sqrt()\n",
        "\n",
        "def noisify_ddpm(x0):\n",
        "    \"Noise by ddpm\"\n",
        "    device = x0.device\n",
        "    n = len(x0)\n",
        "    t = torch.randint(0, n_steps, (n,), dtype=torch.long)\n",
        "    ε = torch.randn(x0.shape, device=device)\n",
        "    ᾱ_t = alphabar[t].reshape(-1, 1, 1, 1, 1).to(device)\n",
        "    xt = ᾱ_t.sqrt()*x0 + (1-ᾱ_t).sqrt()*ε\n",
        "    return xt, t.to(device), ε\n",
        "\n",
        "@torch.no_grad()\n",
        "def diffusers_sampler(model, past_frames, sched, **kwargs):\n",
        "    \"Using Diffusers built-in samplers\"\n",
        "    model.eval()\n",
        "    device = next(model.parameters()).device\n",
        "    new_frame = torch.randn_like(past_frames[:,-1:], dtype=past_frames.dtype, device=device)\n",
        "    preds = []\n",
        "    pbar = progress_bar(sched.timesteps, leave=False)\n",
        "    for t in pbar:\n",
        "        pbar.comment = f\"DDIM Sampler: frame {t}\"\n",
        "        noise = model(torch.cat([past_frames, new_frame], dim=1), t)\n",
        "        new_frame = sched.step(noise, t, new_frame, **kwargs).prev_sample\n",
        "        preds.append(new_frame.float().cpu())\n",
        "    return preds[-1]\n",
        "\n",
        "def ddim_sampler(steps=350, eta=1.):\n",
        "    \"DDIM sampler, faster and a bit better than the built-in sampler\"\n",
        "    ddim_sched = DDIMScheduler()\n",
        "    ddim_sched.set_timesteps(steps)\n",
        "    return partial(diffusers_sampler, sched=ddim_sched, eta=eta)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PROJECT_NAME = \"cloud_diffusion\"\n",
        "DATASET_ARTIFACT = 'ai-industry/cloud_diffusion/SEVIR:v0'\n",
        "\n",
        "files = download_dataset(DATASET_ARTIFACT, PROJECT_NAME)"
      ],
      "metadata": {
        "id": "3mj5h62TfdEv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = CloudDataset(files=files, num_frames=4, img_size=64)"
      ],
      "metadata": {
        "id": "S4cHssUJfW2R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader = NoisifyDataloader(train_ds, batch_size=32, shuffle=True, \n",
        "                                      noise_func=noisify_ddpm,  num_workers=2)\n",
        "batch = next(iter(train_dataloader))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pq8fmge8fsc8",
        "outputId": "e5320408-5633-4bcd-be40-38647cde1e41"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "noise: torch.Size([32, 1, 3, 64, 64])noise:\n",
            " torch.Size([32, 1, 3, 64, 64])\n",
            "noise:noise: torch.Size([32, 1, 3, 64, 64]) \n",
            "torch.Size([32, 1, 3, 64, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch[0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZGUh_IhXgq42",
        "outputId": "000ca5eb-cfd8-4934-e3e9-9af5c2333a62"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 4, 3, 64, 64])"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.imshow(batch[0][0,2,0])"
      ],
      "metadata": {
        "id": "QGVUWB963QB6",
        "outputId": "72f5852a-f124-4e56-f3e0-45aa77638c05",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        }
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fecc5c85ba0>"
            ]
          },
          "metadata": {},
          "execution_count": 81
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MiniTrainer:\n",
        "    \"A mini trainer for the diffusion process\"\n",
        "    def __init__(self, \n",
        "                 train_dataloader, \n",
        "                 valid_dataloader, \n",
        "                 model, \n",
        "                 sampler, \n",
        "                 device=\"cuda\", \n",
        "                 loss_func=nn.MSELoss(), \n",
        "                 ):\n",
        "        self.train_dataloader = train_dataloader\n",
        "        self.valid_dataloader = valid_dataloader\n",
        "        self.loss_func = loss_func\n",
        "        self.psnr = PeakSignalNoiseRatio().to(device)\n",
        "        self.ssim = StructuralSimilarityIndexMeasure().to(device)\n",
        "        self.model = model.to(device)\n",
        "        self.scaler = torch.cuda.amp.GradScaler()\n",
        "        self.device = device\n",
        "        self.sampler = sampler\n",
        "        self.val_batch = next(iter(valid_dataloader))[0].to(device)  # grab a fixed batch to log predictions\n",
        "    \n",
        "    def train_step(self, loss):\n",
        "        \"Train for one step\"\n",
        "        self.optimizer.zero_grad()\n",
        "        self.scaler.scale(loss).backward()\n",
        "        self.scaler.step(self.optimizer)\n",
        "        self.scaler.update()\n",
        "        self.scheduler.step()\n",
        "\n",
        "    def one_epoch(self, epoch=None):\n",
        "        \"Train for one epoch, log metrics and save model\"\n",
        "        self.model.train()\n",
        "        pbar = progress_bar(self.train_dataloader, leave=False)\n",
        "        for batch in pbar:\n",
        "            frames, t, noise = to_device(batch, device=self.device)\n",
        "            with torch.autocast(\"cuda\"):\n",
        "                predicted_noise = self.model(frames, t)\n",
        "                loss = self.loss_func(noise, predicted_noise)\n",
        "            self.train_step(loss)\n",
        "            wandb.log({\"train_mse\": loss.item(),\n",
        "                       \"learning_rate\": self.scheduler.get_last_lr()[0]})\n",
        "            pbar.comment = f\"epoch={epoch}, MSE={loss.item():2.3f}\"\n",
        "\n",
        "    def one_epoch_validation(self, epoch=None):\n",
        "        \"Validates on val set\"\n",
        "        pbar = progress_bar(self.valid_dataloader, leave=False)\n",
        "        psnr_metric = 0\n",
        "        mse_metric = 0\n",
        "        ssmi_metric = 0\n",
        "        for val_batch in pbar:\n",
        "            frames = val_batch[0].to(self.device)\n",
        "            samples = self.sampler(self.model, past_frames=frames[:,:-1]).to(self.device)\n",
        "            target = frames[:,-1].unsqueeze(1)\n",
        "            psnr_metric += self.psnr(samples, target).float().cpu()\n",
        "            ssmi_metric += self.ssim(samples, target).float().cpu()\n",
        "            mse_metric += self.loss_func(samples, target).float().cpu()\n",
        "        psnr_metric = psnr_metric / len(self.valid_dataloader)\n",
        "        ssmi_metric = ssmi_metric / len(self.valid_dataloader)\n",
        "        mse_metric = mse_metric / len(self.valid_dataloader)\n",
        "        wandb.log({\"val_psnr\": psnr_metric,\n",
        "                   \"val_ssmi\": ssmi_metric,\n",
        "                   \"val_mse\": mse_metric})\n",
        "\n",
        "    def prepare(self, config):\n",
        "        wandb.config.update(config)\n",
        "        config.total_train_steps = config.epochs * len(self.train_dataloader)\n",
        "        self.optimizer = AdamW(self.model.parameters(), lr=config.lr, eps=1e-5)\n",
        "        self.scheduler = OneCycleLR(self.optimizer, max_lr=config.lr, total_steps=config.total_train_steps)\n",
        "\n",
        "    def fit(self, config):\n",
        "        self.prepare(config)\n",
        "        self.val_batch = self.val_batch[:min(config.n_preds, 8)]  # log first 8 predictions\n",
        "        for epoch in progress_bar(range(config.epochs), total=config.epochs, leave=True):\n",
        "            self.one_epoch(epoch)\n",
        "            if config.validate_epochs:\n",
        "                self.one_epoch_validation(epoch)\n",
        "            \n",
        "            # log predictions\n",
        "            if epoch % config.log_every_epoch == 0:\n",
        "                samples = self.sampler(self.model, past_frames=self.val_batch[:,:-1])\n",
        "                self.one_epoch_validation(epoch)\n",
        "                log_images(self.val_batch, samples)\n",
        "\n",
        "        save_model(self.model, config.model_name)\n",
        "\n",
        "\n",
        "def set_seed(s, reproducible=False):\n",
        "    \"Set random seed for `random`, `torch`, and `numpy` (where available)\"\n",
        "    try: torch.manual_seed(s)\n",
        "    except NameError: pass\n",
        "    try: torch.cuda.manual_seed_all(s)\n",
        "    except NameError: pass\n",
        "    try: np.random.seed(s%(2**32-1))\n",
        "    except NameError: pass\n",
        "    random.seed(s)\n",
        "    if reproducible:\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "\n",
        "def to_device(t, device=\"cpu\"):\n",
        "    if isinstance(t, (tuple, list)):\n",
        "        return [_t.to(device) for _t in t]\n",
        "    elif isinstance(t, torch.Tensor):\n",
        "        return t.to(device)\n",
        "    else:\n",
        "        raise(\"Not a Tensor or list of Tensors\")\n",
        "\n",
        "\n",
        "def ls(path: Path): \n",
        "    \"Return files on Path, sorted\"\n",
        "    return sorted(list(path.iterdir()))\n",
        "\n",
        "\n",
        "def parse_args(config):\n",
        "    \"A brute force way to parse arguments, it is probably not a good idea to use it\"\n",
        "    parser = argparse.ArgumentParser(description='Run training baseline')\n",
        "    for k,v in config.__dict__.items():\n",
        "        parser.add_argument('--'+k, type=type(v), default=v)\n",
        "    args = vars(parser.parse_args())\n",
        "    \n",
        "    # update config with parsed args\n",
        "    for k, v in args.items():\n",
        "        setattr(config, k, v)"
      ],
      "metadata": {
        "id": "Q4M7BOOS0HAQ"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "-uNhPHz3wpqD"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "from types import SimpleNamespace\n",
        "\n",
        "import wandb\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# from cloud_diffusion.dataset import download_dataset, CloudDataset\n",
        "#from cloud_diffusion.utils import NoisifyDataloader, MiniTrainer, set_seed, parse_args\n",
        "#from cloud_diffusion.ddpm import noisify_ddpm, ddim_sampler\n",
        "# from cloud_diffusion.models import UNet2D, get_unet_params\n",
        "\n",
        "\n",
        "PROJECT_NAME = \"cloud_diffusion\"\n",
        "DATASET_ARTIFACT = 'ai-industry/cloud_diffusion/SEVIR:v0'\n",
        "\n",
        "config = SimpleNamespace(    \n",
        "    epochs=50, # number of epochs\n",
        "    model_name=\"unet_small\", # model name to save [unet_small, unet_big]\n",
        "    strategy=\"ddpm\", # strategy to use ddpm\n",
        "    noise_steps=1000, # number of noise steps on the diffusion process\n",
        "    sampler_steps=333, # number of sampler steps on the diffusion process\n",
        "    seed=42, # random seed\n",
        "    batch_size=32, # batch size\n",
        "    img_size=64, # image size\n",
        "    device=\"cuda\", # device\n",
        "    num_workers=8, # number of workers for dataloader\n",
        "    num_frames=4, # number of frames to use as input\n",
        "    lr=5e-4, # learning rate\n",
        "    validation_days=3, # number of days to use for validation\n",
        "    log_every_epoch=5, # log every n epochs to wandb\n",
        "    n_preds=8, # number of predictions to make\n",
        "    validate_epochs=False # compute metrics at each epoch \n",
        "    )\n",
        "\n",
        "def train_func(config):\n",
        "    config.model_params = get_unet_params(config.model_name, config.num_frames)\n",
        "\n",
        "    set_seed(config.seed)\n",
        "    device = torch.device(config.device)\n",
        "\n",
        "    # downlaod the dataset from the wandb.Artifact\n",
        "    files = download_dataset(DATASET_ARTIFACT, PROJECT_NAME)\n",
        "    # train_days, valid_days = files[:-config.validation_days], files[-config.validation_days:]\n",
        "    train_ds = CloudDataset(files=files, num_frames=config.num_frames, img_size=config.img_size)\n",
        "    # valid_ds = CloudDataset(files=files, num_frames=config.num_frames, img_size=config.img_size).shuffle()\n",
        "\n",
        "    # DDPM dataloaders\n",
        "    train_dataloader = NoisifyDataloader(train_ds, config.batch_size, shuffle=True, \n",
        "                                         noise_func=noisify_ddpm,  num_workers=config.num_workers)\n",
        "    valid_dataloader = NoisifyDataloader(train_ds, config.batch_size, shuffle=False, \n",
        "                                          noise_func=noisify_ddpm,  num_workers=config.num_workers)\n",
        "\n",
        "    # model setup\n",
        "    model = UNet2DTemporalCondition(**config.model_params)\n",
        "\n",
        "    # sampler\n",
        "    sampler = ddim_sampler(steps=config.sampler_steps)\n",
        "\n",
        "    # A simple training loop\n",
        "    trainer = MiniTrainer(train_dataloader, valid_dataloader, model, sampler, device)\n",
        "    trainer.fit(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "0b2d54c154084d13960ec3801557fd5b",
            "0f16dd30f52841a09128f0a796439801",
            "3ca897170c7942328347cb74d898d8ca",
            "98e3ffe929f9468b971aeb21ab3e3e1b",
            "c1a9d69e37b94b029a9ed695655395cb",
            "a25d7970e4f84be39032d38a5743c6db",
            "d107c5f883454d55a2bf754f08256009",
            "666fd4288d304dba8089c3b501a7c6a9",
            "f4e940d96e664e11bdbecfe7f5457419",
            "a9ef273d636a46128f0eab908755b332",
            "36207c7b5ad846208000295d59017f4d",
            "4518856a26a141fb9adbf1c46c47ee27",
            "77588e78cefd47089fc54eadd17e6708",
            "43c3c07b25bc4c81b900f342936adead",
            "fdebb0cd1f424bc494231503c186333c",
            "d902ea2dba0045868e98f7ba5cef48b5"
          ]
        },
        "id": "5XPIJlC0xO0T",
        "outputId": "f5a9d78d-50d7-4282-fae7-d540b1bcbeb1",
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmaidacundo\u001b[0m (\u001b[33mai-industry\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016671207749999438, max=1.0…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0b2d54c154084d13960ec3801557fd5b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.15.3"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230529_170832-g2gfz6f8</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/ai-industry/cloud_diffusion/runs/g2gfz6f8' target=\"_blank\">decent-silence-69</a></strong> to <a href='https://wandb.ai/ai-industry/cloud_diffusion' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/ai-industry/cloud_diffusion' target=\"_blank\">https://wandb.ai/ai-industry/cloud_diffusion</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/ai-industry/cloud_diffusion/runs/g2gfz6f8' target=\"_blank\">https://wandb.ai/ai-industry/cloud_diffusion/runs/g2gfz6f8</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact SEVIR:v0, 2756.25MB. 3 files... \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   3 of 3 files downloaded.  \n",
            "Done. 0:0:39.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      <progress value='0' class='' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      0.00% [0/50 00:00&lt;?]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      <progress value='0' class='' max='75' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      0.00% [0/75 00:00&lt;?]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:red\">(failed 1).</strong> Press Control-C to abort syncing."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f4e940d96e664e11bdbecfe7f5457419"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">decent-silence-69</strong> at: <a href='https://wandb.ai/ai-industry/cloud_diffusion/runs/g2gfz6f8' target=\"_blank\">https://wandb.ai/ai-industry/cloud_diffusion/runs/g2gfz6f8</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20230529_170832-g2gfz6f8/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
              "\u001b[31m│\u001b[0m in \u001b[92m<cell line: 1>\u001b[0m:\u001b[94m2\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m in \u001b[92mtrain_func\u001b[0m:\u001b[94m62\u001b[0m                                                                                 \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m in \u001b[92mfit\u001b[0m:\u001b[94m110\u001b[0m                                                                                       \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m in \u001b[92mone_epoch\u001b[0m:\u001b[94m73\u001b[0m                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1501\u001b[0m in \u001b[92m_call_impl\u001b[0m            \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1498 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._backward_pre_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks   \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1499 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_backward_pre_hooks \u001b[95mor\u001b[0m _global_backward_hooks                   \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1500 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1501 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*args, **kwargs)                                          \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1502 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                          \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1503 \u001b[0m\u001b[2m│   │   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1504 \u001b[0m\u001b[2m│   │   \u001b[0mbackward_pre_hooks = []                                                           \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m in \u001b[92mforward\u001b[0m:\u001b[94m89\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1501\u001b[0m in \u001b[92m_call_impl\u001b[0m            \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1498 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._backward_pre_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks   \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1499 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_backward_pre_hooks \u001b[95mor\u001b[0m _global_backward_hooks                   \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1500 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1501 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*args, **kwargs)                                          \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1502 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                          \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1503 \u001b[0m\u001b[2m│   │   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1504 \u001b[0m\u001b[2m│   │   \u001b[0mbackward_pre_hooks = []                                                           \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m in \u001b[92mforward\u001b[0m:\u001b[94m159\u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1501\u001b[0m in \u001b[92m_call_impl\u001b[0m            \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1498 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._backward_pre_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks   \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1499 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_backward_pre_hooks \u001b[95mor\u001b[0m _global_backward_hooks                   \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1500 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1501 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*args, **kwargs)                                          \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1502 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                          \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1503 \u001b[0m\u001b[2m│   │   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1504 \u001b[0m\u001b[2m│   │   \u001b[0mbackward_pre_hooks = []                                                           \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m in \u001b[92mforward\u001b[0m:\u001b[94m57\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
              "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
              "\u001b[1;91mRuntimeError: \u001b[0mTensors must have same number of dimensions: got \u001b[1;36m5\u001b[0m and \u001b[1;36m4\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;cell line: 1&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2</span>                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">train_func</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">62</span>                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">fit</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">110</span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">one_epoch</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">73</span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1501</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1498 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1499 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1500 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1501 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*args, **kwargs)                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1502 │   │   # Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1503 │   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1504 │   │   </span>backward_pre_hooks = []                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">89</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1501</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1498 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1499 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1500 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1501 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*args, **kwargs)                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1502 │   │   # Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1503 │   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1504 │   │   </span>backward_pre_hooks = []                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">159</span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1501</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1498 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1499 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1500 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1501 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*args, **kwargs)                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1502 │   │   # Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1503 │   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1504 │   │   </span>backward_pre_hooks = []                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">57</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
              "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">RuntimeError: </span>Tensors must have same number of dimensions: got <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> and <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "with wandb.init(project=PROJECT_NAME, entity='ai-industry', config=config, tags=[\"ddpm\", config.model_name]):\n",
        "    train_func(config)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0b2d54c154084d13960ec3801557fd5b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0f16dd30f52841a09128f0a796439801",
              "IPY_MODEL_3ca897170c7942328347cb74d898d8ca"
            ],
            "layout": "IPY_MODEL_98e3ffe929f9468b971aeb21ab3e3e1b"
          }
        },
        "0f16dd30f52841a09128f0a796439801": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c1a9d69e37b94b029a9ed695655395cb",
            "placeholder": "​",
            "style": "IPY_MODEL_a25d7970e4f84be39032d38a5743c6db",
            "value": "Waiting for wandb.init()...\r"
          }
        },
        "3ca897170c7942328347cb74d898d8ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d107c5f883454d55a2bf754f08256009",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_666fd4288d304dba8089c3b501a7c6a9",
            "value": 1
          }
        },
        "98e3ffe929f9468b971aeb21ab3e3e1b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c1a9d69e37b94b029a9ed695655395cb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a25d7970e4f84be39032d38a5743c6db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d107c5f883454d55a2bf754f08256009": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "666fd4288d304dba8089c3b501a7c6a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f4e940d96e664e11bdbecfe7f5457419": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a9ef273d636a46128f0eab908755b332",
              "IPY_MODEL_36207c7b5ad846208000295d59017f4d"
            ],
            "layout": "IPY_MODEL_4518856a26a141fb9adbf1c46c47ee27"
          }
        },
        "a9ef273d636a46128f0eab908755b332": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_77588e78cefd47089fc54eadd17e6708",
            "placeholder": "​",
            "style": "IPY_MODEL_43c3c07b25bc4c81b900f342936adead",
            "value": "0.001 MB of 0.002 MB uploaded (0.000 MB deduped)\r"
          }
        },
        "36207c7b5ad846208000295d59017f4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fdebb0cd1f424bc494231503c186333c",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d902ea2dba0045868e98f7ba5cef48b5",
            "value": 0.6861244019138756
          }
        },
        "4518856a26a141fb9adbf1c46c47ee27": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "77588e78cefd47089fc54eadd17e6708": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "43c3c07b25bc4c81b900f342936adead": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fdebb0cd1f424bc494231503c186333c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d902ea2dba0045868e98f7ba5cef48b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}